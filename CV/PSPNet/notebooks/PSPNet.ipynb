{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADE-PSPNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_cmN9FGSn-P",
        "outputId": "0f30b3cb-56cd-4005-d756-36490d115d41"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6933ZH7eSqfd"
      },
      "source": [
        "#!cd \"/content/drive/MyDrive/Projects/Clubs/Analytics/Coord Projects/Model Zoo/PSPNet\" && wget http://data.csail.mit.edu/places/ADEchallenge/ADEChallengeData2016.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeWcoiWmNgaS"
      },
      "source": [
        "#!cd \"/content/drive/MyDrive/Projects/Clubs/Analytics/Coord Projects/Model Zoo/PSPNet/datasets\" && unzip ../ADEChallengeData2016.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeMRtVx-pPhn"
      },
      "source": [
        "PATH = \"/content/drive/MyDrive/Projects/Clubs/Analytics/Coord Projects/Model Zoo/PSPNet/datasets/ADEChallengeData2016\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc_6X3pRpSFa"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JVOCCN2uyrn"
      },
      "source": [
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWsLjbf75MVj"
      },
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "import os, glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "class CreateDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, PATH, mode='training', n_classes=150):\n",
        "        self.mode = mode\n",
        "        self.n_classes = n_classes\n",
        "        self.entry = np.array([os.path.splitext(os.path.basename(entry))[0] for entry in glob.glob(os.path.join(PATH, \"images\", self.mode, \"*.jpg\"))])\n",
        "        np.random.shuffle(self.entry)\n",
        "        if self.mode == 'training':\n",
        "            max_size = 16384\n",
        "        elif self.mode == 'validation':\n",
        "            max_size = 128\n",
        "        self.entry = self.entry[:max_size]\n",
        "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "    def image_transform(self, image):\n",
        "        image = np.float32(np.array(image))/255.\n",
        "        image = image.transpose((2, 0, 1))\n",
        "        image = self.normalize(torch.from_numpy(image.copy()))\n",
        "        return image\n",
        "    def label_transform(self, label):\n",
        "        label = self.encode_label(label)\n",
        "        label = label.transpose((2, 0, 1))\n",
        "        return torch.from_numpy(label)\n",
        "\n",
        "    def encode_label(self, label):\n",
        "        encoded = np.zeros((label.shape[0], label.shape[1], self.n_classes))\n",
        "        for row in range(label.shape[0]):\n",
        "            for col in range(label.shape[1]):\n",
        "                idx = label[row][col]-1\n",
        "                encoded[row][col][idx] = 1\n",
        "        return encoded\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = cv2.imread(os.path.join(PATH, \"images\", self.mode, str(self.entry[index]+ \".jpg\")), 1)\n",
        "        label = cv2.imread(os.path.join(PATH, \"annotations\", self.mode, str(self.entry[index]+ \".png\")), 0)\n",
        "        image = cv2.resize(image, (224,224), interpolation=cv2.INTER_AREA)\n",
        "        label = cv2.resize(label, (224,224), interpolation=cv2.INTER_NEAREST)\n",
        "        image = self.image_transform(image)\n",
        "        label = self.label_transform(label)\n",
        "        return {'image': image, 'label': label}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.entry)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAWYXhFCv_y4"
      },
      "source": [
        "PATH = \"/content/drive/MyDrive/Projects/Clubs/Analytics/Coord Projects/Model Zoo/PSPNet/datasets/ADEChallengeData2016\"\n",
        "\n",
        "train_data = CreateDataset(PATH, mode='training')\n",
        "val_data = CreateDataset(PATH, mode='validation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jB3pkG5MML1h",
        "outputId": "d33cdde7-01fb-4365-fc53-c9c4f029b2d4"
      },
      "source": [
        "print(len(train_data))\n",
        "print(len(val_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16384\n",
            "128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZf2sVqtqWRt"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Grj3Hnrt3u7Q"
      },
      "source": [
        "!cp \"/content/drive/MyDrive/Projects/Clubs/Analytics/Coord Projects/Model Zoo/PSPNet/scripts/resnet.py\" /content/resnet.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7xPyCcL6X9U"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "\n",
        "from resnet import resnet101, resnet50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL3n9zq26dU9"
      },
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=1, padding=0, stride=1, dilation=1, bias=False):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        padding = (kernel_size + (kernel_size - 1) * (dilation - 1)) // 2\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        return out\n",
        "\n",
        "def upsample(input, size=None, scale_factor=None, align_corners=False):\n",
        "    out = F.interpolate(input, size=size, scale_factor=scale_factor, mode='bilinear', align_corners=align_corners)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aCcE4lU6hm0"
      },
      "source": [
        "class PyramidPooling(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(PyramidPooling, self).__init__()\n",
        "        self.pooling_size = [1, 2, 3, 6]\n",
        "        self.channels = in_channels // 4\n",
        "\n",
        "        self.pool1 = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(self.pooling_size[0]),\n",
        "            ConvBlock(in_channels, self.channels, kernel_size=1),\n",
        "        )\n",
        "\n",
        "        self.pool2 = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(self.pooling_size[1]),\n",
        "            ConvBlock(in_channels, self.channels, kernel_size=1),\n",
        "        )\n",
        "\n",
        "        self.pool3 = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(self.pooling_size[2]),\n",
        "            ConvBlock(in_channels, self.channels, kernel_size=1),\n",
        "        )\n",
        "\n",
        "        self.pool4 = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(self.pooling_size[3]),\n",
        "            ConvBlock(in_channels, self.channels, kernel_size=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.pool1(x)\n",
        "        out1 = upsample(out1, size=x.size()[-2:])\n",
        "\n",
        "        out2 = self.pool2(x)\n",
        "        out2 = upsample(out2, size=x.size()[-2:])\n",
        "\n",
        "        out3 = self.pool3(x)\n",
        "        out3 = upsample(out3, size=x.size()[-2:])\n",
        "\n",
        "        out4 = self.pool4(x)\n",
        "        out4 = upsample(out4, size=x.size()[-2:])\n",
        "\n",
        "        out = torch.cat([x, out1, out2, out3, out4], dim=1)\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHluSIWS6V75"
      },
      "source": [
        "class PSPNet(nn.Module):\n",
        "    def __init__(self, n_classes=150):\n",
        "        super(PSPNet, self).__init__()\n",
        "        self.out_channels = 2048\n",
        "\n",
        "        self.backbone = resnet50(pretrained=True)\n",
        "        self.stem = nn.Sequential(\n",
        "            *list(self.backbone.children())[:4],\n",
        "        )\n",
        "        self.block1 = self.backbone.layer1\n",
        "        self.block2 = self.backbone.layer2\n",
        "        self.block3 = self.backbone.layer3\n",
        "        self.block4 = self.backbone.layer4\n",
        "        self.low_level_features_conv = ConvBlock(512, 64, kernel_size=3)\n",
        "\n",
        "        self.depth = self.out_channels // 4\n",
        "        self.pyramid_pooling = PyramidPooling(self.out_channels)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            ConvBlock(self.out_channels * 2, self.depth, kernel_size=3),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Conv2d(self.depth, n_classes, kernel_size=1),\n",
        "        )\n",
        "\n",
        "        self.aux = nn.Sequential(\n",
        "            ConvBlock(self.out_channels // 2, self.depth // 2, kernel_size=3),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Conv2d(self.depth // 2, n_classes, kernel_size=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, image, label=None):\n",
        "        out = self.stem(image)\n",
        "        out1 = self.block1(out)\n",
        "        out2 = self.block2(out1)\n",
        "        out3 = self.block3(out2)\n",
        "        aux_out = self.aux(out3)\n",
        "        aux_out = upsample(aux_out, size=image.size()[-2:], align_corners=True)\n",
        "        out4 = self.block4(out3)\n",
        "\n",
        "        out = self.pyramid_pooling(out4)\n",
        "        out = self.decoder(out)\n",
        "        out = upsample(out, size=image.size()[-2:])\n",
        "\n",
        "        out = upsample(out, size=image.size()[-2:], align_corners=True)\n",
        "        #out = F.softmax(out, dim=1)\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2g0Yo5wqXe2"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAl3VVmvqYib"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import gc\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vhrnwqha-rfo"
      },
      "source": [
        "batch_size = 16\n",
        "start_epochs = 0\n",
        "epochs = 2\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "CHECKPOINT = \"/content/drive/MyDrive/Projects/Clubs/Analytics/Coord Projects/Model Zoo/PSPNet/checkpoints\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_pheWRK-ke_"
      },
      "source": [
        "trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "valloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rEawFJKApLU",
        "outputId": "2b8fea56-7c52-485e-999b-00a95ae689f2"
      },
      "source": [
        "model = PSPNet()\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "resnet50-19c8e357.pth: 103MB [00:00, 113MB/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PSPNet(\n",
              "  (backbone): ResNet(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (5): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              "  )\n",
              "  (stem): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (block1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (block2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (block3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (block4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (low_level_features_conv): ConvBlock(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (pyramid_pooling): PyramidPooling(\n",
              "    (pool1): Sequential(\n",
              "      (0): AdaptiveAvgPool2d(output_size=1)\n",
              "      (1): ConvBlock(\n",
              "        (conv): Sequential(\n",
              "          (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pool2): Sequential(\n",
              "      (0): AdaptiveAvgPool2d(output_size=2)\n",
              "      (1): ConvBlock(\n",
              "        (conv): Sequential(\n",
              "          (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pool3): Sequential(\n",
              "      (0): AdaptiveAvgPool2d(output_size=3)\n",
              "      (1): ConvBlock(\n",
              "        (conv): Sequential(\n",
              "          (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pool4): Sequential(\n",
              "      (0): AdaptiveAvgPool2d(output_size=6)\n",
              "      (1): ConvBlock(\n",
              "        (conv): Sequential(\n",
              "          (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): Sequential(\n",
              "    (0): ConvBlock(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2d(4096, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "      )\n",
              "    )\n",
              "    (1): Dropout(p=0.1, inplace=False)\n",
              "    (2): Conv2d(512, 150, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (aux): Sequential(\n",
              "    (0): ConvBlock(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "      )\n",
              "    )\n",
              "    (1): Dropout(p=0.1, inplace=False)\n",
              "    (2): Conv2d(256, 150, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0_yTVp1CqKF"
      },
      "source": [
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PwtRYKQTK_V"
      },
      "source": [
        "if os.path.exists(os.path.join(CHECKPOINT, \"model.pth\")):\n",
        "    checkpoints = torch.load(os.path.join(CHECKPOINT, \"model.pth\"))\n",
        "\n",
        "    model.load_state_dict(checkpoints['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoints['optimizer_state_dict'])\n",
        "    start_epochs = checkpoints['epoch']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddqAYYeAJRBU"
      },
      "source": [
        "train_loss = []\n",
        "train_acc = []\n",
        "val_loss = []\n",
        "val_acc = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk2K0xh4JXFG"
      },
      "source": [
        "def plot_loss(epoch, train_loss, val_loss):\n",
        "    epochs = np.arange(len(train_loss))\n",
        "    plt.plot(epochs, train_loss, label=\"Train Loss\", color=\"green\")\n",
        "    plt.plot(epochs, val_loss, label=\"Validation Loss\", color=\"red\")\n",
        "\n",
        "    plt.title(\"Training Loss - \"+str(epoch))\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc=\"best\")\n",
        "\n",
        "    plt.savefig(os.path.join(CHECKPOINT, \"Loss.png\"))\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-gq3xjq-uj-",
        "outputId": "eb91a5d5-f5d9-4a54-c232-44d32e12b734"
      },
      "source": [
        "for epoch in range(start_epochs+1, epochs+start_epochs+1):\n",
        "    print(\"Starting Epoch[{0}/{1}]\".format(epoch, epochs+start_epochs))\n",
        "    \n",
        "    time_epoch_start = time.time()\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    model.train()\n",
        "    epoch_train_loss = []\n",
        "    epoch_val_loss = []\n",
        "    for idx, batch in enumerate(trainloader, 1):\n",
        "        time_batch_start = time.time()\n",
        "        image = batch['image'].to(device)\n",
        "        label = batch['label'].to(device)\n",
        "        preds = model(image)\n",
        "        loss = criterion(preds, label)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_train_loss.append(loss.item())\n",
        "        time_batch_end = time.time()\n",
        "        print(\"Epoch[{0}]: Batch[{1}]   Train Loss: {2}     Time: {3}\".format(epoch, idx, loss.item(), time_batch_end-time_batch_start))\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    train_loss.append(epoch_train_loss[-1])\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(valloader, 1):\n",
        "            time_batch_start = time.time()\n",
        "            image = batch['image'].to(device)\n",
        "            label = batch['label'].to(device)\n",
        "            preds = model(image)\n",
        "            loss = criterion(preds, label)\n",
        "\n",
        "            epoch_val_loss.append(loss.item())\n",
        "            time_batch_end = time.time()\n",
        "            print(\"Epoch[{0}]: Batch[{1}]   Val Loss: {2}     Time: {3}\".format(epoch, idx, loss.item(), time_batch_end-time_batch_start))\n",
        "\n",
        "        val_loss.append(epoch_val_loss[-1])\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': train_loss[-1],\n",
        "            }, os.path.join(CHECKPOINT, \"model.pth\"))\n",
        "    \n",
        "    plot_loss(epoch, train_loss, val_loss)\n",
        "\n",
        "    time_epoch_end = time.time()\n",
        "    print(\"Finished Epoch[{0}/{1}] in Time: {2}\".format(epoch, epochs+start_epochs, time_epoch_end-time_epoch_start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Epoch[5/6]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch[5]: Batch[1]   Train Loss: 0.029129930609841665     Time: 1.4080426692962646\n",
            "Epoch[5]: Batch[2]   Train Loss: 0.028919635106608964     Time: 1.3773972988128662\n",
            "Epoch[5]: Batch[3]   Train Loss: 0.030620687727820756     Time: 1.3787586688995361\n",
            "Epoch[5]: Batch[4]   Train Loss: 0.028615320370407125     Time: 1.3880276679992676\n",
            "Epoch[5]: Batch[5]   Train Loss: 0.02991612826465397     Time: 1.3839471340179443\n",
            "Epoch[5]: Batch[6]   Train Loss: 0.025937067124050518     Time: 1.3778319358825684\n",
            "Epoch[5]: Batch[7]   Train Loss: 0.02735940616616979     Time: 1.3742187023162842\n",
            "Epoch[5]: Batch[8]   Train Loss: 0.028439698274732687     Time: 1.3883349895477295\n",
            "Epoch[5]: Batch[9]   Train Loss: 0.026399859088310666     Time: 1.3877308368682861\n",
            "Epoch[5]: Batch[10]   Train Loss: 0.02785656333546975     Time: 1.36895751953125\n",
            "Epoch[5]: Batch[11]   Train Loss: 0.028080850275465915     Time: 1.3809592723846436\n",
            "Epoch[5]: Batch[12]   Train Loss: 0.02446043168081937     Time: 1.3722093105316162\n",
            "Epoch[5]: Batch[13]   Train Loss: 0.02760790081886934     Time: 1.3937351703643799\n",
            "Epoch[5]: Batch[14]   Train Loss: 0.030772731023292445     Time: 1.3794238567352295\n",
            "Epoch[5]: Batch[15]   Train Loss: 0.03159907089149586     Time: 1.3779499530792236\n",
            "Epoch[5]: Batch[16]   Train Loss: 0.026825953673831233     Time: 1.391890048980713\n",
            "Epoch[5]: Batch[17]   Train Loss: 0.02684537369844596     Time: 1.379964828491211\n",
            "Epoch[5]: Batch[18]   Train Loss: 0.027518042329951843     Time: 1.3781964778900146\n",
            "Epoch[5]: Batch[19]   Train Loss: 0.028929888600259617     Time: 1.384629249572754\n",
            "Epoch[5]: Batch[20]   Train Loss: 0.027475814887306705     Time: 1.3859634399414062\n",
            "Epoch[5]: Batch[21]   Train Loss: 0.026554368834548665     Time: 1.3754279613494873\n",
            "Epoch[5]: Batch[22]   Train Loss: 0.0285369462861666     Time: 1.389533519744873\n",
            "Epoch[5]: Batch[23]   Train Loss: 0.028512911080537023     Time: 1.3851206302642822\n",
            "Epoch[5]: Batch[24]   Train Loss: 0.026558678051572818     Time: 1.3996078968048096\n",
            "Epoch[5]: Batch[25]   Train Loss: 0.026481921885455874     Time: 1.3701066970825195\n",
            "Epoch[5]: Batch[26]   Train Loss: 0.02335533417347616     Time: 1.3919544219970703\n",
            "Epoch[5]: Batch[27]   Train Loss: 0.02776416318908539     Time: 1.383068561553955\n",
            "Epoch[5]: Batch[28]   Train Loss: 0.027331775227315093     Time: 1.3796656131744385\n",
            "Epoch[5]: Batch[29]   Train Loss: 0.027597585014861325     Time: 1.3862555027008057\n",
            "Epoch[5]: Batch[30]   Train Loss: 0.030006614876773904     Time: 1.3989734649658203\n",
            "Epoch[5]: Batch[31]   Train Loss: 0.030766257150684765     Time: 1.3677771091461182\n",
            "Epoch[5]: Batch[32]   Train Loss: 0.0315805653977763     Time: 1.3792214393615723\n",
            "Epoch[5]: Batch[33]   Train Loss: 0.02761321267116203     Time: 1.3771514892578125\n",
            "Epoch[5]: Batch[34]   Train Loss: 0.02851041614608017     Time: 1.3840253353118896\n",
            "Epoch[5]: Batch[35]   Train Loss: 0.02906296232487018     Time: 1.381481409072876\n",
            "Epoch[5]: Batch[36]   Train Loss: 0.026105133495508755     Time: 1.3951237201690674\n",
            "Epoch[5]: Batch[37]   Train Loss: 0.03186769554798699     Time: 1.3765287399291992\n",
            "Epoch[5]: Batch[38]   Train Loss: 0.027060112051241824     Time: 1.360318660736084\n",
            "Epoch[5]: Batch[39]   Train Loss: 0.025816479821192517     Time: 1.379345417022705\n",
            "Epoch[5]: Batch[40]   Train Loss: 0.026063743691860933     Time: 1.4026134014129639\n",
            "Epoch[5]: Batch[41]   Train Loss: 0.029924003187103934     Time: 1.3829355239868164\n",
            "Epoch[5]: Batch[42]   Train Loss: 0.02681680278010587     Time: 1.378051519393921\n",
            "Epoch[5]: Batch[43]   Train Loss: 0.02788844180169764     Time: 1.3703067302703857\n",
            "Epoch[5]: Batch[44]   Train Loss: 0.02312199778740703     Time: 1.3736460208892822\n",
            "Epoch[5]: Batch[45]   Train Loss: 0.027419324274081253     Time: 1.3793268203735352\n",
            "Epoch[5]: Batch[46]   Train Loss: 0.025446963461889075     Time: 1.3780324459075928\n",
            "Epoch[5]: Batch[47]   Train Loss: 0.02690612661641837     Time: 1.3737082481384277\n",
            "Epoch[5]: Batch[48]   Train Loss: 0.02565482732956115     Time: 1.3886265754699707\n",
            "Epoch[5]: Batch[49]   Train Loss: 0.03000827997016755     Time: 1.3824470043182373\n",
            "Epoch[5]: Batch[50]   Train Loss: 0.02819317884644661     Time: 1.3829107284545898\n",
            "Epoch[5]: Batch[51]   Train Loss: 0.03356752978050201     Time: 1.3693456649780273\n",
            "Epoch[5]: Batch[52]   Train Loss: 0.02715446719880986     Time: 1.375579833984375\n",
            "Epoch[5]: Batch[53]   Train Loss: 0.029227644408563295     Time: 1.3734471797943115\n",
            "Epoch[5]: Batch[54]   Train Loss: 0.02865721641841517     Time: 1.371769666671753\n",
            "Epoch[5]: Batch[55]   Train Loss: 0.027973915266151183     Time: 1.3965752124786377\n",
            "Epoch[5]: Batch[56]   Train Loss: 0.031004164573945624     Time: 1.381793737411499\n",
            "Epoch[5]: Batch[57]   Train Loss: 0.026172320403454484     Time: 1.3868467807769775\n",
            "Epoch[5]: Batch[58]   Train Loss: 0.028276241931856152     Time: 1.3901069164276123\n",
            "Epoch[5]: Batch[59]   Train Loss: 0.028014710988771688     Time: 1.399010419845581\n",
            "Epoch[5]: Batch[60]   Train Loss: 0.02779940356556209     Time: 1.3747954368591309\n",
            "Epoch[5]: Batch[61]   Train Loss: 0.03134431719578274     Time: 1.3728694915771484\n",
            "Epoch[5]: Batch[62]   Train Loss: 0.028165221477051243     Time: 1.3708481788635254\n",
            "Epoch[5]: Batch[63]   Train Loss: 0.030627403158206296     Time: 1.3714599609375\n",
            "Epoch[5]: Batch[64]   Train Loss: 0.02917286657234143     Time: 1.3847472667694092\n",
            "Epoch[5]: Batch[65]   Train Loss: 0.030982322051403596     Time: 1.3885393142700195\n",
            "Epoch[5]: Batch[66]   Train Loss: 0.027941296313488306     Time: 1.3991472721099854\n",
            "Epoch[5]: Batch[67]   Train Loss: 0.02750037181600568     Time: 1.3857941627502441\n",
            "Epoch[5]: Batch[68]   Train Loss: 0.027148053467972753     Time: 1.378335952758789\n",
            "Epoch[5]: Batch[69]   Train Loss: 0.02742709158499375     Time: 1.386366605758667\n",
            "Epoch[5]: Batch[70]   Train Loss: 0.02480081662387948     Time: 1.3965177536010742\n",
            "Epoch[5]: Batch[71]   Train Loss: 0.027518631936018764     Time: 1.3883726596832275\n",
            "Epoch[5]: Batch[72]   Train Loss: 0.031592703899082954     Time: 1.3834927082061768\n",
            "Epoch[5]: Batch[73]   Train Loss: 0.026661499153065052     Time: 1.3741936683654785\n",
            "Epoch[5]: Batch[74]   Train Loss: 0.025647014758936576     Time: 1.3947253227233887\n",
            "Epoch[5]: Batch[75]   Train Loss: 0.030415321827018234     Time: 1.3805952072143555\n",
            "Epoch[5]: Batch[76]   Train Loss: 0.024486503286338626     Time: 1.3736422061920166\n",
            "Epoch[5]: Batch[77]   Train Loss: 0.027189964257075556     Time: 1.3882653713226318\n",
            "Epoch[5]: Batch[78]   Train Loss: 0.031059024074672238     Time: 1.378993034362793\n",
            "Epoch[5]: Batch[79]   Train Loss: 0.026660793120352154     Time: 1.3670120239257812\n",
            "Epoch[5]: Batch[80]   Train Loss: 0.027110440961677612     Time: 1.3796684741973877\n",
            "Epoch[5]: Batch[81]   Train Loss: 0.027686024169011444     Time: 1.3715441226959229\n",
            "Epoch[5]: Batch[82]   Train Loss: 0.0286449107641198     Time: 1.3718018531799316\n",
            "Epoch[5]: Batch[83]   Train Loss: 0.026276771942608954     Time: 1.368328332901001\n",
            "Epoch[5]: Batch[84]   Train Loss: 0.02740878943115017     Time: 1.3700392246246338\n",
            "Epoch[5]: Batch[85]   Train Loss: 0.027929651644981245     Time: 1.3736085891723633\n",
            "Epoch[5]: Batch[86]   Train Loss: 0.028952353067862514     Time: 1.3703598976135254\n",
            "Epoch[5]: Batch[87]   Train Loss: 0.027706515655053294     Time: 1.3747620582580566\n",
            "Epoch[5]: Batch[88]   Train Loss: 0.027970503369065373     Time: 1.368227243423462\n",
            "Epoch[5]: Batch[89]   Train Loss: 0.028836711040060813     Time: 1.378248929977417\n",
            "Epoch[5]: Batch[90]   Train Loss: 0.026120803644173008     Time: 1.3617322444915771\n",
            "Epoch[5]: Batch[91]   Train Loss: 0.02801866725562458     Time: 1.3725395202636719\n",
            "Epoch[5]: Batch[92]   Train Loss: 0.026298972513890642     Time: 1.3775012493133545\n",
            "Epoch[5]: Batch[93]   Train Loss: 0.025715244291808645     Time: 1.3786282539367676\n",
            "Epoch[5]: Batch[94]   Train Loss: 0.024437822077165265     Time: 1.3977811336517334\n",
            "Epoch[5]: Batch[95]   Train Loss: 0.025248227006735624     Time: 1.3856687545776367\n",
            "Epoch[5]: Batch[96]   Train Loss: 0.024438947902933595     Time: 1.3944547176361084\n",
            "Epoch[5]: Batch[97]   Train Loss: 0.026365656179147955     Time: 1.3805394172668457\n",
            "Epoch[5]: Batch[98]   Train Loss: 0.025786204896806192     Time: 1.3761968612670898\n",
            "Epoch[5]: Batch[99]   Train Loss: 0.028627811699428542     Time: 1.391599178314209\n",
            "Epoch[5]: Batch[100]   Train Loss: 0.023219697865549206     Time: 1.3698534965515137\n",
            "Epoch[5]: Batch[101]   Train Loss: 0.026850650452136016     Time: 1.388662576675415\n",
            "Epoch[5]: Batch[102]   Train Loss: 0.027419889983068022     Time: 1.3768374919891357\n",
            "Epoch[5]: Batch[103]   Train Loss: 0.02523789165569359     Time: 1.4003689289093018\n",
            "Epoch[5]: Batch[104]   Train Loss: 0.02614944221090062     Time: 1.3789498805999756\n",
            "Epoch[5]: Batch[105]   Train Loss: 0.02514154881378212     Time: 1.381838083267212\n",
            "Epoch[5]: Batch[106]   Train Loss: 0.025738121136158344     Time: 1.3847455978393555\n",
            "Epoch[5]: Batch[107]   Train Loss: 0.026301455084450524     Time: 1.377181053161621\n",
            "Epoch[5]: Batch[108]   Train Loss: 0.026006892393977115     Time: 1.392357349395752\n",
            "Epoch[5]: Batch[109]   Train Loss: 0.02581675253341758     Time: 1.3832006454467773\n",
            "Epoch[5]: Batch[110]   Train Loss: 0.02311013044545217     Time: 1.3810207843780518\n",
            "Epoch[5]: Batch[111]   Train Loss: 0.026490280172690454     Time: 1.3677613735198975\n",
            "Epoch[5]: Batch[112]   Train Loss: 0.02672298928280757     Time: 1.3821210861206055\n",
            "Epoch[5]: Batch[113]   Train Loss: 0.027959077156293335     Time: 1.3764753341674805\n",
            "Epoch[5]: Batch[114]   Train Loss: 0.02552137726833334     Time: 1.3868858814239502\n",
            "Epoch[5]: Batch[115]   Train Loss: 0.026208856211218885     Time: 1.385491132736206\n",
            "Epoch[5]: Batch[116]   Train Loss: 0.024546379504761715     Time: 1.3872032165527344\n",
            "Epoch[5]: Batch[117]   Train Loss: 0.026777137390501932     Time: 1.3852207660675049\n",
            "Epoch[5]: Batch[118]   Train Loss: 0.024708213658975516     Time: 1.3774940967559814\n",
            "Epoch[5]: Batch[119]   Train Loss: 0.025895814259344264     Time: 1.3746488094329834\n",
            "Epoch[5]: Batch[120]   Train Loss: 0.02507631839328553     Time: 1.3760738372802734\n",
            "Epoch[5]: Batch[121]   Train Loss: 0.027744042596533746     Time: 1.3817479610443115\n",
            "Epoch[5]: Batch[122]   Train Loss: 0.025423006347663975     Time: 1.378960132598877\n",
            "Epoch[5]: Batch[123]   Train Loss: 0.024595612998611974     Time: 1.375906229019165\n",
            "Epoch[5]: Batch[124]   Train Loss: 0.027066878117561664     Time: 1.3795850276947021\n",
            "Epoch[5]: Batch[125]   Train Loss: 0.024523560141080657     Time: 1.372426986694336\n",
            "Epoch[5]: Batch[126]   Train Loss: 0.027612929479775866     Time: 1.3719325065612793\n",
            "Epoch[5]: Batch[127]   Train Loss: 0.030142658901264035     Time: 1.3917222023010254\n",
            "Epoch[5]: Batch[128]   Train Loss: 0.025294164193885922     Time: 1.3760199546813965\n",
            "Epoch[5]: Batch[129]   Train Loss: 0.02677803542750179     Time: 1.3747632503509521\n",
            "Epoch[5]: Batch[130]   Train Loss: 0.02696084183824888     Time: 1.3726227283477783\n",
            "Epoch[5]: Batch[131]   Train Loss: 0.02887570857925518     Time: 1.3924822807312012\n",
            "Epoch[5]: Batch[132]   Train Loss: 0.025833735474373763     Time: 1.3772454261779785\n",
            "Epoch[5]: Batch[133]   Train Loss: 0.025631147608278414     Time: 1.3904993534088135\n",
            "Epoch[5]: Batch[134]   Train Loss: 0.031314702901775335     Time: 1.3920984268188477\n",
            "Epoch[5]: Batch[135]   Train Loss: 0.027677040707662363     Time: 1.3963124752044678\n",
            "Epoch[5]: Batch[136]   Train Loss: 0.026999083294747393     Time: 1.386777639389038\n",
            "Epoch[5]: Batch[137]   Train Loss: 0.02397253345131567     Time: 1.374239444732666\n",
            "Epoch[5]: Batch[138]   Train Loss: 0.027124115244230394     Time: 1.3792805671691895\n",
            "Epoch[5]: Batch[139]   Train Loss: 0.026206544333394276     Time: 1.383660078048706\n",
            "Epoch[5]: Batch[140]   Train Loss: 0.027815593014111243     Time: 1.3714196681976318\n",
            "Epoch[5]: Batch[141]   Train Loss: 0.02334741825433867     Time: 1.3847723007202148\n",
            "Epoch[5]: Batch[142]   Train Loss: 0.02822195513196705     Time: 1.3744115829467773\n",
            "Epoch[5]: Batch[143]   Train Loss: 0.027236181872373005     Time: 1.3795607089996338\n",
            "Epoch[5]: Batch[144]   Train Loss: 0.028054746195215648     Time: 1.3708112239837646\n",
            "Epoch[5]: Batch[145]   Train Loss: 0.031234181609499985     Time: 1.3781354427337646\n",
            "Epoch[5]: Batch[146]   Train Loss: 0.02708617704073291     Time: 1.3679091930389404\n",
            "Epoch[5]: Batch[147]   Train Loss: 0.028362039865526417     Time: 1.3955881595611572\n",
            "Epoch[5]: Batch[148]   Train Loss: 0.02733349502023138     Time: 1.3791329860687256\n",
            "Epoch[5]: Batch[149]   Train Loss: 0.02773740614839881     Time: 1.3735880851745605\n",
            "Epoch[5]: Batch[150]   Train Loss: 0.024075263874640283     Time: 1.385711431503296\n",
            "Epoch[5]: Batch[151]   Train Loss: 0.0244352821252651     Time: 1.370774507522583\n",
            "Epoch[5]: Batch[152]   Train Loss: 0.025077513240376516     Time: 1.3827846050262451\n",
            "Epoch[5]: Batch[153]   Train Loss: 0.02582368795204263     Time: 1.3782360553741455\n",
            "Epoch[5]: Batch[154]   Train Loss: 0.02652210725934228     Time: 1.379462480545044\n",
            "Epoch[5]: Batch[155]   Train Loss: 0.02409394849783594     Time: 1.3780608177185059\n",
            "Epoch[5]: Batch[156]   Train Loss: 0.026093638212941292     Time: 1.3774845600128174\n",
            "Epoch[5]: Batch[157]   Train Loss: 0.02453661066837274     Time: 1.380744218826294\n",
            "Epoch[5]: Batch[158]   Train Loss: 0.024128069756593918     Time: 1.393235206604004\n",
            "Epoch[5]: Batch[159]   Train Loss: 0.027442120763297263     Time: 1.3757729530334473\n",
            "Epoch[5]: Batch[160]   Train Loss: 0.02612812937143666     Time: 1.3929705619812012\n",
            "Epoch[5]: Batch[161]   Train Loss: 0.025735829864154965     Time: 1.3787217140197754\n",
            "Epoch[5]: Batch[162]   Train Loss: 0.02567460851202206     Time: 1.3780460357666016\n",
            "Epoch[5]: Batch[163]   Train Loss: 0.026722928043296214     Time: 1.3760871887207031\n",
            "Epoch[5]: Batch[164]   Train Loss: 0.0274373569564081     Time: 1.3833279609680176\n",
            "Epoch[5]: Batch[165]   Train Loss: 0.028413621849991345     Time: 1.3862919807434082\n",
            "Epoch[5]: Batch[166]   Train Loss: 0.027758596812362746     Time: 1.3853998184204102\n",
            "Epoch[5]: Batch[167]   Train Loss: 0.026573120286065496     Time: 1.3785555362701416\n",
            "Epoch[5]: Batch[168]   Train Loss: 0.030309043924621568     Time: 1.3763315677642822\n",
            "Epoch[5]: Batch[169]   Train Loss: 0.02430637990443872     Time: 1.398665189743042\n",
            "Epoch[5]: Batch[170]   Train Loss: 0.02716526159621019     Time: 1.383331537246704\n",
            "Epoch[5]: Batch[171]   Train Loss: 0.023590579960684945     Time: 1.381629467010498\n",
            "Epoch[5]: Batch[172]   Train Loss: 0.029728793461378595     Time: 1.3823444843292236\n",
            "Epoch[5]: Batch[173]   Train Loss: 0.029260165696117432     Time: 1.3772804737091064\n",
            "Epoch[5]: Batch[174]   Train Loss: 0.02579287864420066     Time: 1.3746135234832764\n",
            "Epoch[5]: Batch[175]   Train Loss: 0.0269758362679572     Time: 1.3836522102355957\n",
            "Epoch[5]: Batch[176]   Train Loss: 0.03080246012260878     Time: 1.3780488967895508\n",
            "Epoch[5]: Batch[177]   Train Loss: 0.028000785647303184     Time: 1.3704445362091064\n",
            "Epoch[5]: Batch[178]   Train Loss: 0.02534203947844571     Time: 1.3743844032287598\n",
            "Epoch[5]: Batch[179]   Train Loss: 0.02874455245711757     Time: 1.3712260723114014\n",
            "Epoch[5]: Batch[180]   Train Loss: 0.02723630612952453     Time: 1.396270513534546\n",
            "Epoch[5]: Batch[181]   Train Loss: 0.02830935609612014     Time: 1.384228229522705\n",
            "Epoch[5]: Batch[182]   Train Loss: 0.027984958095778008     Time: 1.3732080459594727\n",
            "Epoch[5]: Batch[183]   Train Loss: 0.025643019185716803     Time: 1.3788154125213623\n",
            "Epoch[5]: Batch[184]   Train Loss: 0.02805295189278266     Time: 1.39139986038208\n",
            "Epoch[5]: Batch[185]   Train Loss: 0.026986621675839435     Time: 1.379241943359375\n",
            "Epoch[5]: Batch[186]   Train Loss: 0.026096649542579904     Time: 1.3688416481018066\n",
            "Epoch[5]: Batch[187]   Train Loss: 0.030196731225971427     Time: 1.388502836227417\n",
            "Epoch[5]: Batch[188]   Train Loss: 0.028930963317796557     Time: 1.3711979389190674\n",
            "Epoch[5]: Batch[189]   Train Loss: 0.029776837873838235     Time: 1.3673627376556396\n",
            "Epoch[5]: Batch[190]   Train Loss: 0.027050360990630424     Time: 1.3829753398895264\n",
            "Epoch[5]: Batch[191]   Train Loss: 0.027558630431149186     Time: 1.3716704845428467\n",
            "Epoch[5]: Batch[192]   Train Loss: 0.025533893688798482     Time: 1.3734540939331055\n",
            "Epoch[5]: Batch[193]   Train Loss: 0.0251570236665659     Time: 1.3710317611694336\n",
            "Epoch[5]: Batch[194]   Train Loss: 0.025428464092308308     Time: 1.372748851776123\n",
            "Epoch[5]: Batch[195]   Train Loss: 0.02686853198471685     Time: 1.370880126953125\n",
            "Epoch[5]: Batch[196]   Train Loss: 0.02841104325117232     Time: 1.3713738918304443\n",
            "Epoch[5]: Batch[197]   Train Loss: 0.029532458629572184     Time: 1.38130521774292\n",
            "Epoch[5]: Batch[198]   Train Loss: 0.025197608292788474     Time: 1.3706066608428955\n",
            "Epoch[5]: Batch[199]   Train Loss: 0.02635278355223698     Time: 1.3679835796356201\n",
            "Epoch[5]: Batch[200]   Train Loss: 0.028594390413247787     Time: 1.3699285984039307\n",
            "Epoch[5]: Batch[201]   Train Loss: 0.026210601574763532     Time: 1.3853931427001953\n",
            "Epoch[5]: Batch[202]   Train Loss: 0.027199193990144564     Time: 1.3869633674621582\n",
            "Epoch[5]: Batch[203]   Train Loss: 0.02630426920760933     Time: 1.3897831439971924\n",
            "Epoch[5]: Batch[204]   Train Loss: 0.027161477555224495     Time: 1.3897099494934082\n",
            "Epoch[5]: Batch[205]   Train Loss: 0.027841129523566124     Time: 1.374936580657959\n",
            "Epoch[5]: Batch[206]   Train Loss: 0.02720102334859474     Time: 1.3869330883026123\n",
            "Epoch[5]: Batch[207]   Train Loss: 0.02739342500316134     Time: 1.3879213333129883\n",
            "Epoch[5]: Batch[208]   Train Loss: 0.02559697895783523     Time: 1.402348279953003\n",
            "Epoch[5]: Batch[209]   Train Loss: 0.02886624033490633     Time: 1.3892648220062256\n",
            "Epoch[5]: Batch[210]   Train Loss: 0.028492596866210747     Time: 1.3793866634368896\n",
            "Epoch[5]: Batch[211]   Train Loss: 0.02679717877034337     Time: 1.3797528743743896\n",
            "Epoch[5]: Batch[212]   Train Loss: 0.02699879826316569     Time: 1.3825581073760986\n",
            "Epoch[5]: Batch[213]   Train Loss: 0.026268371344138095     Time: 1.3830845355987549\n",
            "Epoch[5]: Batch[214]   Train Loss: 0.02929015667044369     Time: 1.374342679977417\n",
            "Epoch[5]: Batch[215]   Train Loss: 0.02829352800396051     Time: 1.3839476108551025\n",
            "Epoch[5]: Batch[216]   Train Loss: 0.026610281727361754     Time: 1.3701815605163574\n",
            "Epoch[5]: Batch[217]   Train Loss: 0.030802970886673584     Time: 1.3853824138641357\n",
            "Epoch[5]: Batch[218]   Train Loss: 0.025468857202452247     Time: 1.389678716659546\n",
            "Epoch[5]: Batch[219]   Train Loss: 0.030972531700947957     Time: 1.3690457344055176\n",
            "Epoch[5]: Batch[220]   Train Loss: 0.028110113621516445     Time: 1.3919434547424316\n",
            "Epoch[5]: Batch[221]   Train Loss: 0.026532227630542617     Time: 1.3710598945617676\n",
            "Epoch[5]: Batch[222]   Train Loss: 0.02689973731827063     Time: 1.377084493637085\n",
            "Epoch[5]: Batch[223]   Train Loss: 0.03041239782881022     Time: 1.3726778030395508\n",
            "Epoch[5]: Batch[224]   Train Loss: 0.028920288189674497     Time: 1.3873772621154785\n",
            "Epoch[5]: Batch[225]   Train Loss: 0.029151953776624263     Time: 1.3792240619659424\n",
            "Epoch[5]: Batch[226]   Train Loss: 0.02595143701288339     Time: 1.3820538520812988\n",
            "Epoch[5]: Batch[227]   Train Loss: 0.027375024807455642     Time: 1.3842167854309082\n",
            "Epoch[5]: Batch[228]   Train Loss: 0.02595571017487031     Time: 1.3712849617004395\n",
            "Epoch[5]: Batch[229]   Train Loss: 0.028414466452792702     Time: 1.378455638885498\n",
            "Epoch[5]: Batch[230]   Train Loss: 0.029408078182673443     Time: 1.3754944801330566\n",
            "Epoch[5]: Batch[231]   Train Loss: 0.028930244267436477     Time: 1.3845951557159424\n",
            "Epoch[5]: Batch[232]   Train Loss: 0.024652594037075915     Time: 1.380772352218628\n",
            "Epoch[5]: Batch[233]   Train Loss: 0.028962617331869334     Time: 1.37937331199646\n",
            "Epoch[5]: Batch[234]   Train Loss: 0.028642963345888114     Time: 1.3864972591400146\n",
            "Epoch[5]: Batch[235]   Train Loss: 0.029336999053416794     Time: 1.3798747062683105\n",
            "Epoch[5]: Batch[236]   Train Loss: 0.02859579011520355     Time: 1.3864307403564453\n",
            "Epoch[5]: Batch[237]   Train Loss: 0.02970111847753704     Time: 1.3663113117218018\n",
            "Epoch[5]: Batch[238]   Train Loss: 0.02565076376571824     Time: 1.3847074508666992\n",
            "Epoch[5]: Batch[239]   Train Loss: 0.030701516239278977     Time: 1.3704931735992432\n",
            "Epoch[5]: Batch[240]   Train Loss: 0.026205754126726277     Time: 1.3742270469665527\n",
            "Epoch[5]: Batch[241]   Train Loss: 0.030993132240694273     Time: 1.3779950141906738\n",
            "Epoch[5]: Batch[242]   Train Loss: 0.026515817523833897     Time: 1.3788411617279053\n",
            "Epoch[5]: Batch[243]   Train Loss: 0.023392757570813282     Time: 1.3795170783996582\n",
            "Epoch[5]: Batch[244]   Train Loss: 0.028355177279635595     Time: 1.3814160823822021\n",
            "Epoch[5]: Batch[245]   Train Loss: 0.028298585699368044     Time: 1.3745887279510498\n",
            "Epoch[5]: Batch[246]   Train Loss: 0.028446749960523172     Time: 1.382646083831787\n",
            "Epoch[5]: Batch[247]   Train Loss: 0.027330278006316388     Time: 1.3895788192749023\n",
            "Epoch[5]: Batch[248]   Train Loss: 0.025668476152678962     Time: 1.380089282989502\n",
            "Epoch[5]: Batch[249]   Train Loss: 0.02604425091621619     Time: 1.393798589706421\n",
            "Epoch[5]: Batch[250]   Train Loss: 0.026725454037471865     Time: 1.3913264274597168\n",
            "Epoch[5]: Batch[251]   Train Loss: 0.02802054090792113     Time: 1.3736374378204346\n",
            "Epoch[5]: Batch[252]   Train Loss: 0.03044702766160465     Time: 1.3701608180999756\n",
            "Epoch[5]: Batch[253]   Train Loss: 0.0250748120974676     Time: 1.382659912109375\n",
            "Epoch[5]: Batch[254]   Train Loss: 0.030554896703842684     Time: 1.3869328498840332\n",
            "Epoch[5]: Batch[255]   Train Loss: 0.029384718623543454     Time: 1.3899109363555908\n",
            "Epoch[5]: Batch[256]   Train Loss: 0.02756782035807193     Time: 1.3731820583343506\n",
            "Epoch[5]: Batch[257]   Train Loss: 0.02507867046590165     Time: 1.3842475414276123\n",
            "Epoch[5]: Batch[258]   Train Loss: 0.025644445348144605     Time: 1.3870348930358887\n",
            "Epoch[5]: Batch[259]   Train Loss: 0.025241981740010182     Time: 1.384267807006836\n",
            "Epoch[5]: Batch[260]   Train Loss: 0.028212550376914818     Time: 1.3786852359771729\n",
            "Epoch[5]: Batch[261]   Train Loss: 0.026613746327832328     Time: 1.3873610496520996\n",
            "Epoch[5]: Batch[262]   Train Loss: 0.026452029527442925     Time: 1.379101037979126\n",
            "Epoch[5]: Batch[263]   Train Loss: 0.02602971809802786     Time: 1.380537509918213\n",
            "Epoch[5]: Batch[264]   Train Loss: 0.024940310780249896     Time: 1.3844733238220215\n",
            "Epoch[5]: Batch[265]   Train Loss: 0.029554362357969023     Time: 1.3858487606048584\n",
            "Epoch[5]: Batch[266]   Train Loss: 0.02587123932064151     Time: 1.373246192932129\n",
            "Epoch[5]: Batch[267]   Train Loss: 0.02783104076947229     Time: 1.3760888576507568\n",
            "Epoch[5]: Batch[268]   Train Loss: 0.026009349091215755     Time: 1.373065710067749\n",
            "Epoch[5]: Batch[269]   Train Loss: 0.025833363770855287     Time: 1.3861291408538818\n",
            "Epoch[5]: Batch[270]   Train Loss: 0.026955383616118146     Time: 1.36995530128479\n",
            "Epoch[5]: Batch[271]   Train Loss: 0.030843447854679056     Time: 1.3983168601989746\n",
            "Epoch[5]: Batch[272]   Train Loss: 0.025113761420445663     Time: 1.3906400203704834\n",
            "Epoch[5]: Batch[273]   Train Loss: 0.02920891955291088     Time: 1.3775317668914795\n",
            "Epoch[5]: Batch[274]   Train Loss: 0.025136104829829455     Time: 1.3802762031555176\n",
            "Epoch[5]: Batch[275]   Train Loss: 0.02613963219629274     Time: 1.369997262954712\n",
            "Epoch[5]: Batch[276]   Train Loss: 0.02498515328630521     Time: 1.390442132949829\n",
            "Epoch[5]: Batch[277]   Train Loss: 0.025926844141540423     Time: 1.3671715259552002\n",
            "Epoch[5]: Batch[278]   Train Loss: 0.025813593735448174     Time: 1.3910510540008545\n",
            "Epoch[5]: Batch[279]   Train Loss: 0.030358784232243145     Time: 1.3953688144683838\n",
            "Epoch[5]: Batch[280]   Train Loss: 0.026329745542671822     Time: 1.3725829124450684\n",
            "Epoch[5]: Batch[281]   Train Loss: 0.028195942623040243     Time: 1.3877789974212646\n",
            "Epoch[5]: Batch[282]   Train Loss: 0.02828218451908386     Time: 1.3792791366577148\n",
            "Epoch[5]: Batch[283]   Train Loss: 0.02691121766233816     Time: 1.3773579597473145\n",
            "Epoch[5]: Batch[284]   Train Loss: 0.025509318897898002     Time: 1.391038417816162\n",
            "Epoch[5]: Batch[285]   Train Loss: 0.024834497382649194     Time: 1.3847172260284424\n",
            "Epoch[5]: Batch[286]   Train Loss: 0.023554524274233362     Time: 1.367422342300415\n",
            "Epoch[5]: Batch[287]   Train Loss: 0.028224478884763347     Time: 1.3644495010375977\n",
            "Epoch[5]: Batch[288]   Train Loss: 0.027732888371233323     Time: 1.3767797946929932\n",
            "Epoch[5]: Batch[289]   Train Loss: 0.02754203603071406     Time: 1.370863676071167\n",
            "Epoch[5]: Batch[290]   Train Loss: 0.02678502721634348     Time: 1.3910176753997803\n",
            "Epoch[5]: Batch[291]   Train Loss: 0.02504410049265651     Time: 1.3757603168487549\n",
            "Epoch[5]: Batch[292]   Train Loss: 0.027877510497653597     Time: 1.3839075565338135\n",
            "Epoch[5]: Batch[293]   Train Loss: 0.024322175361712342     Time: 1.3730549812316895\n",
            "Epoch[5]: Batch[294]   Train Loss: 0.024520106352693888     Time: 1.3742733001708984\n",
            "Epoch[5]: Batch[295]   Train Loss: 0.024196887159032437     Time: 1.377380132675171\n",
            "Epoch[5]: Batch[296]   Train Loss: 0.02663293074032309     Time: 1.3717777729034424\n",
            "Epoch[5]: Batch[297]   Train Loss: 0.024313120383593824     Time: 1.3736097812652588\n",
            "Epoch[5]: Batch[298]   Train Loss: 0.0245425315688807     Time: 1.3706228733062744\n",
            "Epoch[5]: Batch[299]   Train Loss: 0.03304470652545787     Time: 1.3792870044708252\n",
            "Epoch[5]: Batch[300]   Train Loss: 0.023200026048925113     Time: 1.3674745559692383\n",
            "Epoch[5]: Batch[301]   Train Loss: 0.026003732560930037     Time: 1.3725972175598145\n",
            "Epoch[5]: Batch[302]   Train Loss: 0.02692180833989581     Time: 1.3709189891815186\n",
            "Epoch[5]: Batch[303]   Train Loss: 0.028269597707759515     Time: 1.368474006652832\n",
            "Epoch[5]: Batch[304]   Train Loss: 0.02699904785869327     Time: 1.3797039985656738\n",
            "Epoch[5]: Batch[305]   Train Loss: 0.0275740006233823     Time: 1.3702194690704346\n",
            "Epoch[5]: Batch[306]   Train Loss: 0.025252073079389873     Time: 1.3732562065124512\n",
            "Epoch[5]: Batch[307]   Train Loss: 0.023695234665619572     Time: 1.365182876586914\n",
            "Epoch[5]: Batch[308]   Train Loss: 0.025045854162016937     Time: 1.3669500350952148\n",
            "Epoch[5]: Batch[309]   Train Loss: 0.024278450870683456     Time: 1.3723909854888916\n",
            "Epoch[5]: Batch[310]   Train Loss: 0.026116297696732124     Time: 1.3855068683624268\n",
            "Epoch[5]: Batch[311]   Train Loss: 0.025829046871420343     Time: 1.3794569969177246\n",
            "Epoch[5]: Batch[312]   Train Loss: 0.024228512889042663     Time: 1.3868563175201416\n",
            "Epoch[5]: Batch[313]   Train Loss: 0.029290581534182254     Time: 1.3804268836975098\n",
            "Epoch[5]: Batch[314]   Train Loss: 0.023963595000887947     Time: 1.3837809562683105\n",
            "Epoch[5]: Batch[315]   Train Loss: 0.02626627751942959     Time: 1.3901698589324951\n",
            "Epoch[5]: Batch[316]   Train Loss: 0.025347229387203383     Time: 1.3770508766174316\n",
            "Epoch[5]: Batch[317]   Train Loss: 0.026745534051729566     Time: 1.3885836601257324\n",
            "Epoch[5]: Batch[318]   Train Loss: 0.031092423307211202     Time: 1.3921537399291992\n",
            "Epoch[5]: Batch[319]   Train Loss: 0.024680497480149315     Time: 1.3857710361480713\n",
            "Epoch[5]: Batch[320]   Train Loss: 0.026481185331418734     Time: 1.3874144554138184\n",
            "Epoch[5]: Batch[321]   Train Loss: 0.025516741183041446     Time: 1.3926270008087158\n",
            "Epoch[5]: Batch[322]   Train Loss: 0.024746671014086778     Time: 1.382765293121338\n",
            "Epoch[5]: Batch[323]   Train Loss: 0.029045295830426258     Time: 1.3847854137420654\n",
            "Epoch[5]: Batch[324]   Train Loss: 0.02647543990658158     Time: 1.3698971271514893\n",
            "Epoch[5]: Batch[325]   Train Loss: 0.0224723200001958     Time: 1.3812944889068604\n",
            "Epoch[5]: Batch[326]   Train Loss: 0.025508018424602673     Time: 1.3825838565826416\n",
            "Epoch[5]: Batch[327]   Train Loss: 0.028197949418806666     Time: 1.3849313259124756\n",
            "Epoch[5]: Batch[328]   Train Loss: 0.028653272999503633     Time: 1.3743548393249512\n",
            "Epoch[5]: Batch[329]   Train Loss: 0.023927732866860056     Time: 1.3779628276824951\n",
            "Epoch[5]: Batch[330]   Train Loss: 0.02563816698371192     Time: 1.376107931137085\n",
            "Epoch[5]: Batch[331]   Train Loss: 0.031301796919333484     Time: 1.3904900550842285\n",
            "Epoch[5]: Batch[332]   Train Loss: 0.025747175534918267     Time: 1.3812754154205322\n",
            "Epoch[5]: Batch[333]   Train Loss: 0.026723040687610798     Time: 1.369509220123291\n",
            "Epoch[5]: Batch[334]   Train Loss: 0.025680782118246336     Time: 1.3744399547576904\n",
            "Epoch[5]: Batch[335]   Train Loss: 0.027037058256392982     Time: 1.3676490783691406\n",
            "Epoch[5]: Batch[336]   Train Loss: 0.025321542281858227     Time: 1.3757946491241455\n",
            "Epoch[5]: Batch[337]   Train Loss: 0.023730747634722073     Time: 1.3998751640319824\n",
            "Epoch[5]: Batch[338]   Train Loss: 0.02715684726520819     Time: 1.3961966037750244\n",
            "Epoch[5]: Batch[339]   Train Loss: 0.028863652024873274     Time: 1.3780152797698975\n",
            "Epoch[5]: Batch[340]   Train Loss: 0.026681660779346455     Time: 1.3657972812652588\n",
            "Epoch[5]: Batch[341]   Train Loss: 0.02644932906185657     Time: 1.3733818531036377\n",
            "Epoch[5]: Batch[342]   Train Loss: 0.026075704767603288     Time: 1.374990701675415\n",
            "Epoch[5]: Batch[343]   Train Loss: 0.024826034227542728     Time: 1.369251012802124\n",
            "Epoch[5]: Batch[344]   Train Loss: 0.024235551350367247     Time: 1.3818144798278809\n",
            "Epoch[5]: Batch[345]   Train Loss: 0.028077487171324463     Time: 1.3754208087921143\n",
            "Epoch[5]: Batch[346]   Train Loss: 0.025910787418417167     Time: 1.3758656978607178\n",
            "Epoch[5]: Batch[347]   Train Loss: 0.02875647748315671     Time: 1.390272855758667\n",
            "Epoch[5]: Batch[348]   Train Loss: 0.02656654144496199     Time: 1.3764281272888184\n",
            "Epoch[5]: Batch[349]   Train Loss: 0.024537445864584107     Time: 1.3841004371643066\n",
            "Epoch[5]: Batch[350]   Train Loss: 0.027362124668777684     Time: 1.3710033893585205\n",
            "Epoch[5]: Batch[351]   Train Loss: 0.02693997287032538     Time: 1.4030137062072754\n",
            "Epoch[5]: Batch[352]   Train Loss: 0.025586599549081802     Time: 1.377931833267212\n",
            "Epoch[5]: Batch[353]   Train Loss: 0.02872686091655987     Time: 1.3883979320526123\n",
            "Epoch[5]: Batch[354]   Train Loss: 0.023356447616656437     Time: 1.3819799423217773\n",
            "Epoch[5]: Batch[355]   Train Loss: 0.027784039440774268     Time: 1.393284559249878\n",
            "Epoch[5]: Batch[356]   Train Loss: 0.023846477698645886     Time: 1.3742055892944336\n",
            "Epoch[5]: Batch[357]   Train Loss: 0.029680930514028987     Time: 1.3794939517974854\n",
            "Epoch[5]: Batch[358]   Train Loss: 0.028878780642081336     Time: 1.3720715045928955\n",
            "Epoch[5]: Batch[359]   Train Loss: 0.02322252175851734     Time: 1.3849987983703613\n",
            "Epoch[5]: Batch[360]   Train Loss: 0.026533492680840716     Time: 1.3900892734527588\n",
            "Epoch[5]: Batch[361]   Train Loss: 0.02263528832202446     Time: 1.3707401752471924\n",
            "Epoch[5]: Batch[362]   Train Loss: 0.02738000624875928     Time: 1.3872523307800293\n",
            "Epoch[5]: Batch[363]   Train Loss: 0.02330260475398999     Time: 1.3803138732910156\n",
            "Epoch[5]: Batch[364]   Train Loss: 0.025039538441428807     Time: 1.3849308490753174\n",
            "Epoch[5]: Batch[365]   Train Loss: 0.025331383880975503     Time: 1.3930647373199463\n",
            "Epoch[5]: Batch[366]   Train Loss: 0.025381099585937223     Time: 1.3756015300750732\n",
            "Epoch[5]: Batch[367]   Train Loss: 0.02380331998913897     Time: 1.3798375129699707\n",
            "Epoch[5]: Batch[368]   Train Loss: 0.027436313004983694     Time: 1.366523027420044\n",
            "Epoch[5]: Batch[369]   Train Loss: 0.02538648006935478     Time: 1.3751893043518066\n",
            "Epoch[5]: Batch[370]   Train Loss: 0.028052294137702764     Time: 1.3710205554962158\n",
            "Epoch[5]: Batch[371]   Train Loss: 0.02305978424915305     Time: 1.373199462890625\n",
            "Epoch[5]: Batch[372]   Train Loss: 0.02749602350774673     Time: 1.3811426162719727\n",
            "Epoch[5]: Batch[373]   Train Loss: 0.028852278318144537     Time: 1.3859097957611084\n",
            "Epoch[5]: Batch[374]   Train Loss: 0.023784485583824337     Time: 1.3803205490112305\n",
            "Epoch[5]: Batch[375]   Train Loss: 0.02873387399696664     Time: 1.3714447021484375\n",
            "Epoch[5]: Batch[376]   Train Loss: 0.02703772585480444     Time: 1.3862192630767822\n",
            "Epoch[5]: Batch[377]   Train Loss: 0.027657020792331788     Time: 1.37428879737854\n",
            "Epoch[5]: Batch[378]   Train Loss: 0.026233507323316835     Time: 1.3961107730865479\n",
            "Epoch[5]: Batch[379]   Train Loss: 0.025523916107418144     Time: 1.3801209926605225\n",
            "Epoch[5]: Batch[380]   Train Loss: 0.0301685151471766     Time: 1.3766505718231201\n",
            "Epoch[5]: Batch[381]   Train Loss: 0.02498885951716745     Time: 1.381356954574585\n",
            "Epoch[5]: Batch[382]   Train Loss: 0.024254804820194584     Time: 1.379152774810791\n",
            "Epoch[5]: Batch[383]   Train Loss: 0.02787083615508234     Time: 1.3891656398773193\n",
            "Epoch[5]: Batch[384]   Train Loss: 0.02629215719692414     Time: 1.3742804527282715\n",
            "Epoch[5]: Batch[385]   Train Loss: 0.028903043295026725     Time: 1.3805749416351318\n",
            "Epoch[5]: Batch[386]   Train Loss: 0.027659372292297285     Time: 1.3849241733551025\n",
            "Epoch[5]: Batch[387]   Train Loss: 0.028593473276126977     Time: 1.385554313659668\n",
            "Epoch[5]: Batch[388]   Train Loss: 0.027408004576523422     Time: 1.3868114948272705\n",
            "Epoch[5]: Batch[389]   Train Loss: 0.027735283792886034     Time: 1.3722989559173584\n",
            "Epoch[5]: Batch[390]   Train Loss: 0.02902001161816585     Time: 1.388394832611084\n",
            "Epoch[5]: Batch[391]   Train Loss: 0.025831178999093166     Time: 1.3719704151153564\n",
            "Epoch[5]: Batch[392]   Train Loss: 0.02555921993519735     Time: 1.376122236251831\n",
            "Epoch[5]: Batch[393]   Train Loss: 0.027847300397772126     Time: 1.3772180080413818\n",
            "Epoch[5]: Batch[394]   Train Loss: 0.030386553089794554     Time: 1.3715496063232422\n",
            "Epoch[5]: Batch[395]   Train Loss: 0.027466981070019363     Time: 1.3904683589935303\n",
            "Epoch[5]: Batch[396]   Train Loss: 0.026600508680572114     Time: 1.382080078125\n",
            "Epoch[5]: Batch[397]   Train Loss: 0.027007931381626493     Time: 1.373929500579834\n",
            "Epoch[5]: Batch[398]   Train Loss: 0.023057950399981215     Time: 1.3746275901794434\n",
            "Epoch[5]: Batch[399]   Train Loss: 0.0320884351646406     Time: 1.3883938789367676\n",
            "Epoch[5]: Batch[400]   Train Loss: 0.027192590623736364     Time: 1.388836145401001\n",
            "Epoch[5]: Batch[401]   Train Loss: 0.026436754589070754     Time: 1.3733060359954834\n",
            "Epoch[5]: Batch[402]   Train Loss: 0.02841549415888425     Time: 1.3797082901000977\n",
            "Epoch[5]: Batch[403]   Train Loss: 0.025643387513311345     Time: 1.3802433013916016\n",
            "Epoch[5]: Batch[404]   Train Loss: 0.024208354965554253     Time: 1.3996737003326416\n",
            "Epoch[5]: Batch[405]   Train Loss: 0.025659634471270218     Time: 1.3735995292663574\n",
            "Epoch[5]: Batch[406]   Train Loss: 0.027062495030849273     Time: 1.380070447921753\n",
            "Epoch[5]: Batch[407]   Train Loss: 0.02795946458915243     Time: 1.3688921928405762\n",
            "Epoch[5]: Batch[408]   Train Loss: 0.025959437461834076     Time: 1.3704981803894043\n",
            "Epoch[5]: Batch[409]   Train Loss: 0.028229500750751896     Time: 1.3727824687957764\n",
            "Epoch[5]: Batch[410]   Train Loss: 0.025872964164823962     Time: 1.373533010482788\n",
            "Epoch[5]: Batch[411]   Train Loss: 0.02483494554545008     Time: 1.3737812042236328\n",
            "Epoch[5]: Batch[412]   Train Loss: 0.026041732597671476     Time: 1.369969367980957\n",
            "Epoch[5]: Batch[413]   Train Loss: 0.02591572191001413     Time: 1.3767752647399902\n",
            "Epoch[5]: Batch[414]   Train Loss: 0.02698257770345408     Time: 1.3686389923095703\n",
            "Epoch[5]: Batch[415]   Train Loss: 0.030934865014685366     Time: 1.370588779449463\n",
            "Epoch[5]: Batch[416]   Train Loss: 0.029132879174870466     Time: 1.3704893589019775\n",
            "Epoch[5]: Batch[417]   Train Loss: 0.024552979179442092     Time: 1.372422456741333\n",
            "Epoch[5]: Batch[418]   Train Loss: 0.02628389647566055     Time: 1.380096435546875\n",
            "Epoch[5]: Batch[419]   Train Loss: 0.02756732991943376     Time: 1.3753087520599365\n",
            "Epoch[5]: Batch[420]   Train Loss: 0.026822454975673718     Time: 1.3772621154785156\n",
            "Epoch[5]: Batch[421]   Train Loss: 0.02669651223666818     Time: 1.3835797309875488\n",
            "Epoch[5]: Batch[422]   Train Loss: 0.02544194532474379     Time: 1.3740489482879639\n",
            "Epoch[5]: Batch[423]   Train Loss: 0.029964808206677362     Time: 1.3856744766235352\n",
            "Epoch[5]: Batch[424]   Train Loss: 0.026628068664780215     Time: 1.3757195472717285\n",
            "Epoch[5]: Batch[425]   Train Loss: 0.02946579256734089     Time: 1.3931350708007812\n",
            "Epoch[5]: Batch[426]   Train Loss: 0.028201234914677405     Time: 1.3797271251678467\n",
            "Epoch[5]: Batch[427]   Train Loss: 0.02956991206112022     Time: 1.3766953945159912\n",
            "Epoch[5]: Batch[428]   Train Loss: 0.027634873313873595     Time: 1.3954570293426514\n",
            "Epoch[5]: Batch[429]   Train Loss: 0.029852516264072006     Time: 1.371537685394287\n",
            "Epoch[5]: Batch[430]   Train Loss: 0.02819784711470108     Time: 1.382148027420044\n",
            "Epoch[5]: Batch[431]   Train Loss: 0.02719308047039047     Time: 1.370682716369629\n",
            "Epoch[5]: Batch[432]   Train Loss: 0.028781797635954162     Time: 1.3847923278808594\n",
            "Epoch[5]: Batch[433]   Train Loss: 0.027866903400399577     Time: 1.3844196796417236\n",
            "Epoch[5]: Batch[434]   Train Loss: 0.027723764183673037     Time: 1.3783488273620605\n",
            "Epoch[5]: Batch[435]   Train Loss: 0.026825673334467143     Time: 1.3721516132354736\n",
            "Epoch[5]: Batch[436]   Train Loss: 0.03167753378927383     Time: 1.374542474746704\n",
            "Epoch[5]: Batch[437]   Train Loss: 0.02399973888415892     Time: 1.3897242546081543\n",
            "Epoch[5]: Batch[438]   Train Loss: 0.024618777784566875     Time: 1.3712162971496582\n",
            "Epoch[5]: Batch[439]   Train Loss: 0.029079821645643177     Time: 1.3770453929901123\n",
            "Epoch[5]: Batch[440]   Train Loss: 0.027145735549552446     Time: 1.3792738914489746\n",
            "Epoch[5]: Batch[441]   Train Loss: 0.02841345970223025     Time: 1.3900799751281738\n",
            "Epoch[5]: Batch[442]   Train Loss: 0.02867100746534175     Time: 1.3705050945281982\n",
            "Epoch[5]: Batch[443]   Train Loss: 0.02547732504476813     Time: 1.3847613334655762\n",
            "Epoch[5]: Batch[444]   Train Loss: 0.027645277094858273     Time: 1.3761568069458008\n",
            "Epoch[5]: Batch[445]   Train Loss: 0.027721620044892918     Time: 1.3822355270385742\n",
            "Epoch[5]: Batch[446]   Train Loss: 0.02779503804671354     Time: 1.371070146560669\n",
            "Epoch[5]: Batch[447]   Train Loss: 0.02566711937954009     Time: 1.3747875690460205\n",
            "Epoch[5]: Batch[448]   Train Loss: 0.02757682071749762     Time: 1.38136887550354\n",
            "Epoch[5]: Batch[449]   Train Loss: 0.027725544174756932     Time: 1.3752343654632568\n",
            "Epoch[5]: Batch[450]   Train Loss: 0.0267250352821368     Time: 1.3759539127349854\n",
            "Epoch[5]: Batch[451]   Train Loss: 0.029750945307382173     Time: 1.372826337814331\n",
            "Epoch[5]: Batch[452]   Train Loss: 0.026642167582389117     Time: 1.3703107833862305\n",
            "Epoch[5]: Batch[453]   Train Loss: 0.028925162950417728     Time: 1.390657663345337\n",
            "Epoch[5]: Batch[454]   Train Loss: 0.030450142781700152     Time: 1.372743844985962\n",
            "Epoch[5]: Batch[455]   Train Loss: 0.02932400653191114     Time: 1.3802211284637451\n",
            "Epoch[5]: Batch[456]   Train Loss: 0.031399094243176336     Time: 1.391249418258667\n",
            "Epoch[5]: Batch[457]   Train Loss: 0.027476281592069992     Time: 1.372267246246338\n",
            "Epoch[5]: Batch[458]   Train Loss: 0.02737114767348641     Time: 1.3880040645599365\n",
            "Epoch[5]: Batch[459]   Train Loss: 0.028974090807785617     Time: 1.3679964542388916\n",
            "Epoch[5]: Batch[460]   Train Loss: 0.025155354475512214     Time: 1.3952574729919434\n",
            "Epoch[5]: Batch[461]   Train Loss: 0.025856154555105215     Time: 1.3769254684448242\n",
            "Epoch[5]: Batch[462]   Train Loss: 0.028247792505105014     Time: 1.383434534072876\n",
            "Epoch[5]: Batch[463]   Train Loss: 0.029066497576374997     Time: 1.3767437934875488\n",
            "Epoch[5]: Batch[464]   Train Loss: 0.030250468243681695     Time: 1.3753883838653564\n",
            "Epoch[5]: Batch[465]   Train Loss: 0.031980522495886905     Time: 1.3771758079528809\n",
            "Epoch[5]: Batch[466]   Train Loss: 0.030147743472127345     Time: 1.3745880126953125\n",
            "Epoch[5]: Batch[467]   Train Loss: 0.028697219028033898     Time: 1.375614881515503\n",
            "Epoch[5]: Batch[468]   Train Loss: 0.02960759480941191     Time: 1.3692200183868408\n",
            "Epoch[5]: Batch[469]   Train Loss: 0.026209275874888964     Time: 1.3858869075775146\n",
            "Epoch[5]: Batch[470]   Train Loss: 0.027752841817119324     Time: 1.3829636573791504\n",
            "Epoch[5]: Batch[471]   Train Loss: 0.028902140035401497     Time: 1.372591495513916\n",
            "Epoch[5]: Batch[472]   Train Loss: 0.028148090002221363     Time: 1.3704264163970947\n",
            "Epoch[5]: Batch[473]   Train Loss: 0.02716782879094817     Time: 1.3821711540222168\n",
            "Epoch[5]: Batch[474]   Train Loss: 0.027573848491547433     Time: 1.3886418342590332\n",
            "Epoch[5]: Batch[475]   Train Loss: 0.02681419149577293     Time: 1.3797876834869385\n",
            "Epoch[5]: Batch[476]   Train Loss: 0.02611479350832119     Time: 1.3756694793701172\n",
            "Epoch[5]: Batch[477]   Train Loss: 0.027507121085421287     Time: 1.371819019317627\n",
            "Epoch[5]: Batch[478]   Train Loss: 0.029000452608345984     Time: 1.3891098499298096\n",
            "Epoch[5]: Batch[479]   Train Loss: 0.02820949539694359     Time: 1.378715991973877\n",
            "Epoch[5]: Batch[480]   Train Loss: 0.02819285397694745     Time: 1.374305009841919\n",
            "Epoch[5]: Batch[481]   Train Loss: 0.024909506816571364     Time: 1.387392282485962\n",
            "Epoch[5]: Batch[482]   Train Loss: 0.02794665472828966     Time: 1.3718650341033936\n",
            "Epoch[5]: Batch[483]   Train Loss: 0.027668288485650634     Time: 1.392195463180542\n",
            "Epoch[5]: Batch[484]   Train Loss: 0.0294521276317463     Time: 1.375196933746338\n",
            "Epoch[5]: Batch[485]   Train Loss: 0.027931754034364098     Time: 1.3726153373718262\n",
            "Epoch[5]: Batch[486]   Train Loss: 0.028134789182740504     Time: 1.374739408493042\n",
            "Epoch[5]: Batch[487]   Train Loss: 0.02572691925731966     Time: 1.3846471309661865\n",
            "Epoch[5]: Batch[488]   Train Loss: 0.029420952981415945     Time: 1.3835811614990234\n",
            "Epoch[5]: Batch[489]   Train Loss: 0.02795554015643578     Time: 1.3721797466278076\n",
            "Epoch[5]: Batch[490]   Train Loss: 0.026664070049424494     Time: 1.398207426071167\n",
            "Epoch[5]: Batch[491]   Train Loss: 0.03140225167064734     Time: 1.391829252243042\n",
            "Epoch[5]: Batch[492]   Train Loss: 0.026399792800479504     Time: 1.3851418495178223\n",
            "Epoch[5]: Batch[493]   Train Loss: 0.02688447417273064     Time: 1.3679163455963135\n",
            "Epoch[5]: Batch[494]   Train Loss: 0.023632526989167228     Time: 1.386545181274414\n",
            "Epoch[5]: Batch[495]   Train Loss: 0.025477796196045243     Time: 1.3729169368743896\n",
            "Epoch[5]: Batch[496]   Train Loss: 0.02580456158802267     Time: 1.3927955627441406\n",
            "Epoch[5]: Batch[497]   Train Loss: 0.029567512161008495     Time: 1.4002628326416016\n",
            "Epoch[5]: Batch[498]   Train Loss: 0.024979274407193212     Time: 1.3907842636108398\n",
            "Epoch[5]: Batch[499]   Train Loss: 0.027783153581681753     Time: 1.3905856609344482\n",
            "Epoch[5]: Batch[500]   Train Loss: 0.02874900482562511     Time: 1.3723714351654053\n",
            "Epoch[5]: Batch[501]   Train Loss: 0.026134614724339814     Time: 1.3869223594665527\n",
            "Epoch[5]: Batch[502]   Train Loss: 0.028708484704408115     Time: 1.3716843128204346\n",
            "Epoch[5]: Batch[503]   Train Loss: 0.02597531190440209     Time: 1.394623041152954\n",
            "Epoch[5]: Batch[504]   Train Loss: 0.027065575439390175     Time: 1.3733134269714355\n",
            "Epoch[5]: Batch[505]   Train Loss: 0.025583258817155112     Time: 1.3892936706542969\n",
            "Epoch[5]: Batch[506]   Train Loss: 0.028990570220652465     Time: 1.374495506286621\n",
            "Epoch[5]: Batch[507]   Train Loss: 0.03136723447359649     Time: 1.3682284355163574\n",
            "Epoch[5]: Batch[508]   Train Loss: 0.027764925509176322     Time: 1.3861510753631592\n",
            "Epoch[5]: Batch[509]   Train Loss: 0.02445067168903395     Time: 1.3700454235076904\n",
            "Epoch[5]: Batch[510]   Train Loss: 0.028032546133355818     Time: 1.3945956230163574\n",
            "Epoch[5]: Batch[511]   Train Loss: 0.02687942099958424     Time: 1.3763275146484375\n",
            "Epoch[5]: Batch[512]   Train Loss: 0.024548361896498297     Time: 1.392014741897583\n",
            "Epoch[5]: Batch[513]   Train Loss: 0.02729013511299713     Time: 1.3866548538208008\n",
            "Epoch[5]: Batch[514]   Train Loss: 0.026586140122576866     Time: 1.3664748668670654\n",
            "Epoch[5]: Batch[515]   Train Loss: 0.027903787285969813     Time: 1.3703818321228027\n",
            "Epoch[5]: Batch[516]   Train Loss: 0.02793043100663854     Time: 1.3693664073944092\n",
            "Epoch[5]: Batch[517]   Train Loss: 0.028813570565749215     Time: 1.3766119480133057\n",
            "Epoch[5]: Batch[518]   Train Loss: 0.028575753192014903     Time: 1.3657374382019043\n",
            "Epoch[5]: Batch[519]   Train Loss: 0.026060302580183204     Time: 1.3674745559692383\n",
            "Epoch[5]: Batch[520]   Train Loss: 0.02631582610886219     Time: 1.3586978912353516\n",
            "Epoch[5]: Batch[521]   Train Loss: 0.028937802712632736     Time: 1.369434118270874\n",
            "Epoch[5]: Batch[522]   Train Loss: 0.030789131759444418     Time: 1.3698556423187256\n",
            "Epoch[5]: Batch[523]   Train Loss: 0.028656929257597025     Time: 1.3670172691345215\n",
            "Epoch[5]: Batch[524]   Train Loss: 0.027577419222895493     Time: 1.3707375526428223\n",
            "Epoch[5]: Batch[525]   Train Loss: 0.025330914539730742     Time: 1.368544101715088\n",
            "Epoch[5]: Batch[526]   Train Loss: 0.02702004452492543     Time: 1.3704450130462646\n",
            "Epoch[5]: Batch[527]   Train Loss: 0.02697912946724374     Time: 1.371994972229004\n",
            "Epoch[5]: Batch[528]   Train Loss: 0.026282592566092788     Time: 1.375110387802124\n",
            "Epoch[5]: Batch[529]   Train Loss: 0.026446429042737675     Time: 1.3781349658966064\n",
            "Epoch[5]: Batch[530]   Train Loss: 0.027785752333905483     Time: 1.3714280128479004\n",
            "Epoch[5]: Batch[531]   Train Loss: 0.02659002893423436     Time: 1.394207239151001\n",
            "Epoch[5]: Batch[532]   Train Loss: 0.028067301519217387     Time: 1.3724288940429688\n",
            "Epoch[5]: Batch[533]   Train Loss: 0.026145492868809843     Time: 1.3897016048431396\n",
            "Epoch[5]: Batch[534]   Train Loss: 0.026988071272287015     Time: 1.3931095600128174\n",
            "Epoch[5]: Batch[535]   Train Loss: 0.032957554218092634     Time: 1.3763747215270996\n",
            "Epoch[5]: Batch[536]   Train Loss: 0.028358145492202298     Time: 1.379906415939331\n",
            "Epoch[5]: Batch[537]   Train Loss: 0.025643075924631796     Time: 1.3809075355529785\n",
            "Epoch[5]: Batch[538]   Train Loss: 0.027493652602744646     Time: 1.3886778354644775\n",
            "Epoch[5]: Batch[539]   Train Loss: 0.023460016020167434     Time: 1.3873052597045898\n",
            "Epoch[5]: Batch[540]   Train Loss: 0.02888298301378232     Time: 1.3873209953308105\n",
            "Epoch[5]: Batch[541]   Train Loss: 0.026332227447240723     Time: 1.3826026916503906\n",
            "Epoch[5]: Batch[542]   Train Loss: 0.0294503706441981     Time: 1.3763394355773926\n",
            "Epoch[5]: Batch[543]   Train Loss: 0.02871471152188512     Time: 1.3901984691619873\n",
            "Epoch[5]: Batch[544]   Train Loss: 0.027490192401929712     Time: 1.3902835845947266\n",
            "Epoch[5]: Batch[545]   Train Loss: 0.02769884937228598     Time: 1.3741328716278076\n",
            "Epoch[5]: Batch[546]   Train Loss: 0.02727814301505795     Time: 1.382154941558838\n",
            "Epoch[5]: Batch[547]   Train Loss: 0.02663594093609427     Time: 1.3673431873321533\n",
            "Epoch[5]: Batch[548]   Train Loss: 0.026656390284129666     Time: 1.380204439163208\n",
            "Epoch[5]: Batch[549]   Train Loss: 0.025011705228844435     Time: 1.3735277652740479\n",
            "Epoch[5]: Batch[550]   Train Loss: 0.027689096998118848     Time: 1.37320876121521\n",
            "Epoch[5]: Batch[551]   Train Loss: 0.028080491646311906     Time: 1.3816900253295898\n",
            "Epoch[5]: Batch[552]   Train Loss: 0.027288736624677186     Time: 1.3690862655639648\n",
            "Epoch[5]: Batch[553]   Train Loss: 0.027267443943039107     Time: 1.3853912353515625\n",
            "Epoch[5]: Batch[554]   Train Loss: 0.028311609371078104     Time: 1.3747925758361816\n",
            "Epoch[5]: Batch[555]   Train Loss: 0.02720194397527453     Time: 1.3823668956756592\n",
            "Epoch[5]: Batch[556]   Train Loss: 0.026958160444053744     Time: 1.375286340713501\n",
            "Epoch[5]: Batch[557]   Train Loss: 0.030609075804837585     Time: 1.3759422302246094\n",
            "Epoch[5]: Batch[558]   Train Loss: 0.028445860303938635     Time: 1.3747735023498535\n",
            "Epoch[5]: Batch[559]   Train Loss: 0.023844502074961266     Time: 1.3915750980377197\n",
            "Epoch[5]: Batch[560]   Train Loss: 0.026258174109974747     Time: 1.38033127784729\n",
            "Epoch[5]: Batch[561]   Train Loss: 0.027329419728460473     Time: 1.4014148712158203\n",
            "Epoch[5]: Batch[562]   Train Loss: 0.024983998684397463     Time: 1.3958539962768555\n",
            "Epoch[5]: Batch[563]   Train Loss: 0.02620934329148075     Time: 1.380922555923462\n",
            "Epoch[5]: Batch[564]   Train Loss: 0.028306137654387553     Time: 1.3941679000854492\n",
            "Epoch[5]: Batch[565]   Train Loss: 0.02530117036768172     Time: 1.379183292388916\n",
            "Epoch[5]: Batch[566]   Train Loss: 0.02578893912629692     Time: 1.37673020362854\n",
            "Epoch[5]: Batch[567]   Train Loss: 0.028902514094372528     Time: 1.3859248161315918\n",
            "Epoch[5]: Batch[568]   Train Loss: 0.029271695925925073     Time: 1.3767340183258057\n",
            "Epoch[5]: Batch[569]   Train Loss: 0.028909752206425526     Time: 1.3966591358184814\n",
            "Epoch[5]: Batch[570]   Train Loss: 0.027175138888405777     Time: 1.3877863883972168\n",
            "Epoch[5]: Batch[571]   Train Loss: 0.027060435513207547     Time: 1.368276596069336\n",
            "Epoch[5]: Batch[572]   Train Loss: 0.026646056165225137     Time: 1.3832643032073975\n",
            "Epoch[5]: Batch[573]   Train Loss: 0.025851469506656597     Time: 1.3766767978668213\n",
            "Epoch[5]: Batch[574]   Train Loss: 0.028967668799160174     Time: 1.3803856372833252\n",
            "Epoch[5]: Batch[575]   Train Loss: 0.026342788071674655     Time: 1.3908312320709229\n",
            "Epoch[5]: Batch[576]   Train Loss: 0.02782723403449887     Time: 1.3735830783843994\n",
            "Epoch[5]: Batch[577]   Train Loss: 0.02723276448007617     Time: 1.3734714984893799\n",
            "Epoch[5]: Batch[578]   Train Loss: 0.02657338066154578     Time: 1.3926455974578857\n",
            "Epoch[5]: Batch[579]   Train Loss: 0.02989474793361706     Time: 1.3724849224090576\n",
            "Epoch[5]: Batch[580]   Train Loss: 0.028778507645585833     Time: 1.381528377532959\n",
            "Epoch[5]: Batch[581]   Train Loss: 0.028101623247631113     Time: 1.380493402481079\n",
            "Epoch[5]: Batch[582]   Train Loss: 0.026406133162858943     Time: 1.385188102722168\n",
            "Epoch[5]: Batch[583]   Train Loss: 0.026437349486570887     Time: 1.3806257247924805\n",
            "Epoch[5]: Batch[584]   Train Loss: 0.028239570164770918     Time: 1.385010004043579\n",
            "Epoch[5]: Batch[585]   Train Loss: 0.025412776204516402     Time: 1.3750088214874268\n",
            "Epoch[5]: Batch[586]   Train Loss: 0.025715567810954944     Time: 1.3868539333343506\n",
            "Epoch[5]: Batch[587]   Train Loss: 0.023826309908081354     Time: 1.3780839443206787\n",
            "Epoch[5]: Batch[588]   Train Loss: 0.027301084171363222     Time: 1.3858869075775146\n",
            "Epoch[5]: Batch[589]   Train Loss: 0.02669072248330508     Time: 1.3832333087921143\n",
            "Epoch[5]: Batch[590]   Train Loss: 0.028329845477491556     Time: 1.3849377632141113\n",
            "Epoch[5]: Batch[591]   Train Loss: 0.026548226848830733     Time: 1.3862626552581787\n",
            "Epoch[5]: Batch[592]   Train Loss: 0.026284692266923017     Time: 1.3751325607299805\n",
            "Epoch[5]: Batch[593]   Train Loss: 0.027483316569004594     Time: 1.3895633220672607\n",
            "Epoch[5]: Batch[594]   Train Loss: 0.02794739336061897     Time: 1.383772611618042\n",
            "Epoch[5]: Batch[595]   Train Loss: 0.029694094669137864     Time: 1.3829097747802734\n",
            "Epoch[5]: Batch[596]   Train Loss: 0.029611364032329304     Time: 1.3771891593933105\n",
            "Epoch[5]: Batch[597]   Train Loss: 0.024918548946525412     Time: 1.3747053146362305\n",
            "Epoch[5]: Batch[598]   Train Loss: 0.02573092278509816     Time: 1.3831679821014404\n",
            "Epoch[5]: Batch[599]   Train Loss: 0.02835387436373123     Time: 1.3696568012237549\n",
            "Epoch[5]: Batch[600]   Train Loss: 0.027401657853131565     Time: 1.376070261001587\n",
            "Epoch[5]: Batch[601]   Train Loss: 0.027661986957941035     Time: 1.3714516162872314\n",
            "Epoch[5]: Batch[602]   Train Loss: 0.028569876605923895     Time: 1.380565881729126\n",
            "Epoch[5]: Batch[603]   Train Loss: 0.025111781748658902     Time: 1.377943754196167\n",
            "Epoch[5]: Batch[604]   Train Loss: 0.023817544416807633     Time: 1.3755345344543457\n",
            "Epoch[5]: Batch[605]   Train Loss: 0.030407975491883187     Time: 1.3810412883758545\n",
            "Epoch[5]: Batch[606]   Train Loss: 0.029147510226415055     Time: 1.3726515769958496\n",
            "Epoch[5]: Batch[607]   Train Loss: 0.025511922371460227     Time: 1.380479335784912\n",
            "Epoch[5]: Batch[608]   Train Loss: 0.026326133926429247     Time: 1.3717966079711914\n",
            "Epoch[5]: Batch[609]   Train Loss: 0.025893588217103428     Time: 1.3785061836242676\n",
            "Epoch[5]: Batch[610]   Train Loss: 0.02960961552010735     Time: 1.3730442523956299\n",
            "Epoch[5]: Batch[611]   Train Loss: 0.025154648889094505     Time: 1.3811123371124268\n",
            "Epoch[5]: Batch[612]   Train Loss: 0.02386331229800147     Time: 1.3723912239074707\n",
            "Epoch[5]: Batch[613]   Train Loss: 0.02863043746223976     Time: 1.3796522617340088\n",
            "Epoch[5]: Batch[614]   Train Loss: 0.02719439402324592     Time: 1.3840041160583496\n",
            "Epoch[5]: Batch[615]   Train Loss: 0.030998835601876832     Time: 1.393998622894287\n",
            "Epoch[5]: Batch[616]   Train Loss: 0.02619089325651959     Time: 1.3811225891113281\n",
            "Epoch[5]: Batch[617]   Train Loss: 0.027097378972039766     Time: 1.3696608543395996\n",
            "Epoch[5]: Batch[618]   Train Loss: 0.029120120726053182     Time: 1.381113052368164\n",
            "Epoch[5]: Batch[619]   Train Loss: 0.02757849068396742     Time: 1.3744111061096191\n",
            "Epoch[5]: Batch[620]   Train Loss: 0.029253379898118537     Time: 1.3720805644989014\n",
            "Epoch[5]: Batch[621]   Train Loss: 0.028162949080996286     Time: 1.372495174407959\n",
            "Epoch[5]: Batch[622]   Train Loss: 0.025418057163657852     Time: 1.3783135414123535\n",
            "Epoch[5]: Batch[623]   Train Loss: 0.02409300327178796     Time: 1.4064462184906006\n",
            "Epoch[5]: Batch[624]   Train Loss: 0.02357528517449724     Time: 1.372560739517212\n",
            "Epoch[5]: Batch[625]   Train Loss: 0.02638006797626824     Time: 1.3692729473114014\n",
            "Epoch[5]: Batch[626]   Train Loss: 0.0255330365807642     Time: 1.3740568161010742\n",
            "Epoch[5]: Batch[627]   Train Loss: 0.027442431716148144     Time: 1.367659091949463\n",
            "Epoch[5]: Batch[628]   Train Loss: 0.0283013797467278     Time: 1.3745291233062744\n",
            "Epoch[5]: Batch[629]   Train Loss: 0.027096684524278887     Time: 1.3751716613769531\n",
            "Epoch[5]: Batch[630]   Train Loss: 0.025469628808298037     Time: 1.367563247680664\n",
            "Epoch[5]: Batch[631]   Train Loss: 0.028100238771513318     Time: 1.361586570739746\n",
            "Epoch[5]: Batch[632]   Train Loss: 0.027974202884286637     Time: 1.365065574645996\n",
            "Epoch[5]: Batch[633]   Train Loss: 0.024432588086409357     Time: 1.3721158504486084\n",
            "Epoch[5]: Batch[634]   Train Loss: 0.024370271555480313     Time: 1.375838041305542\n",
            "Epoch[5]: Batch[635]   Train Loss: 0.024997506096042642     Time: 1.3754377365112305\n",
            "Epoch[5]: Batch[636]   Train Loss: 0.025585026098971186     Time: 1.374173641204834\n",
            "Epoch[5]: Batch[637]   Train Loss: 0.026368492803047497     Time: 1.386625051498413\n",
            "Epoch[5]: Batch[638]   Train Loss: 0.025486622361799428     Time: 1.3739821910858154\n",
            "Epoch[5]: Batch[639]   Train Loss: 0.026520564295372903     Time: 1.371757984161377\n",
            "Epoch[5]: Batch[640]   Train Loss: 0.026294000061613334     Time: 1.3755683898925781\n",
            "Epoch[5]: Batch[641]   Train Loss: 0.02969241097455714     Time: 1.374803066253662\n",
            "Epoch[5]: Batch[642]   Train Loss: 0.02532047493940266     Time: 1.37587571144104\n",
            "Epoch[5]: Batch[643]   Train Loss: 0.02415572869776069     Time: 1.3750197887420654\n",
            "Epoch[5]: Batch[644]   Train Loss: 0.025667211525353843     Time: 1.38236403465271\n",
            "Epoch[5]: Batch[645]   Train Loss: 0.026405881803210775     Time: 1.3723082542419434\n",
            "Epoch[5]: Batch[646]   Train Loss: 0.025421324119652065     Time: 1.3797507286071777\n",
            "Epoch[5]: Batch[647]   Train Loss: 0.024829703356294384     Time: 1.3732664585113525\n",
            "Epoch[5]: Batch[648]   Train Loss: 0.025885584044981397     Time: 1.3826897144317627\n",
            "Epoch[5]: Batch[649]   Train Loss: 0.028196251889894407     Time: 1.3755102157592773\n",
            "Epoch[5]: Batch[650]   Train Loss: 0.024233007158798565     Time: 1.3816297054290771\n",
            "Epoch[5]: Batch[651]   Train Loss: 0.026676664164823023     Time: 1.3856210708618164\n",
            "Epoch[5]: Batch[652]   Train Loss: 0.029860280993723003     Time: 1.3748152256011963\n",
            "Epoch[5]: Batch[653]   Train Loss: 0.026364140488916037     Time: 1.3852801322937012\n",
            "Epoch[5]: Batch[654]   Train Loss: 0.028958888803531323     Time: 1.3718547821044922\n",
            "Epoch[5]: Batch[655]   Train Loss: 0.028345988812096674     Time: 1.3936047554016113\n",
            "Epoch[5]: Batch[656]   Train Loss: 0.02405970863937034     Time: 1.3722126483917236\n",
            "Epoch[5]: Batch[657]   Train Loss: 0.02463342932589693     Time: 1.389559268951416\n",
            "Epoch[5]: Batch[658]   Train Loss: 0.023907927393253953     Time: 1.385953664779663\n",
            "Epoch[5]: Batch[659]   Train Loss: 0.02422678058485258     Time: 1.3766398429870605\n",
            "Epoch[5]: Batch[660]   Train Loss: 0.025781368410368398     Time: 1.37735915184021\n",
            "Epoch[5]: Batch[661]   Train Loss: 0.027617324985545862     Time: 1.37420654296875\n",
            "Epoch[5]: Batch[662]   Train Loss: 0.02580732288851434     Time: 1.3750901222229004\n",
            "Epoch[5]: Batch[663]   Train Loss: 0.025574450090038617     Time: 1.371694564819336\n",
            "Epoch[5]: Batch[664]   Train Loss: 0.02348113384392467     Time: 1.3814947605133057\n",
            "Epoch[5]: Batch[665]   Train Loss: 0.024139191299453484     Time: 1.3643717765808105\n",
            "Epoch[5]: Batch[666]   Train Loss: 0.026541869234102174     Time: 1.3974025249481201\n",
            "Epoch[5]: Batch[667]   Train Loss: 0.02666508043891721     Time: 1.3807952404022217\n",
            "Epoch[5]: Batch[668]   Train Loss: 0.02636792888495258     Time: 1.3767976760864258\n",
            "Epoch[5]: Batch[669]   Train Loss: 0.027406907888724064     Time: 1.3875377178192139\n",
            "Epoch[5]: Batch[670]   Train Loss: 0.0256822893743469     Time: 1.371828317642212\n",
            "Epoch[5]: Batch[671]   Train Loss: 0.023251686277435665     Time: 1.3757052421569824\n",
            "Epoch[5]: Batch[672]   Train Loss: 0.027526358178196993     Time: 1.3698139190673828\n",
            "Epoch[5]: Batch[673]   Train Loss: 0.02920483386163353     Time: 1.3773882389068604\n",
            "Epoch[5]: Batch[674]   Train Loss: 0.026706352730031684     Time: 1.3697481155395508\n",
            "Epoch[5]: Batch[675]   Train Loss: 0.028795369949670704     Time: 1.382045030593872\n",
            "Epoch[5]: Batch[676]   Train Loss: 0.025004321282590714     Time: 1.3719582557678223\n",
            "Epoch[5]: Batch[677]   Train Loss: 0.026914975119601506     Time: 1.3734705448150635\n",
            "Epoch[5]: Batch[678]   Train Loss: 0.024726572925830352     Time: 1.3762645721435547\n",
            "Epoch[5]: Batch[679]   Train Loss: 0.027747746941775227     Time: 1.370312213897705\n",
            "Epoch[5]: Batch[680]   Train Loss: 0.02510864767292695     Time: 1.39607834815979\n",
            "Epoch[5]: Batch[681]   Train Loss: 0.02696056716630133     Time: 1.3739385604858398\n",
            "Epoch[5]: Batch[682]   Train Loss: 0.02677467285306443     Time: 1.3893482685089111\n",
            "Epoch[5]: Batch[683]   Train Loss: 0.028278705939773188     Time: 1.3838403224945068\n",
            "Epoch[5]: Batch[684]   Train Loss: 0.025358232352601588     Time: 1.37105393409729\n",
            "Epoch[5]: Batch[685]   Train Loss: 0.02765883459296237     Time: 1.3907008171081543\n",
            "Epoch[5]: Batch[686]   Train Loss: 0.02322068591596459     Time: 1.3862991333007812\n",
            "Epoch[5]: Batch[687]   Train Loss: 0.02518884272127364     Time: 1.3844034671783447\n",
            "Epoch[5]: Batch[688]   Train Loss: 0.02729541594754194     Time: 1.3722515106201172\n",
            "Epoch[5]: Batch[689]   Train Loss: 0.026623472456180526     Time: 1.385453462600708\n",
            "Epoch[5]: Batch[690]   Train Loss: 0.024061714863278273     Time: 1.374079704284668\n",
            "Epoch[5]: Batch[691]   Train Loss: 0.026610804842630457     Time: 1.3761801719665527\n",
            "Epoch[5]: Batch[692]   Train Loss: 0.023815067318081824     Time: 1.3826415538787842\n",
            "Epoch[5]: Batch[693]   Train Loss: 0.023905001509118003     Time: 1.3756890296936035\n",
            "Epoch[5]: Batch[694]   Train Loss: 0.025443304695156294     Time: 1.3685472011566162\n",
            "Epoch[5]: Batch[695]   Train Loss: 0.02480752731256004     Time: 1.3737595081329346\n",
            "Epoch[5]: Batch[696]   Train Loss: 0.027079397044380588     Time: 1.3779423236846924\n",
            "Epoch[5]: Batch[697]   Train Loss: 0.024986592330759127     Time: 1.3752334117889404\n",
            "Epoch[5]: Batch[698]   Train Loss: 0.029409922737465784     Time: 1.400838851928711\n",
            "Epoch[5]: Batch[699]   Train Loss: 0.027632037130003926     Time: 1.3723983764648438\n",
            "Epoch[5]: Batch[700]   Train Loss: 0.02727319292638022     Time: 1.382030725479126\n",
            "Epoch[5]: Batch[701]   Train Loss: 0.02749081300800481     Time: 1.3985755443572998\n",
            "Epoch[5]: Batch[702]   Train Loss: 0.02767037135673313     Time: 1.3747367858886719\n",
            "Epoch[5]: Batch[703]   Train Loss: 0.02581823181786089     Time: 1.3881335258483887\n",
            "Epoch[5]: Batch[704]   Train Loss: 0.027406387676970727     Time: 1.371910572052002\n",
            "Epoch[5]: Batch[705]   Train Loss: 0.028237857898390516     Time: 1.3831679821014404\n",
            "Epoch[5]: Batch[706]   Train Loss: 0.02418747233783615     Time: 1.384382724761963\n",
            "Epoch[5]: Batch[707]   Train Loss: 0.026296009470064607     Time: 1.3833799362182617\n",
            "Epoch[5]: Batch[708]   Train Loss: 0.025395737070042907     Time: 1.3909540176391602\n",
            "Epoch[5]: Batch[709]   Train Loss: 0.026456047741413025     Time: 1.3903915882110596\n",
            "Epoch[5]: Batch[710]   Train Loss: 0.027328785145849247     Time: 1.3906817436218262\n",
            "Epoch[5]: Batch[711]   Train Loss: 0.02577464962093669     Time: 1.3807437419891357\n",
            "Epoch[5]: Batch[712]   Train Loss: 0.027633208630046917     Time: 1.3782799243927002\n",
            "Epoch[5]: Batch[713]   Train Loss: 0.02531051827652261     Time: 1.3695852756500244\n",
            "Epoch[5]: Batch[714]   Train Loss: 0.027145551247403022     Time: 1.3867180347442627\n",
            "Epoch[5]: Batch[715]   Train Loss: 0.026575547801332835     Time: 1.3735482692718506\n",
            "Epoch[5]: Batch[716]   Train Loss: 0.026077157057125634     Time: 1.3771960735321045\n",
            "Epoch[5]: Batch[717]   Train Loss: 0.024055913239529315     Time: 1.3948347568511963\n",
            "Epoch[5]: Batch[718]   Train Loss: 0.02682258574573569     Time: 1.380824089050293\n",
            "Epoch[5]: Batch[719]   Train Loss: 0.02682029542662288     Time: 1.3876533508300781\n",
            "Epoch[5]: Batch[720]   Train Loss: 0.027218843698590154     Time: 1.3687803745269775\n",
            "Epoch[5]: Batch[721]   Train Loss: 0.028896283370564557     Time: 1.378758192062378\n",
            "Epoch[5]: Batch[722]   Train Loss: 0.032640593566691425     Time: 1.3716647624969482\n",
            "Epoch[5]: Batch[723]   Train Loss: 0.027512723951883276     Time: 1.3700954914093018\n",
            "Epoch[5]: Batch[724]   Train Loss: 0.025797514783883098     Time: 1.3879368305206299\n",
            "Epoch[5]: Batch[725]   Train Loss: 0.027507329404201378     Time: 1.3676722049713135\n",
            "Epoch[5]: Batch[726]   Train Loss: 0.026451970621619143     Time: 1.3867802619934082\n",
            "Epoch[5]: Batch[727]   Train Loss: 0.02324873599487305     Time: 1.3709096908569336\n",
            "Epoch[5]: Batch[728]   Train Loss: 0.02935776407878217     Time: 1.3837060928344727\n",
            "Epoch[5]: Batch[729]   Train Loss: 0.026540687521657284     Time: 1.37168550491333\n",
            "Epoch[5]: Batch[730]   Train Loss: 0.027608440327447543     Time: 1.3709242343902588\n",
            "Epoch[5]: Batch[731]   Train Loss: 0.02481927898034588     Time: 1.3715832233428955\n",
            "Epoch[5]: Batch[732]   Train Loss: 0.026439548658140626     Time: 1.3673157691955566\n",
            "Epoch[5]: Batch[733]   Train Loss: 0.027527906578351417     Time: 1.382915735244751\n",
            "Epoch[5]: Batch[734]   Train Loss: 0.026465139998964566     Time: 1.3713743686676025\n",
            "Epoch[5]: Batch[735]   Train Loss: 0.025112860147509304     Time: 1.3738620281219482\n",
            "Epoch[5]: Batch[736]   Train Loss: 0.02731180134945638     Time: 1.365145206451416\n",
            "Epoch[5]: Batch[737]   Train Loss: 0.026724817188360055     Time: 1.3801116943359375\n",
            "Epoch[5]: Batch[738]   Train Loss: 0.02565316486782712     Time: 1.37013840675354\n",
            "Epoch[5]: Batch[739]   Train Loss: 0.02639852382586755     Time: 1.3672819137573242\n",
            "Epoch[5]: Batch[740]   Train Loss: 0.023111009607838674     Time: 1.3769621849060059\n",
            "Epoch[5]: Batch[741]   Train Loss: 0.028556985338512284     Time: 1.3704209327697754\n",
            "Epoch[5]: Batch[742]   Train Loss: 0.026954214195047732     Time: 1.3781154155731201\n",
            "Epoch[5]: Batch[743]   Train Loss: 0.027363842517849027     Time: 1.3675475120544434\n",
            "Epoch[5]: Batch[744]   Train Loss: 0.02864489413659017     Time: 1.3775205612182617\n",
            "Epoch[5]: Batch[745]   Train Loss: 0.027787441049594325     Time: 1.3815295696258545\n",
            "Epoch[5]: Batch[746]   Train Loss: 0.028433229034548896     Time: 1.3891513347625732\n",
            "Epoch[5]: Batch[747]   Train Loss: 0.030411491672837465     Time: 1.375061273574829\n",
            "Epoch[5]: Batch[748]   Train Loss: 0.029228817157187243     Time: 1.3698351383209229\n",
            "Epoch[5]: Batch[749]   Train Loss: 0.031132472674768114     Time: 1.3922920227050781\n",
            "Epoch[5]: Batch[750]   Train Loss: 0.02634869648190266     Time: 1.3730249404907227\n",
            "Epoch[5]: Batch[751]   Train Loss: 0.026030981638009213     Time: 1.3918843269348145\n",
            "Epoch[5]: Batch[752]   Train Loss: 0.026639758062321     Time: 1.375471591949463\n",
            "Epoch[5]: Batch[753]   Train Loss: 0.0277115484314985     Time: 1.3871872425079346\n",
            "Epoch[5]: Batch[754]   Train Loss: 0.023828045695055918     Time: 1.3955109119415283\n",
            "Epoch[5]: Batch[755]   Train Loss: 0.026370551805373062     Time: 1.3959636688232422\n",
            "Epoch[5]: Batch[756]   Train Loss: 0.02517266926972973     Time: 1.3923346996307373\n",
            "Epoch[5]: Batch[757]   Train Loss: 0.02513387799091139     Time: 1.3790643215179443\n",
            "Epoch[5]: Batch[758]   Train Loss: 0.02565883067407595     Time: 1.3798391819000244\n",
            "Epoch[5]: Batch[759]   Train Loss: 0.027209379517275725     Time: 1.3865771293640137\n",
            "Epoch[5]: Batch[760]   Train Loss: 0.026859423989220095     Time: 1.3799591064453125\n",
            "Epoch[5]: Batch[761]   Train Loss: 0.029378394358775192     Time: 1.3813884258270264\n",
            "Epoch[5]: Batch[762]   Train Loss: 0.027283014485138268     Time: 1.390681505203247\n",
            "Epoch[5]: Batch[763]   Train Loss: 0.022780313568140826     Time: 1.367290735244751\n",
            "Epoch[5]: Batch[764]   Train Loss: 0.026868207409539213     Time: 1.3844265937805176\n",
            "Epoch[5]: Batch[765]   Train Loss: 0.02726778040042067     Time: 1.4056622982025146\n",
            "Epoch[5]: Batch[766]   Train Loss: 0.025338026210456645     Time: 1.377190113067627\n",
            "Epoch[5]: Batch[767]   Train Loss: 0.024739712131856533     Time: 1.3865036964416504\n",
            "Epoch[5]: Batch[768]   Train Loss: 0.025485767427339073     Time: 1.3700659275054932\n",
            "Epoch[5]: Batch[769]   Train Loss: 0.02565735073477302     Time: 1.3789622783660889\n",
            "Epoch[5]: Batch[770]   Train Loss: 0.02818770876375121     Time: 1.3705487251281738\n",
            "Epoch[5]: Batch[771]   Train Loss: 0.028827477850109487     Time: 1.3740935325622559\n",
            "Epoch[5]: Batch[772]   Train Loss: 0.023631721071824527     Time: 1.3658409118652344\n",
            "Epoch[5]: Batch[773]   Train Loss: 0.024355417735815916     Time: 1.3718197345733643\n",
            "Epoch[5]: Batch[774]   Train Loss: 0.02807698896653363     Time: 1.3726465702056885\n",
            "Epoch[5]: Batch[775]   Train Loss: 0.02779747929748081     Time: 1.384932279586792\n",
            "Epoch[5]: Batch[776]   Train Loss: 0.028116471508047244     Time: 1.3903124332427979\n",
            "Epoch[5]: Batch[777]   Train Loss: 0.03056530008741426     Time: 1.374814748764038\n",
            "Epoch[5]: Batch[778]   Train Loss: 0.027373369008899147     Time: 1.4057865142822266\n",
            "Epoch[5]: Batch[779]   Train Loss: 0.023637561806335052     Time: 1.3733923435211182\n",
            "Epoch[5]: Batch[780]   Train Loss: 0.02746405965040674     Time: 1.3634989261627197\n",
            "Epoch[5]: Batch[781]   Train Loss: 0.02450938378497103     Time: 1.3736627101898193\n",
            "Epoch[5]: Batch[782]   Train Loss: 0.021848218027747262     Time: 1.3653242588043213\n",
            "Epoch[5]: Batch[783]   Train Loss: 0.02756209987448843     Time: 1.3780779838562012\n",
            "Epoch[5]: Batch[784]   Train Loss: 0.022507372308041913     Time: 1.3876910209655762\n",
            "Epoch[5]: Batch[785]   Train Loss: 0.025758844306927164     Time: 1.3930532932281494\n",
            "Epoch[5]: Batch[786]   Train Loss: 0.025839093943139546     Time: 1.3806381225585938\n",
            "Epoch[5]: Batch[787]   Train Loss: 0.025146305380404462     Time: 1.3741488456726074\n",
            "Epoch[5]: Batch[788]   Train Loss: 0.025658624119546173     Time: 1.3731167316436768\n",
            "Epoch[5]: Batch[789]   Train Loss: 0.02822508779744225     Time: 1.3716697692871094\n",
            "Epoch[5]: Batch[790]   Train Loss: 0.026746916303125336     Time: 1.3737092018127441\n",
            "Epoch[5]: Batch[791]   Train Loss: 0.026790437499427642     Time: 1.3749196529388428\n",
            "Epoch[5]: Batch[792]   Train Loss: 0.02697807094159666     Time: 1.3875596523284912\n",
            "Epoch[5]: Batch[793]   Train Loss: 0.02897544338889315     Time: 1.3723297119140625\n",
            "Epoch[5]: Batch[794]   Train Loss: 0.02859464897932983     Time: 1.3815534114837646\n",
            "Epoch[5]: Batch[795]   Train Loss: 0.025822488528962465     Time: 1.3877754211425781\n",
            "Epoch[5]: Batch[796]   Train Loss: 0.026009962928051928     Time: 1.390535831451416\n",
            "Epoch[5]: Batch[797]   Train Loss: 0.024568836565783472     Time: 1.382897138595581\n",
            "Epoch[5]: Batch[798]   Train Loss: 0.025790248924231676     Time: 1.3837919235229492\n",
            "Epoch[5]: Batch[799]   Train Loss: 0.030191520108110187     Time: 1.3854255676269531\n",
            "Epoch[5]: Batch[800]   Train Loss: 0.022943224914115625     Time: 1.3783793449401855\n",
            "Epoch[5]: Batch[801]   Train Loss: 0.02611059002285579     Time: 1.3919084072113037\n",
            "Epoch[5]: Batch[802]   Train Loss: 0.02725299158010516     Time: 1.3717284202575684\n",
            "Epoch[5]: Batch[803]   Train Loss: 0.02634496662916372     Time: 1.387427806854248\n",
            "Epoch[5]: Batch[804]   Train Loss: 0.02393959074297711     Time: 1.388190746307373\n",
            "Epoch[5]: Batch[805]   Train Loss: 0.02401541542357404     Time: 1.3774244785308838\n",
            "Epoch[5]: Batch[806]   Train Loss: 0.024844195397745295     Time: 1.3732616901397705\n",
            "Epoch[5]: Batch[807]   Train Loss: 0.0306731343454833     Time: 1.3738009929656982\n",
            "Epoch[5]: Batch[808]   Train Loss: 0.024731774997946494     Time: 1.373335838317871\n",
            "Epoch[5]: Batch[809]   Train Loss: 0.026655235003227966     Time: 1.3881170749664307\n",
            "Epoch[5]: Batch[810]   Train Loss: 0.027041890053560034     Time: 1.3959262371063232\n",
            "Epoch[5]: Batch[811]   Train Loss: 0.0270020734056771     Time: 1.3712189197540283\n",
            "Epoch[5]: Batch[812]   Train Loss: 0.025557205619225674     Time: 1.3847935199737549\n",
            "Epoch[5]: Batch[813]   Train Loss: 0.028611170798656195     Time: 1.4005022048950195\n",
            "Epoch[5]: Batch[814]   Train Loss: 0.025517835616928688     Time: 1.3784751892089844\n",
            "Epoch[5]: Batch[815]   Train Loss: 0.02759382515687107     Time: 1.3887834548950195\n",
            "Epoch[5]: Batch[816]   Train Loss: 0.02468429453124144     Time: 1.380300760269165\n",
            "Epoch[5]: Batch[817]   Train Loss: 0.028626434555951544     Time: 1.3880116939544678\n",
            "Epoch[5]: Batch[818]   Train Loss: 0.026728656101465006     Time: 1.3705785274505615\n",
            "Epoch[5]: Batch[819]   Train Loss: 0.022081210390926597     Time: 1.3817391395568848\n",
            "Epoch[5]: Batch[820]   Train Loss: 0.03008758233757253     Time: 1.3700220584869385\n",
            "Epoch[5]: Batch[821]   Train Loss: 0.023476488890157035     Time: 1.3849971294403076\n",
            "Epoch[5]: Batch[822]   Train Loss: 0.02337710070277964     Time: 1.3780648708343506\n",
            "Epoch[5]: Batch[823]   Train Loss: 0.030530439979935854     Time: 1.3675134181976318\n",
            "Epoch[5]: Batch[824]   Train Loss: 0.02303534999107835     Time: 1.3842875957489014\n",
            "Epoch[5]: Batch[825]   Train Loss: 0.025802259962435     Time: 1.3797898292541504\n",
            "Epoch[5]: Batch[826]   Train Loss: 0.023684349979050855     Time: 1.3807034492492676\n",
            "Epoch[5]: Batch[827]   Train Loss: 0.034051191291089966     Time: 1.3895361423492432\n",
            "Epoch[5]: Batch[828]   Train Loss: 0.028172617951059006     Time: 1.3747382164001465\n",
            "Epoch[5]: Batch[829]   Train Loss: 0.026653220942339594     Time: 1.3799235820770264\n",
            "Epoch[5]: Batch[830]   Train Loss: 0.02742518249622114     Time: 1.3928821086883545\n",
            "Epoch[5]: Batch[831]   Train Loss: 0.02768437145345642     Time: 1.378715991973877\n",
            "Epoch[5]: Batch[832]   Train Loss: 0.026206793657593704     Time: 1.3735008239746094\n",
            "Epoch[5]: Batch[833]   Train Loss: 0.026877901928352783     Time: 1.3737525939941406\n",
            "Epoch[5]: Batch[834]   Train Loss: 0.02625395881165849     Time: 1.3898735046386719\n",
            "Epoch[5]: Batch[835]   Train Loss: 0.0255581883951488     Time: 1.3758292198181152\n",
            "Epoch[5]: Batch[836]   Train Loss: 0.027790348400505697     Time: 1.3762645721435547\n",
            "Epoch[5]: Batch[837]   Train Loss: 0.023402619277323584     Time: 1.3800876140594482\n",
            "Epoch[5]: Batch[838]   Train Loss: 0.027350043120352194     Time: 1.3902673721313477\n",
            "Epoch[5]: Batch[839]   Train Loss: 0.027410647507519807     Time: 1.3684077262878418\n",
            "Epoch[5]: Batch[840]   Train Loss: 0.02625315291185017     Time: 1.3792355060577393\n",
            "Epoch[5]: Batch[841]   Train Loss: 0.025362103710256944     Time: 1.3683843612670898\n",
            "Epoch[5]: Batch[842]   Train Loss: 0.029558887630866955     Time: 1.37172532081604\n",
            "Epoch[5]: Batch[843]   Train Loss: 0.022888018966383192     Time: 1.369821548461914\n",
            "Epoch[5]: Batch[844]   Train Loss: 0.027627182367911503     Time: 1.3677089214324951\n",
            "Epoch[5]: Batch[845]   Train Loss: 0.024302059801696516     Time: 1.3817689418792725\n",
            "Epoch[5]: Batch[846]   Train Loss: 0.02919147076864721     Time: 1.3712797164916992\n",
            "Epoch[5]: Batch[847]   Train Loss: 0.030357229317391616     Time: 1.3794350624084473\n",
            "Epoch[5]: Batch[848]   Train Loss: 0.02770106171786051     Time: 1.370326042175293\n",
            "Epoch[5]: Batch[849]   Train Loss: 0.02347189651925956     Time: 1.3652825355529785\n",
            "Epoch[5]: Batch[850]   Train Loss: 0.02606786813978524     Time: 1.373103380203247\n",
            "Epoch[5]: Batch[851]   Train Loss: 0.02498436153470084     Time: 1.3665030002593994\n",
            "Epoch[5]: Batch[852]   Train Loss: 0.0271939633058096     Time: 1.3823449611663818\n",
            "Epoch[5]: Batch[853]   Train Loss: 0.02088838347883642     Time: 1.3853638172149658\n",
            "Epoch[5]: Batch[854]   Train Loss: 0.029049403144762335     Time: 1.3855652809143066\n",
            "Epoch[5]: Batch[855]   Train Loss: 0.024780826411538275     Time: 1.3716778755187988\n",
            "Epoch[5]: Batch[856]   Train Loss: 0.02712634998766691     Time: 1.3914790153503418\n",
            "Epoch[5]: Batch[857]   Train Loss: 0.03028447293052503     Time: 1.3707683086395264\n",
            "Epoch[5]: Batch[858]   Train Loss: 0.024145503155938428     Time: 1.3951222896575928\n",
            "Epoch[5]: Batch[859]   Train Loss: 0.02977652199690484     Time: 1.3944411277770996\n",
            "Epoch[5]: Batch[860]   Train Loss: 0.028457945757183155     Time: 1.3796331882476807\n",
            "Epoch[5]: Batch[861]   Train Loss: 0.02548826803538947     Time: 1.3768155574798584\n",
            "Epoch[5]: Batch[862]   Train Loss: 0.026749369918604143     Time: 1.3767969608306885\n",
            "Epoch[5]: Batch[863]   Train Loss: 0.028086431856328888     Time: 1.397775411605835\n",
            "Epoch[5]: Batch[864]   Train Loss: 0.026332485340576868     Time: 1.3713784217834473\n",
            "Epoch[5]: Batch[865]   Train Loss: 0.027592961890671886     Time: 1.381246566772461\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OC20WAQo_SL8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}