{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Inpainting.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbo8nMqnYW12",
        "outputId": "18481758-fe52-4eb7-c5fd-011af40723b4"
      },
      "source": [
        "!pip3 install pyprind"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyprind\n",
            "  Downloading PyPrind-2.11.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: pyprind\n",
            "Successfully installed pyprind-2.11.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJ-qCvdkWNBs",
        "outputId": "852224ac-9ac5-4462-c7a9-b9eef45d0170"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Owqy93gWLVj"
      },
      "source": [
        "PATH = \"/content/drive/MyDrive/Projects/Clubs/Analytics/Coord Projects/Model Zoo/Inpainting/datasets\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxSubVRDW7Jd"
      },
      "source": [
        "#!wget https://image-net.org/data/decathlon-1.0-data-imagenet.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skNAJKYaeD4W"
      },
      "source": [
        "#!tar -xvf \"/content/decathlon-1.0-data-imagenet.tar\" -C \"/content/drive/MyDrive/Projects/Clubs/Analytics/Coord Projects/Model Zoo/Inpainting/datasets/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mEQPxlzWRVS"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VgmqNpHNHxO"
      },
      "source": [
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYPBkWJGUz7q"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os, glob\n",
        "\n",
        "\n",
        "class CreateDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, PATH, dataset, mode='train', sub_folder=True, img_size=256):\n",
        "        self.PATH = PATH\n",
        "        self.dataset = dataset\n",
        "        self.mode = mode\n",
        "        self.img_size = 256\n",
        "        self.images = np.array([])\n",
        "        self.normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        if mode=='train':\n",
        "            self.size = 16384*2\n",
        "            self.transform = True\n",
        "        elif mode=='val':\n",
        "            self.size = 1024\n",
        "            self.transform = True\n",
        "        elif mode=='test':\n",
        "            self.size = 32\n",
        "            self.transform = False\n",
        "\n",
        "        if sub_folder:\n",
        "            directories = self.find_sub_folders(os.path.join(self.PATH, self.dataset, self.mode))\n",
        "            for directory in directories:\n",
        "                entries = [ os.path.basename(entry) for entry in glob.glob(os.path.join(self.PATH, self.dataset, self.mode, directory, \"*.jpg\")) ]\n",
        "                paths = [os.path.join(self.PATH, self.dataset, self.mode, directory, entry) for entry in entries]\n",
        "                self.images = np.append(self.images, paths)\n",
        "        else:\n",
        "            entries = [ os.path.basename(entry) for entry in glob.glob(os.path.join(self.PATH, self.dataset, self.mode, \"*.jpg\")) ]\n",
        "            paths = [os.path.join(self.PATH, self.dataset, self.mode, entry) for entry in entries]\n",
        "            self.images = np.append(self.images, paths)\n",
        "\n",
        "        np.random.shuffle(self.images)\n",
        "        self.images = self.images[:self.size]\n",
        "\n",
        "    def find_sub_folders(self, directory):\n",
        "        directories = [dir for dir in os.listdir(directory) if os.path.isdir(os.path.join(directory, dir))]\n",
        "        return directories\n",
        "\n",
        "    def image_transform(self, image):\n",
        "        image = np.array(image)/255.\n",
        "        image = image.transpose((2, 0, 1))\n",
        "        if self.transform:\n",
        "            image = self.normalize(torch.from_numpy(image.copy()))\n",
        "        return image\n",
        "    \n",
        "    def image_detransform(self, image):\n",
        "        image = image.numpy()\n",
        "        if image.shape[0] == 3:\n",
        "            image = np.moveaxis(image, 0, -1)\n",
        "        return image*255\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = cv2.imread(self.images[index], cv2.IMREAD_COLOR)\n",
        "        image = cv2.resize(image, (self.img_size, self.img_size), interpolation=cv2.INTER_AREA)\n",
        "        image = self.image_transform(image)\n",
        "\n",
        "        return image\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVtuzje8WaEd"
      },
      "source": [
        "train_data = CreateDataset(PATH, \"imagenet12\", mode='train', sub_folder=True)\n",
        "val_data = CreateDataset(PATH, \"imagenet12\", mode='val', sub_folder=True)\n",
        "test_data = CreateDataset(PATH, \"imagenet12\", mode='test', sub_folder=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-o9ypx2Ok9q",
        "outputId": "2022448e-40c6-49d5-9995-505d4e19f29a"
      },
      "source": [
        "print(len(train_data))\n",
        "print(len(val_data))\n",
        "print(len(test_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32768\n",
            "1024\n",
            "32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJH28eIfnxhJ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpV2ZjwenyV2"
      },
      "source": [
        "def extract_image_patches(images, ksizes, strides, rates, padding='same'):\n",
        "\n",
        "    assert len(images.size()) == 4\n",
        "    assert padding in ['same', 'valid']\n",
        "    batch_size, channel, height, width = images.size()\n",
        "\n",
        "    if padding == 'same':\n",
        "        images = same_padding(images, ksizes, strides, rates)\n",
        "    elif padding == 'valid':\n",
        "        pass\n",
        "    else:\n",
        "        raise NotImplementedError('Unsupported padding type: {}.\\\n",
        "                Only \"same\" or \"valid\" are supported.'.format(padding))\n",
        "\n",
        "    unfold = torch.nn.Unfold(kernel_size=ksizes,\n",
        "                             dilation=rates,\n",
        "                             padding=0,\n",
        "                             stride=strides)\n",
        "    patches = unfold(images)\n",
        "    return patches  # [N, C*k*k, L], L is the total number of such blocks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIwwPLezn9fG"
      },
      "source": [
        "def same_padding(images, ksizes, strides, rates):\n",
        "    assert len(images.size()) == 4\n",
        "    batch_size, channel, rows, cols = images.size()\n",
        "    out_rows = (rows + strides[0] - 1) // strides[0]\n",
        "    out_cols = (cols + strides[1] - 1) // strides[1]\n",
        "    effective_k_row = (ksizes[0] - 1) * rates[0] + 1\n",
        "    effective_k_col = (ksizes[1] - 1) * rates[1] + 1\n",
        "    padding_rows = max(0, (out_rows-1)*strides[0]+effective_k_row-rows)\n",
        "    padding_cols = max(0, (out_cols-1)*strides[1]+effective_k_col-cols)\n",
        "    # Pad the input\n",
        "    padding_top = int(padding_rows / 2.)\n",
        "    padding_left = int(padding_cols / 2.)\n",
        "    padding_bottom = padding_rows - padding_top\n",
        "    padding_right = padding_cols - padding_left\n",
        "    paddings = (padding_left, padding_right, padding_top, padding_bottom)\n",
        "    images = torch.nn.ZeroPad2d(paddings)(images)\n",
        "    return images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xJqmHj5oFp7"
      },
      "source": [
        "def reduce_mean(x, axis=None, keepdim=False):\n",
        "    if not axis:\n",
        "        axis = range(len(x.shape))\n",
        "    for i in sorted(axis, reverse=True):\n",
        "        x = torch.mean(x, dim=i, keepdim=keepdim)\n",
        "    return x\n",
        "\n",
        "\n",
        "def reduce_std(x, axis=None, keepdim=False):\n",
        "    if not axis:\n",
        "        axis = range(len(x.shape))\n",
        "    for i in sorted(axis, reverse=True):\n",
        "        x = torch.std(x, dim=i, keepdim=keepdim)\n",
        "    return x\n",
        "\n",
        "\n",
        "def reduce_sum(x, axis=None, keepdim=False):\n",
        "    if not axis:\n",
        "        axis = range(len(x.shape))\n",
        "    for i in sorted(axis, reverse=True):\n",
        "        x = torch.sum(x, dim=i, keepdim=keepdim)\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-DLx3i3ogsD"
      },
      "source": [
        "def make_color_wheel():\n",
        "    RY, YG, GC, CB, BM, MR = (15, 6, 4, 11, 13, 6)\n",
        "    ncols = RY + YG + GC + CB + BM + MR\n",
        "    colorwheel = np.zeros([ncols, 3])\n",
        "    col = 0\n",
        "    # RY\n",
        "    colorwheel[0:RY, 0] = 255\n",
        "    colorwheel[0:RY, 1] = np.transpose(np.floor(255 * np.arange(0, RY) / RY))\n",
        "    col += RY\n",
        "    # YG\n",
        "    colorwheel[col:col + YG, 0] = 255 - np.transpose(np.floor(255 * np.arange(0, YG) / YG))\n",
        "    colorwheel[col:col + YG, 1] = 255\n",
        "    col += YG\n",
        "    # GC\n",
        "    colorwheel[col:col + GC, 1] = 255\n",
        "    colorwheel[col:col + GC, 2] = np.transpose(np.floor(255 * np.arange(0, GC) / GC))\n",
        "    col += GC\n",
        "    # CB\n",
        "    colorwheel[col:col + CB, 1] = 255 - np.transpose(np.floor(255 * np.arange(0, CB) / CB))\n",
        "    colorwheel[col:col + CB, 2] = 255\n",
        "    col += CB\n",
        "    # BM\n",
        "    colorwheel[col:col + BM, 2] = 255\n",
        "    colorwheel[col:col + BM, 0] = np.transpose(np.floor(255 * np.arange(0, BM) / BM))\n",
        "    col += + BM\n",
        "    # MR\n",
        "    colorwheel[col:col + MR, 2] = 255 - np.transpose(np.floor(255 * np.arange(0, MR) / MR))\n",
        "    colorwheel[col:col + MR, 0] = 255\n",
        "    return colorwheel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gb4vrXFqoaCq"
      },
      "source": [
        "def compute_color(u, v):\n",
        "    h, w = u.shape\n",
        "    img = np.zeros([h, w, 3])\n",
        "    nanIdx = np.isnan(u) | np.isnan(v)\n",
        "    u[nanIdx] = 0\n",
        "    v[nanIdx] = 0\n",
        "    # colorwheel = COLORWHEEL\n",
        "    colorwheel = make_color_wheel()\n",
        "    ncols = np.size(colorwheel, 0)\n",
        "    rad = np.sqrt(u ** 2 + v ** 2)\n",
        "    a = np.arctan2(-v, -u) / np.pi\n",
        "    fk = (a + 1) / 2 * (ncols - 1) + 1\n",
        "    k0 = np.floor(fk).astype(int)\n",
        "    k1 = k0 + 1\n",
        "    k1[k1 == ncols + 1] = 1\n",
        "    f = fk - k0\n",
        "    for i in range(np.size(colorwheel, 1)):\n",
        "        tmp = colorwheel[:, i]\n",
        "        col0 = tmp[k0 - 1] / 255\n",
        "        col1 = tmp[k1 - 1] / 255\n",
        "        col = (1 - f) * col0 + f * col1\n",
        "        idx = rad <= 1\n",
        "        col[idx] = 1 - rad[idx] * (1 - col[idx])\n",
        "        notidx = np.logical_not(idx)\n",
        "        col[notidx] *= 0.75\n",
        "        img[:, :, i] = np.uint8(np.floor(255 * col * (1 - nanIdx)))\n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq9Gmz2VoL4Q"
      },
      "source": [
        "def flow_to_image(flow):\n",
        "    out = []\n",
        "    maxu = -999.\n",
        "    maxv = -999.\n",
        "    minu = 999.\n",
        "    minv = 999.\n",
        "    maxrad = -1\n",
        "    for i in range(flow.shape[0]):\n",
        "        u = flow[i, :, :, 0]\n",
        "        v = flow[i, :, :, 1]\n",
        "        idxunknow = (abs(u) > 1e7) | (abs(v) > 1e7)\n",
        "        u[idxunknow] = 0\n",
        "        v[idxunknow] = 0\n",
        "        maxu = max(maxu, np.max(u))\n",
        "        minu = min(minu, np.min(u))\n",
        "        maxv = max(maxv, np.max(v))\n",
        "        minv = min(minv, np.min(v))\n",
        "        rad = np.sqrt(u ** 2 + v ** 2)\n",
        "        maxrad = max(maxrad, np.max(rad))\n",
        "        u = u / (maxrad + np.finfo(float).eps)\n",
        "        v = v / (maxrad + np.finfo(float).eps)\n",
        "        img = compute_color(u, v)\n",
        "        out.append(img)\n",
        "    return np.float32(np.uint8(out))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wO2ab1t-aYQ"
      },
      "source": [
        "def random_bbox(batch_size, image_shape=(256,256,3), mask_shape=(128, 128), margin=(0,0), mask_batch_same=True):\n",
        "    img_height, img_width, _ = image_shape\n",
        "    h, w = mask_shape\n",
        "    margin_height, margin_width = margin\n",
        "    maxt = img_height - margin_height - h\n",
        "    maxl = img_width - margin_width - w\n",
        "    bbox_list = []\n",
        "    if mask_batch_same:\n",
        "        t = np.random.randint(margin_height, maxt)\n",
        "        l = np.random.randint(margin_width, maxl)\n",
        "        bbox_list.append((t, l, h, w))\n",
        "        bbox_list = bbox_list * batch_size\n",
        "    else:\n",
        "        for i in range(batch_size):\n",
        "            t = np.random.randint(margin_height, maxt)\n",
        "            l = np.random.randint(margin_width, maxl)\n",
        "            bbox_list.append((t, l, h, w))\n",
        "\n",
        "    return torch.tensor(bbox_list, dtype=torch.int64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-D9Bft--9aR"
      },
      "source": [
        "def bbox2mask(bboxes, height, width, max_delta_h, max_delta_w):\n",
        "    batch_size = bboxes.size(0)\n",
        "    mask = torch.zeros((batch_size, 1, height, width), dtype=torch.float32)\n",
        "    for i in range(batch_size):\n",
        "        bbox = bboxes[i]\n",
        "        delta_h = np.random.randint(max_delta_h // 2 + 1)\n",
        "        delta_w = np.random.randint(max_delta_w // 2 + 1)\n",
        "        mask[i, :, bbox[0] + delta_h:bbox[0] + bbox[2] - delta_h, bbox[1] + delta_w:bbox[1] + bbox[3] - delta_w] = 1.\n",
        "    return mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waVMzNAP_BAf"
      },
      "source": [
        "def mask_image(x, bboxes, image_shape=(256,256,3), max_delta_shape=(32,32), mask_type='hole'):\n",
        "    height, width, _ = image_shape\n",
        "    max_delta_h, max_delta_w = max_delta_shape\n",
        "    mask = bbox2mask(bboxes, height, width, max_delta_h, max_delta_w)\n",
        "    if x.is_cuda:\n",
        "        mask = mask.cuda()\n",
        "\n",
        "    if mask_type == 'hole':\n",
        "        result = x * (1. - mask)\n",
        "    elif mask_type == 'mosaic':\n",
        "        # TODO: Matching the mosaic patch size and the mask size\n",
        "        mosaic_unit_size = 12\n",
        "        downsampled_image = F.interpolate(x, scale_factor=1. / mosaic_unit_size, mode='nearest')\n",
        "        upsampled_image = F.interpolate(downsampled_image, size=(height, width), mode='nearest')\n",
        "        result = upsampled_image * mask + x * (1. - mask)\n",
        "    else:\n",
        "        raise NotImplementedError('Not implemented mask type.')\n",
        "\n",
        "    return result, mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMQP-QHe_sm6"
      },
      "source": [
        "def spatial_discounting_mask(spatial_discounting_gamma=0.9, mask_shape=(128, 128), discounted_mask=True, use_cuda=False):\n",
        "    gamma = spatial_discounting_gamma\n",
        "    height, width = mask_shape\n",
        "    shape = [1, 1, height, width]\n",
        "    if discounted_mask:\n",
        "        mask_values = np.ones((height, width))\n",
        "        for i in range(height):\n",
        "            for j in range(width):\n",
        "                mask_values[i, j] = max(\n",
        "                    gamma ** min(i, height - i),\n",
        "                    gamma ** min(j, width - j))\n",
        "        mask_values = np.expand_dims(mask_values, 0)\n",
        "        mask_values = np.expand_dims(mask_values, 0)\n",
        "    else:\n",
        "        mask_values = np.ones(shape)\n",
        "    spatial_discounting_mask_tensor = torch.tensor(mask_values, dtype=torch.float32)\n",
        "    if use_cuda:\n",
        "        spatial_discounting_mask_tensor = spatial_discounting_mask_tensor.cuda()\n",
        "    return spatial_discounting_mask_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBtI_OfBAH1F"
      },
      "source": [
        "def local_patch(x, bbox_list):\n",
        "    assert len(x.size()) == 4\n",
        "    patches = []\n",
        "    for i, bbox in enumerate(bbox_list):\n",
        "        t, l, h, w = bbox\n",
        "        patches.append(x[i, :, t:t + h, l:l + w])\n",
        "    return torch.stack(patches, dim=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ztruw0prPHgH"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYsLtxWMPJRv"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import spectral_norm as spectral_norm_fn\n",
        "from torch.nn.utils import weight_norm as weight_norm_fn\n",
        "from torchvision import transforms\n",
        "from torchvision import utils as vutils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ehqz2fDvTPIR"
      },
      "source": [
        "class Conv2dBlock(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, kernel_size, stride, padding=0,\n",
        "                 conv_padding=0, dilation=1, weight_norm='none', norm='none',\n",
        "                 activation='relu', pad_type='zero', transpose=False):\n",
        "        super(Conv2dBlock, self).__init__()\n",
        "        self.use_bias = True\n",
        "\n",
        "        if pad_type == 'reflect':\n",
        "            self.pad = nn.ReflectionPad2d(padding)\n",
        "        elif pad_type == 'replicate':\n",
        "            self.pad = nn.ReplicationPad2d(padding)\n",
        "        elif pad_type == 'zero':\n",
        "            self.pad = nn.ZeroPad2d(padding)\n",
        "        elif pad_type == 'none':\n",
        "            self.pad = None\n",
        "        else:\n",
        "            assert 0, \"Unsupported padding type: {}\".format(pad_type)\n",
        "\n",
        "        norm_dim = output_dim\n",
        "        if norm == 'bn':\n",
        "            self.norm = nn.BatchNorm2d(norm_dim)\n",
        "        elif norm == 'in':\n",
        "            self.norm = nn.InstanceNorm2d(norm_dim)\n",
        "        elif norm == 'none':\n",
        "            self.norm = None\n",
        "        else:\n",
        "            assert 0, \"Unsupported normalization: {}\".format(norm)\n",
        "\n",
        "        if weight_norm == 'sn':\n",
        "            self.weight_norm = spectral_norm_fn\n",
        "        elif weight_norm == 'wn':\n",
        "            self.weight_norm = weight_norm_fn\n",
        "        elif weight_norm == 'none':\n",
        "            self.weight_norm = None\n",
        "        else:\n",
        "            assert 0, \"Unsupported normalization: {}\".format(weight_norm)\n",
        "\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU(inplace=True)\n",
        "        elif activation == 'elu':\n",
        "            self.activation = nn.ELU(inplace=True)\n",
        "        elif activation == 'lrelu':\n",
        "            self.activation = nn.LeakyReLU(0.2, inplace=True)\n",
        "        elif activation == 'prelu':\n",
        "            self.activation = nn.PReLU()\n",
        "        elif activation == 'selu':\n",
        "            self.activation = nn.SELU(inplace=True)\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == 'none':\n",
        "            self.activation = None\n",
        "        else:\n",
        "            assert 0, \"Unsupported activation: {}\".format(activation)\n",
        "\n",
        "        if transpose:\n",
        "            self.conv = nn.ConvTranspose2d(input_dim, output_dim,\n",
        "                                           kernel_size, stride,\n",
        "                                           padding=conv_padding,\n",
        "                                           output_padding=conv_padding,\n",
        "                                           dilation=dilation,\n",
        "                                           bias=self.use_bias)\n",
        "        else:\n",
        "            self.conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride,\n",
        "                                  padding=conv_padding, dilation=dilation,\n",
        "                                  bias=self.use_bias)\n",
        "\n",
        "        if self.weight_norm:\n",
        "            self.conv = self.weight_norm(self.conv)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.pad:\n",
        "            x = self.conv(self.pad(x))\n",
        "        else:\n",
        "            x = self.conv(x)\n",
        "        if self.norm:\n",
        "            x = self.norm(x)\n",
        "        if self.activation:\n",
        "            x = self.activation(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phcl4lrdJeXE"
      },
      "source": [
        "def gen_conv(input_dim, output_dim, kernel_size=3, stride=1, padding=0, rate=1,\n",
        "             activation='elu'):\n",
        "    return Conv2dBlock(input_dim, output_dim, kernel_size, stride,\n",
        "                       conv_padding=padding, dilation=rate,\n",
        "                       activation=activation)\n",
        "\n",
        "\n",
        "def dis_conv(input_dim, output_dim, kernel_size=5, stride=2, padding=0, rate=1,\n",
        "             activation='lrelu'):\n",
        "    return Conv2dBlock(input_dim, output_dim, kernel_size, stride,\n",
        "                       conv_padding=padding, dilation=rate,\n",
        "                       activation=activation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2hKCg1Lmsaa"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Bh261TWmqXG"
      },
      "source": [
        "class CoarseGenerator(nn.Module):\n",
        "    def __init__(self, input_dim, cnum, use_cuda=False):\n",
        "        super(CoarseGenerator, self).__init__()\n",
        "        self.use_cuda = use_cuda\n",
        "\n",
        "        self.conv1 = gen_conv(input_dim + 2, cnum, 5, 1, 2)\n",
        "        self.conv2_downsample = gen_conv(cnum, cnum*2, 3, 2, 1)\n",
        "        self.conv3 = gen_conv(cnum*2, cnum*2, 3, 1, 1)\n",
        "        self.conv4_downsample = gen_conv(cnum*2, cnum*4, 3, 2, 1)\n",
        "        self.conv5 = gen_conv(cnum*4, cnum*4, 3, 1, 1)\n",
        "        self.conv6 = gen_conv(cnum*4, cnum*4, 3, 1, 1)\n",
        "\n",
        "        self.conv7_atrous = gen_conv(cnum*4, cnum*4, 3, 1, 2, rate=2)\n",
        "        self.conv8_atrous = gen_conv(cnum*4, cnum*4, 3, 1, 4, rate=4)\n",
        "        self.conv9_atrous = gen_conv(cnum*4, cnum*4, 3, 1, 8, rate=8)\n",
        "        self.conv10_atrous = gen_conv(cnum*4, cnum*4, 3, 1, 16, rate=16)\n",
        "\n",
        "        self.conv11 = gen_conv(cnum*4, cnum*4, 3, 1, 1)\n",
        "        self.conv12 = gen_conv(cnum*4, cnum*4, 3, 1, 1)\n",
        "\n",
        "        self.conv13 = gen_conv(cnum*4, cnum*2, 3, 1, 1)\n",
        "        self.conv14 = gen_conv(cnum*2, cnum*2, 3, 1, 1)\n",
        "        self.conv15 = gen_conv(cnum*2, cnum, 3, 1, 1)\n",
        "        self.conv16 = gen_conv(cnum, cnum//2, 3, 1, 1)\n",
        "        self.conv17 = gen_conv(cnum//2, input_dim, 3, 1, 1, activation='none')\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # For indicating the boundaries of images\n",
        "        ones = torch.ones(x.size(0), 1, x.size(2), x.size(3))\n",
        "        if self.use_cuda:\n",
        "            ones = ones.cuda()\n",
        "            mask = mask.cuda()\n",
        "        # 5 x 256 x 256\n",
        "        x = self.conv1(torch.cat([x, ones, mask], dim=1))\n",
        "        x = self.conv2_downsample(x)\n",
        "        # cnum*2 x 128 x 128\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4_downsample(x)\n",
        "        # cnum*4 x 64 x 64\n",
        "        x = self.conv5(x)\n",
        "        x = self.conv6(x)\n",
        "        x = self.conv7_atrous(x)\n",
        "        x = self.conv8_atrous(x)\n",
        "        x = self.conv9_atrous(x)\n",
        "        x = self.conv10_atrous(x)\n",
        "        x = self.conv11(x)\n",
        "        x = self.conv12(x)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
        "        # cnum*2 x 128 x 128\n",
        "        x = self.conv13(x)\n",
        "        x = self.conv14(x)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
        "        # cnum x 256 x 256\n",
        "        x = self.conv15(x)\n",
        "        x = self.conv16(x)\n",
        "        x = self.conv17(x)\n",
        "        # 3 x 256 x 256\n",
        "        x_stage1 = torch.clamp(x, -1., 1.)\n",
        "\n",
        "        return x_stage1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21lnecblmv7R"
      },
      "source": [
        "class FineGenerator(nn.Module):\n",
        "    def __init__(self, input_dim, cnum, use_cuda=False):\n",
        "        super(FineGenerator, self).__init__()\n",
        "        self.use_cuda = use_cuda\n",
        "        # 3 x 256 x 256\n",
        "        self.conv1 = gen_conv(input_dim + 2, cnum, 5, 1, 2)\n",
        "        self.conv2_downsample = gen_conv(cnum, cnum, 3, 2, 1)\n",
        "        # cnum*2 x 128 x 128\n",
        "        self.conv3 = gen_conv(cnum, cnum*2, 3, 1, 1)\n",
        "        self.conv4_downsample = gen_conv(cnum*2, cnum*2, 3, 2, 1)\n",
        "        # cnum*4 x 64 x 64\n",
        "        self.conv5 = gen_conv(cnum*2, cnum*4, 3, 1, 1)\n",
        "        self.conv6 = gen_conv(cnum*4, cnum*4, 3, 1, 1)\n",
        "\n",
        "        self.conv7_atrous = gen_conv(cnum*4, cnum*4, 3, 1, 2, rate=2)\n",
        "        self.conv8_atrous = gen_conv(cnum*4, cnum*4, 3, 1, 4, rate=4)\n",
        "        self.conv9_atrous = gen_conv(cnum*4, cnum*4, 3, 1, 8, rate=8)\n",
        "        self.conv10_atrous = gen_conv(cnum*4, cnum*4, 3, 1, 16, rate=16)\n",
        "\n",
        "        # attention branch\n",
        "        # 3 x 256 x 256\n",
        "        self.pmconv1 = gen_conv(input_dim + 2, cnum, 5, 1, 2)\n",
        "        self.pmconv2_downsample = gen_conv(cnum, cnum, 3, 2, 1)\n",
        "        # cnum*2 x 128 x 128\n",
        "        self.pmconv3 = gen_conv(cnum, cnum*2, 3, 1, 1)\n",
        "        self.pmconv4_downsample = gen_conv(cnum*2, cnum*4, 3, 2, 1)\n",
        "        # cnum*4 x 64 x 64\n",
        "        self.pmconv5 = gen_conv(cnum*4, cnum*4, 3, 1, 1)\n",
        "        self.pmconv6 = gen_conv(cnum*4, cnum*4, 3, 1, 1, activation='relu')\n",
        "        self.contextul_attention = ContextualAttention(ksize=3, stride=1, rate=2, fuse_k=3, softmax_scale=10, fuse=True, use_cuda=self.use_cuda)\n",
        "        self.pmconv9 = gen_conv(cnum*4, cnum*4, 3, 1, 1)\n",
        "        self.pmconv10 = gen_conv(cnum*4, cnum*4, 3, 1, 1)\n",
        "        self.allconv11 = gen_conv(cnum*8, cnum*4, 3, 1, 1)\n",
        "        self.allconv12 = gen_conv(cnum*4, cnum*4, 3, 1, 1)\n",
        "        self.allconv13 = gen_conv(cnum*4, cnum*2, 3, 1, 1)\n",
        "        self.allconv14 = gen_conv(cnum*2, cnum*2, 3, 1, 1)\n",
        "        self.allconv15 = gen_conv(cnum*2, cnum, 3, 1, 1)\n",
        "        self.allconv16 = gen_conv(cnum, cnum//2, 3, 1, 1)\n",
        "        self.allconv17 = gen_conv(cnum//2, input_dim, 3, 1, 1, activation='none')\n",
        "\n",
        "    def forward(self, xin, x_stage1, mask):\n",
        "        x1_inpaint = x_stage1 * mask + xin * (1. - mask)\n",
        "        # For indicating the boundaries of images\n",
        "        ones = torch.ones(xin.size(0), 1, xin.size(2), xin.size(3))\n",
        "        if self.use_cuda:\n",
        "            ones = ones.cuda()\n",
        "            mask = mask.cuda()\n",
        "\n",
        "        # conv branch\n",
        "        xnow = torch.cat([x1_inpaint, ones, mask], dim=1)\n",
        "        x = self.conv1(xnow)\n",
        "        x = self.conv2_downsample(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4_downsample(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.conv6(x)\n",
        "        x = self.conv7_atrous(x)\n",
        "        x = self.conv8_atrous(x)\n",
        "        x = self.conv9_atrous(x)\n",
        "        x = self.conv10_atrous(x)\n",
        "        x_hallu = x\n",
        "        # attention branch\n",
        "        x = self.pmconv1(xnow)\n",
        "        x = self.pmconv2_downsample(x)\n",
        "        x = self.pmconv3(x)\n",
        "        x = self.pmconv4_downsample(x)\n",
        "        x = self.pmconv5(x)\n",
        "        x = self.pmconv6(x)\n",
        "        x, offset_flow = self.contextul_attention(x, x, mask)\n",
        "        x = self.pmconv9(x)\n",
        "        x = self.pmconv10(x)\n",
        "        pm = x\n",
        "        x = torch.cat([x_hallu, pm], dim=1)\n",
        "        # merge two branches\n",
        "        x = self.allconv11(x)\n",
        "        x = self.allconv12(x)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
        "        x = self.allconv13(x)\n",
        "        x = self.allconv14(x)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
        "        x = self.allconv15(x)\n",
        "        x = self.allconv16(x)\n",
        "        x = self.allconv17(x)\n",
        "        x_stage2 = torch.clamp(x, -1., 1.)\n",
        "\n",
        "        return x_stage2, offset_flow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMJ0nmGYmyOJ"
      },
      "source": [
        "class ContextualAttention(nn.Module):\n",
        "    def __init__(self, ksize=3, stride=1, rate=1, fuse_k=3, softmax_scale=10,\n",
        "                 fuse=False, use_cuda=False):\n",
        "        super(ContextualAttention, self).__init__()\n",
        "        self.ksize = ksize\n",
        "        self.stride = stride\n",
        "        self.rate = rate\n",
        "        self.fuse_k = fuse_k\n",
        "        self.softmax_scale = softmax_scale\n",
        "        self.fuse = fuse\n",
        "        self.use_cuda = use_cuda\n",
        "\n",
        "    def forward(self, f, b, mask=None):\n",
        "        # get shapes\n",
        "        raw_int_fs = list(f.size())   # b*c*h*w\n",
        "        raw_int_bs = list(b.size())   # b*c*h*w\n",
        "\n",
        "        # extract patches from background with stride and rate\n",
        "        kernel = 2 * self.rate\n",
        "        # raw_w is extracted for reconstruction\n",
        "        raw_w = extract_image_patches(b, ksizes=[kernel, kernel],\n",
        "                                      strides=[self.rate*self.stride,\n",
        "                                               self.rate*self.stride],\n",
        "                                      rates=[1, 1],\n",
        "                                      padding='same') # [N, C*k*k, L]\n",
        "        # raw_shape: [N, C, k, k, L]\n",
        "        raw_w = raw_w.view(raw_int_bs[0], raw_int_bs[1], kernel, kernel, -1)\n",
        "        raw_w = raw_w.permute(0, 4, 1, 2, 3)    # raw_shape: [N, L, C, k, k]\n",
        "        raw_w_groups = torch.split(raw_w, 1, dim=0)\n",
        "\n",
        "        # downscaling foreground option: downscaling both foreground and\n",
        "        # background for matching and use original background for reconstruction.\n",
        "        f = F.interpolate(f, scale_factor=1./self.rate, mode='nearest')\n",
        "        b = F.interpolate(b, scale_factor=1./self.rate, mode='nearest')\n",
        "        int_fs = list(f.size())     # b*c*h*w\n",
        "        int_bs = list(b.size())\n",
        "        f_groups = torch.split(f, 1, dim=0)  # split tensors along the batch dimension\n",
        "        # w shape: [N, C*k*k, L]\n",
        "        w = extract_image_patches(b, ksizes=[self.ksize, self.ksize],\n",
        "                                  strides=[self.stride, self.stride],\n",
        "                                  rates=[1, 1],\n",
        "                                  padding='same')\n",
        "        # w shape: [N, C, k, k, L]\n",
        "        w = w.view(int_bs[0], int_bs[1], self.ksize, self.ksize, -1)\n",
        "        w = w.permute(0, 4, 1, 2, 3)    # w shape: [N, L, C, k, k]\n",
        "        w_groups = torch.split(w, 1, dim=0)\n",
        "\n",
        "        # process mask\n",
        "        if mask is None:\n",
        "            mask = torch.zeros([int_bs[0], 1, int_bs[2], int_bs[3]])\n",
        "            if self.use_cuda:\n",
        "                mask = mask.cuda()\n",
        "\n",
        "        else:\n",
        "            mask = F.interpolate(mask, scale_factor=1./(4*self.rate), mode='nearest')\n",
        "        int_ms = list(mask.size())\n",
        "        # m shape: [N, C*k*k, L]\n",
        "        m = extract_image_patches(mask, ksizes=[self.ksize, self.ksize],\n",
        "                                  strides=[self.stride, self.stride],\n",
        "                                  rates=[1, 1],\n",
        "                                  padding='same')\n",
        "        # m shape: [N, C, k, k, L]\n",
        "        m = m.view(int_ms[0], int_ms[1], self.ksize, self.ksize, -1)\n",
        "        m = m.permute(0, 4, 1, 2, 3)    # m shape: [N, L, C, k, k]\n",
        "        m = m[0]    # m shape: [L, C, k, k]\n",
        "        # mm shape: [L, 1, 1, 1]\n",
        "        mm = (reduce_mean(m, axis=[1, 2, 3], keepdim=True)==0.).to(torch.float32)\n",
        "        mm = mm.permute(1, 0, 2, 3) # mm shape: [1, L, 1, 1]\n",
        "\n",
        "        y = []\n",
        "        offsets = []\n",
        "        k = self.fuse_k\n",
        "        scale = self.softmax_scale    # to fit the PyTorch tensor image value range\n",
        "        fuse_weight = torch.eye(k).view(1, 1, k, k)  # 1*1*k*k\n",
        "        if self.use_cuda:\n",
        "            fuse_weight = fuse_weight.cuda()\n",
        "\n",
        "        for xi, wi, raw_wi in zip(f_groups, w_groups, raw_w_groups):\n",
        "            # conv for compare\n",
        "            escape_NaN = torch.FloatTensor([1e-4])\n",
        "            if self.use_cuda:\n",
        "                escape_NaN = escape_NaN.cuda()\n",
        "            \n",
        "            wi = wi[0]  # [L, C, k, k]\n",
        "            max_wi = torch.sqrt(reduce_sum(torch.pow(wi, 2) + escape_NaN, axis=[1, 2, 3], keepdim=True))\n",
        "            wi_normed = wi / max_wi\n",
        "            # xi shape: [1, C, H, W], yi shape: [1, L, H, W]\n",
        "            xi = same_padding(xi, [self.ksize, self.ksize], [1, 1], [1, 1])  # xi: 1*c*H*W\n",
        "            yi = F.conv2d(xi, wi_normed, stride=1)   # [1, L, H, W]\n",
        "            # conv implementation for fuse scores to encourage large patches\n",
        "            if self.fuse:\n",
        "                # make all of depth to spatial resolution\n",
        "                yi = yi.view(1, 1, int_bs[2]*int_bs[3], int_fs[2]*int_fs[3])  # (B=1, I=1, H=32*32, W=32*32)\n",
        "                yi = same_padding(yi, [k, k], [1, 1], [1, 1])\n",
        "                yi = F.conv2d(yi, fuse_weight, stride=1)  # (B=1, C=1, H=32*32, W=32*32)\n",
        "                yi = yi.contiguous().view(1, int_bs[2], int_bs[3], int_fs[2], int_fs[3])  # (B=1, 32, 32, 32, 32)\n",
        "                yi = yi.permute(0, 2, 1, 4, 3)\n",
        "                yi = yi.contiguous().view(1, 1, int_bs[2]*int_bs[3], int_fs[2]*int_fs[3])\n",
        "                yi = same_padding(yi, [k, k], [1, 1], [1, 1])\n",
        "                yi = F.conv2d(yi, fuse_weight, stride=1)\n",
        "                yi = yi.contiguous().view(1, int_bs[3], int_bs[2], int_fs[3], int_fs[2])\n",
        "                yi = yi.permute(0, 2, 1, 4, 3).contiguous()\n",
        "            yi = yi.view(1, int_bs[2] * int_bs[3], int_fs[2], int_fs[3])  # (B=1, C=32*32, H=32, W=32)\n",
        "            # softmax to match\n",
        "            yi = yi * mm\n",
        "            yi = F.softmax(yi*scale, dim=1)\n",
        "            yi = yi * mm  # [1, L, H, W]\n",
        "\n",
        "            offset = torch.argmax(yi, dim=1, keepdim=True)  # 1*1*H*W\n",
        "\n",
        "            if int_bs != int_fs:\n",
        "                # Normalize the offset value to match foreground dimension\n",
        "                times = float(int_fs[2] * int_fs[3]) / float(int_bs[2] * int_bs[3])\n",
        "                offset = ((offset + 1).float() * times - 1).to(torch.int64)\n",
        "            offset = torch.cat([offset//int_fs[3], offset%int_fs[3]], dim=1)  # 1*2*H*W\n",
        "\n",
        "            # deconv for patch pasting\n",
        "            wi_center = raw_wi[0]\n",
        "            # yi = F.pad(yi, [0, 1, 0, 1])    # here may need conv_transpose same padding\n",
        "            yi = F.conv_transpose2d(yi, wi_center, stride=self.rate, padding=1) / 4.  # (B=1, C=128, H=64, W=64)\n",
        "            y.append(yi)\n",
        "            offsets.append(offset)\n",
        "\n",
        "        y = torch.cat(y, dim=0)  # back to the mini-batch\n",
        "        y.contiguous().view(raw_int_fs)\n",
        "\n",
        "        offsets = torch.cat(offsets, dim=0)\n",
        "        offsets = offsets.view(int_fs[0], 2, *int_fs[2:])\n",
        "\n",
        "        # case1: visualize optical flow: minus current position\n",
        "        h_add = torch.arange(int_fs[2]).view([1, 1, int_fs[2], 1]).expand(int_fs[0], -1, -1, int_fs[3])\n",
        "        w_add = torch.arange(int_fs[3]).view([1, 1, 1, int_fs[3]]).expand(int_fs[0], -1, int_fs[2], -1)\n",
        "        ref_coordinate = torch.cat([h_add, w_add], dim=1)\n",
        "        if self.use_cuda:\n",
        "            ref_coordinate = ref_coordinate.cuda()\n",
        "\n",
        "        offsets = offsets - ref_coordinate\n",
        "        # flow = pt_flow_to_image(offsets)\n",
        "\n",
        "        flow = torch.from_numpy(flow_to_image(offsets.permute(0, 2, 3, 1).cpu().data.numpy())) / 255.\n",
        "        flow = flow.permute(0, 3, 1, 2)\n",
        "        if self.use_cuda:\n",
        "            flow = flow.cuda()\n",
        "        # case2: visualize which pixels are attended\n",
        "        # flow = torch.from_numpy(highlight_flow((offsets * mask.long()).cpu().data.numpy()))\n",
        "\n",
        "        if self.rate != 1:\n",
        "            flow = F.interpolate(flow, scale_factor=self.rate*4, mode='nearest')\n",
        "\n",
        "        return y, flow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdP-UMLEm1rX"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim=3, cnum=32, use_cuda=False):\n",
        "        super(Generator, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.cnum = cnum\n",
        "        self.use_cuda = use_cuda\n",
        "\n",
        "        self.coarse_generator = CoarseGenerator(self.input_dim, self.cnum, self.use_cuda)\n",
        "        self.fine_generator = FineGenerator(self.input_dim, self.cnum, self.use_cuda)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x_stage1 = self.coarse_generator(x, mask)\n",
        "        x_stage2, offset_flow = self.fine_generator(x, x_stage1, mask)\n",
        "        return x_stage1, x_stage2, offset_flow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SbZ6pGFm3bN"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLN6VQTHm3Bg"
      },
      "source": [
        "class DisConvModule(nn.Module):\n",
        "    def __init__(self, input_dim, cnum):\n",
        "        super(DisConvModule, self).__init__()\n",
        "\n",
        "        self.conv1 = dis_conv(input_dim, cnum, 5, 2, 2)\n",
        "        self.conv2 = dis_conv(cnum, cnum*2, 5, 2, 2)\n",
        "        self.conv3 = dis_conv(cnum*2, cnum*4, 5, 2, 2)\n",
        "        self.conv4 = dis_conv(cnum*4, cnum*4, 5, 2, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBbJkmJAo3yb"
      },
      "source": [
        "class LocalDis(nn.Module):\n",
        "    def __init__(self, input_dim=3, cnum=64, use_cuda=False):\n",
        "        super(LocalDis, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.cnum = cnum\n",
        "        self.use_cuda = use_cuda\n",
        "\n",
        "        self.dis_conv_module = DisConvModule(self.input_dim, self.cnum)\n",
        "        self.linear = nn.Linear(self.cnum*4*8*8, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dis_conv_module(x)\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix8jUNaCo5Mx"
      },
      "source": [
        "class GlobalDis(nn.Module):\n",
        "    def __init__(self, input_dim=3, cnum=64, use_cuda=False):\n",
        "        super(GlobalDis, self).__init__()\n",
        "        self.use_cuda = use_cuda\n",
        "        self.input_dim = input_dim\n",
        "        self.cnum = cnum\n",
        "\n",
        "        self.dis_conv_module = DisConvModule(self.input_dim, self.cnum)\n",
        "        self.linear = nn.Linear(self.cnum*4*16*16, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dis_conv_module(x)\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziS0sC5bm4Pf"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83vPaSm7m4l4"
      },
      "source": [
        "from torchsummary import summary\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import autograd\n",
        "import gc\n",
        "\n",
        "import time\n",
        "import pyprind\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-BhNLmhtEDz"
      },
      "source": [
        "model_train = True  \n",
        "batch_size = 6\n",
        "start_epochs = 0\n",
        "total_epochs = 64\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "CHECKPOINT = \"/content/drive/MyDrive/Projects/Clubs/Analytics/Coord Projects/Model Zoo/Inpainting/checkpoints\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e42mk3MFm7NE"
      },
      "source": [
        "net_gen = Generator(use_cuda=True)\n",
        "net_local_dis = LocalDis()\n",
        "net_global_dis = GlobalDis()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RT78eeNYBCf3"
      },
      "source": [
        "d_params = list(net_local_dis.parameters()) + list(net_global_dis.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH5nWSpBAv0s"
      },
      "source": [
        "optimizer_g = optim.Adam(net_gen.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
        "optimizer_d = optim.Adam(d_params, lr=0.0001, betas=(0.5, 0.9))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6HSby3Rs3EP"
      },
      "source": [
        "net_gen.to(device)\n",
        "net_local_dis.to(device)\n",
        "net_global_dis.to(device)\n",
        "pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-5C6j5MB-ng"
      },
      "source": [
        "criterionL1 = nn.L1Loss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-l_-bsGCLj4"
      },
      "source": [
        "def calc_gradient_penalty(netD, real_data, fake_data, device):\n",
        "    batch_size = real_data.size(0)\n",
        "    alpha = torch.rand(batch_size, 1, 1, 1)\n",
        "    alpha = alpha.expand_as(real_data)\n",
        "    alpha = alpha.to(device)\n",
        "\n",
        "    interpolates = alpha * real_data + (1 - alpha) * fake_data\n",
        "    interpolates = interpolates.requires_grad_().clone()\n",
        "\n",
        "    disc_interpolates = netD(interpolates.float())\n",
        "    grad_outputs = torch.ones(disc_interpolates.size())\n",
        "    grad_outputs = grad_outputs.to(device)\n",
        "\n",
        "    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
        "                                grad_outputs=grad_outputs, create_graph=True,\n",
        "                                retain_graph=True, only_inputs=True)[0]\n",
        "\n",
        "    gradients = gradients.view(batch_size, -1)\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "\n",
        "    return gradient_penalty"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkcLKRCNI3JN"
      },
      "source": [
        " def dis_forward(netD, ground_truth, x_inpaint):\n",
        "    #assert ground_truth.size() == x_inpaint.size()\n",
        "    batch_size = ground_truth.size(0)\n",
        "    batch_data = torch.cat([ground_truth, x_inpaint], dim=0)\n",
        "    batch_output = netD(batch_data.float())\n",
        "    real_pred, fake_pred = torch.split(batch_output, batch_size, dim=0)\n",
        "\n",
        "    return real_pred, fake_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTJBnXOrm80B"
      },
      "source": [
        "#summary(net_gen, [(3,256,256),(1,256,256)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEJwIDEDm_r8"
      },
      "source": [
        "#summary(net_local_dis, (3,128,128))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dol2jsjApirr"
      },
      "source": [
        "#summary(net_global_dis, (3,256,256))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvxnkRsKBVEC"
      },
      "source": [
        "trainloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
        "valloader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=False)\n",
        "testloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ90RKhHTpet",
        "outputId": "e1dc7302-ea2f-4061-9708-fcdf419949b3"
      },
      "source": [
        "!gdown --id 1a3jdE3Mzz_JiVP6CatF4m58zeE99TYKX\n",
        "!gdown --id 1JXlWTIZBoYZrSQl7KnKIiiEtOiX58A-v"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1a3jdE3Mzz_JiVP6CatF4m58zeE99TYKX\n",
            "To: /content/dis_00430000.pt\n",
            "21.7MB [00:00, 190MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JXlWTIZBoYZrSQl7KnKIiiEtOiX58A-v\n",
            "To: /content/gen_00430000.pt\n",
            "14.4MB [00:00, 127MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBVkZXm1XbM_"
      },
      "source": [
        "if model_train:\n",
        "    if os.path.exists(os.path.join(CHECKPOINT, \"net_gen.pth\")):\n",
        "        checkpoints = torch.load(os.path.join(CHECKPOINT, \"net_gen.pth\"))\n",
        "\n",
        "        net_gen.load_state_dict(checkpoints['net_gen_state_dict'])\n",
        "        optimizer_g.load_state_dict(checkpoints['optimizer_g_state_dict'])\n",
        "        start_epochs = checkpoints['epoch']\n",
        "\n",
        "    if os.path.exists(os.path.join(CHECKPOINT, \"net_dis.pth\")):\n",
        "        checkpoints = torch.load(os.path.join(CHECKPOINT, \"net_dis.pth\"))\n",
        "\n",
        "        net_local_dis.load_state_dict(checkpoints['net_local_dis_state_dict'])\n",
        "        net_global_dis.load_state_dict(checkpoints['net_global_dis_state_dict'])\n",
        "        optimizer_d.load_state_dict(checkpoints['optimizer_d_state_dict'])\n",
        "        start_epochs = checkpoints['epoch']\n",
        "else:\n",
        "    checkpoint_dis = torch.load(\"/content/dis_00430000.pt\")\n",
        "    net_gen.load_state_dict(torch.load(\"/content/gen_00430000.pt\"))\n",
        "    net_local_dis.load_state_dict(checkpoint_dis['localD'])\n",
        "    net_global_dis.load_state_dict(checkpoint_dis['globalD'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjx55YgAJNAL"
      },
      "source": [
        "def epoch_time(epoch_end, epoch_start):\n",
        "    epoch_length = epoch_end - epoch_start\n",
        "\n",
        "    minutes = epoch_length//60\n",
        "    seconds = epoch_length - minutes*60\n",
        "\n",
        "    return (minutes, seconds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kG3W-Eb1DxEP"
      },
      "source": [
        "def train(net_gen, net_local_dis, net_global_dis, iterator, optimizer_g, optimizer_d, criterionL1):\n",
        "\n",
        "    epoch_loss = {'l1':0, 'ae':0, 'wgan_g':0, 'wgan_d':0, 'wgan_gp':0, 'g':0, 'd':0}\n",
        "    train_loss = []\n",
        "\n",
        "    epoch_start = time.time()\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    net_gen.train()\n",
        "    net_local_dis.train()\n",
        "    net_global_dis.train()\n",
        "\n",
        "    bar = pyprind.ProgBar(len(iterator), bar_char='')\n",
        "    for idx, ground_truth in enumerate(iterator, 1):\n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        batch_size = ground_truth.size(0)\n",
        "        bboxes = random_bbox(batch_size=batch_size)\n",
        "        x, mask = mask_image(ground_truth, bboxes)\n",
        "\n",
        "        ground_truth = ground_truth.to(device)\n",
        "        x = x.to(device)\n",
        "        mask = mask.to(device)\n",
        "\n",
        "        losses = {}\n",
        "\n",
        "        ##################\n",
        "        ### Prediction ###\n",
        "        ##################\n",
        "\n",
        "        x1, x2, offset_flow = net_gen(x.float(), mask)\n",
        "        local_patch_gt = local_patch(ground_truth, bboxes)\n",
        "        x1_inpaint = x1 * mask + x * (1. - mask)\n",
        "        x2_inpaint = x2 * mask + x * (1. - mask)\n",
        "        local_patch_x1_inpaint = local_patch(x1_inpaint, bboxes)\n",
        "        local_patch_x2_inpaint = local_patch(x2_inpaint, bboxes)\n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        ##########################\n",
        "        ### Discriminator Loss ###\n",
        "        ##########################\n",
        "\n",
        "        ### Local Discriminator ###\n",
        "                                                    \n",
        "        local_patch_real_pred, local_patch_fake_pred = dis_forward(net_local_dis, local_patch_gt, local_patch_x2_inpaint.detach())\n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        ### Global Discriminator ###\n",
        "\n",
        "        global_real_pred, global_fake_pred = dis_forward(net_global_dis, ground_truth, x2_inpaint.detach())\n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        ### Computing Losses ###\n",
        "\n",
        "        losses['wgan_d'] = torch.mean(local_patch_fake_pred-local_patch_real_pred) + torch.mean(global_fake_pred-global_real_pred)\n",
        "        \n",
        "        local_penalty = calc_gradient_penalty(net_local_dis, local_patch_gt, local_patch_x2_inpaint.detach(), device)\n",
        "        global_penalty = calc_gradient_penalty(net_global_dis, ground_truth, x2_inpaint.detach(), device)\n",
        "        losses['wgan_gp'] = local_penalty + global_penalty\n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        ######################\n",
        "        ### Generator Loss ###\n",
        "        ######################\n",
        "\n",
        "        sd_mask = spatial_discounting_mask(use_cuda=True)\n",
        "\n",
        "        losses['l1'] = 1.2*criterionL1(local_patch_x1_inpaint*sd_mask, local_patch_gt*sd_mask) + criterionL1(local_patch_x2_inpaint*sd_mask, local_patch_gt*sd_mask)\n",
        "\n",
        "        losses['ae'] = 1.2*criterionL1(x1*(1.-mask), ground_truth*(1.-mask)) + criterionL1(x2*(1.-mask), ground_truth*(1.-mask))\n",
        "\n",
        "        local_patch_real_pred_gen, local_patch_fake_pred_gen = dis_forward(net_local_dis, local_patch_gt, local_patch_x2_inpaint)\n",
        "        global_real_pred_gen, global_fake_pred_gen = dis_forward(net_global_dis, ground_truth, x2_inpaint)\n",
        "        losses['wgan_g'] = - torch.mean(local_patch_fake_pred_gen) - torch.mean(global_fake_pred_gen)\n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        ####################\n",
        "        ### Forward Pass ###\n",
        "        ####################\n",
        "\n",
        "        for k in losses.keys():\n",
        "            if not losses[k].dim() == 0:\n",
        "                losses[k] = torch.mean(losses[k])\n",
        "\n",
        "        #####################\n",
        "        ### Backward Pass ###\n",
        "        #####################\n",
        "        with torch.autograd.set_detect_anomaly(True):\n",
        "            if idx%5 !=0:\n",
        "                optimizer_d.zero_grad()\n",
        "                losses['d'] = losses['wgan_d'] + losses['wgan_gp']*10\n",
        "                losses['d'].backward()\n",
        "                optimizer_d.step()\n",
        "            else:\n",
        "                optimizer_g.zero_grad()\n",
        "                losses['g'] = losses['l1']*1.2 + losses['ae']*1.2 + losses['wgan_g']*0.001\n",
        "                losses['g'].backward()\n",
        "                optimizer_g.step()\n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        #####################\n",
        "        ### Visualization ###\n",
        "        #####################\n",
        "\n",
        "        for key in losses.keys():\n",
        "            epoch_loss[key] += losses[key].item()/len(iterator)\n",
        "\n",
        "        #train_loss.append(losses)\n",
        "        \n",
        "        bar.update()\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    epoch_end = time.time()\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    epoch_length = epoch_time(epoch_end, epoch_start)\n",
        "\n",
        "    return epoch_loss, epoch_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKewGaiZWnM8"
      },
      "source": [
        "def evaluate(net_gen, iterator,criterionL1):\n",
        "\n",
        "    epoch_loss = {'l1':0, 'ae':0, 'wgan_g':0, 'g':0}\n",
        "    eval_loss = []\n",
        "\n",
        "    epoch_start = time.time()\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    net_gen.eval()\n",
        "    net_local_dis.eval()\n",
        "    net_global_dis.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        bar = pyprind.ProgBar(len(iterator), bar_char='')\n",
        "        for idx, ground_truth in enumerate(iterator, 1):\n",
        "\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            batch_size = ground_truth.size(0)\n",
        "            bboxes = random_bbox(batch_size=batch_size)\n",
        "            x, mask = mask_image(ground_truth, bboxes)\n",
        "\n",
        "            ground_truth = ground_truth.to(device)\n",
        "            x = x.to(device)\n",
        "            mask = mask.to(device)\n",
        "\n",
        "            losses = {}\n",
        "\n",
        "            ##################\n",
        "            ### Prediction ###\n",
        "            ##################\n",
        "\n",
        "            x1, x2, offset_flow = net_gen(x.float(), mask)\n",
        "            local_patch_gt = local_patch(ground_truth, bboxes)\n",
        "            x1_inpaint = x1 * mask + x * (1. - mask)\n",
        "            x2_inpaint = x2 * mask + x * (1. - mask)\n",
        "            local_patch_x1_inpaint = local_patch(x1_inpaint, bboxes)\n",
        "            local_patch_x2_inpaint = local_patch(x2_inpaint, bboxes)\n",
        "\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            ######################\n",
        "            ### Generator Loss ###\n",
        "            ######################\n",
        "\n",
        "            sd_mask = spatial_discounting_mask(use_cuda=True)\n",
        "\n",
        "            losses['l1'] = 1.2*criterionL1(local_patch_x1_inpaint*sd_mask, local_patch_gt*sd_mask) + criterionL1(local_patch_x2_inpaint*sd_mask, local_patch_gt*sd_mask)\n",
        "\n",
        "            losses['ae'] = 1.2*criterionL1(x1*(1.-mask), ground_truth*(1.-mask)) + criterionL1(x2*(1.-mask), ground_truth*(1.-mask))\n",
        "\n",
        "            local_patch_real_pred, local_patch_fake_pred = dis_forward(net_local_dis, local_patch_gt, local_patch_x2_inpaint)\n",
        "            global_real_pred, global_fake_pred = dis_forward(net_global_dis, ground_truth, x2_inpaint)\n",
        "            losses['wgan_g'] = - torch.mean(local_patch_fake_pred) - torch.mean(global_fake_pred)\n",
        "\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            ####################\n",
        "            ### Forward Pass ###\n",
        "            ####################\n",
        "\n",
        "            losses['g'] = losses['l1']*1.2 + losses['ae']*1.2 + losses['wgan_g']*0.001\n",
        "            for k in losses.keys():\n",
        "                if not losses[k].dim() == 0:\n",
        "                    losses[k] = torch.mean(losses[k])\n",
        "\n",
        "            #####################\n",
        "            ### Visualization ###\n",
        "            #####################\n",
        "\n",
        "            for key in losses.keys():\n",
        "                epoch_loss[key] += losses[key].item()/len(iterator)\n",
        "\n",
        "            #eval_loss.append(losses)\n",
        "            \n",
        "            bar.update()\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    epoch_end = time.time()\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    epoch_length = epoch_time(epoch_end, epoch_start)\n",
        "\n",
        "    return epoch_loss, epoch_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtQvjzUHB5fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 956
        },
        "outputId": "304bd22d-c1c5-4b21-f8c5-f2c90224de7c"
      },
      "source": [
        "train_loss = []\n",
        "val_loss = []\n",
        "\n",
        "if model_train:\n",
        "    for epoch in range(start_epochs+1, total_epochs+start_epochs+1):\n",
        "        print(\"Starting Epoch[{0}/{1}]\".format(epoch, total_epochs+start_epochs))\n",
        "        \n",
        "        epoch_start = time.time()\n",
        "\n",
        "        train_epoch_loss, _ = train(net_gen, net_local_dis, net_global_dis, trainloader, optimizer_g, optimizer_d, criterionL1)\n",
        "        train_loss.append(train_epoch_loss)\n",
        "        print(\" | Train Loss: Generator: {0}  |  Disctiminator: {1}\".format(train_epoch_loss['g'], train_epoch_loss['d']))\n",
        "\n",
        "        val_epoch_loss, _ = evaluate(net_gen, valloader, criterionL1)\n",
        "        val_loss.append(val_epoch_loss)\n",
        "        print(\" | Validation Loss: Generator: {0}\".format(val_epoch_loss['g']))\n",
        "\n",
        "        torch.save({\n",
        "                'epoch': epoch,\n",
        "                'net_gen_state_dict': net_gen.state_dict(),\n",
        "                'optimizer_g_state_dict': optimizer_g.state_dict(),\n",
        "                }, os.path.join(CHECKPOINT, \"net_gen.pth\"))\n",
        "        torch.save({\n",
        "                'epoch': epoch,\n",
        "                'net_local_dis_state_dict': net_local_dis.state_dict(),\n",
        "                'net_global_dis_state_dict': net_global_dis.state_dict(),\n",
        "                'optimizer_d_state_dict': optimizer_d.state_dict(),\n",
        "                }, os.path.join(CHECKPOINT, \"net_dis.pth\"))\n",
        "        \n",
        "\n",
        "        epoch_end = time.time()\n",
        "\n",
        "        minutes, seconds = epoch_time(epoch_end, epoch_start)\n",
        "\n",
        "        print(\"Finished Epoch[{0}/{1}]\".format(epoch, total_epochs+start_epochs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Epoch[27/90]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3658: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n",
            "0% [] 100% | ETA: 00:00:00"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " | Train Loss: Generator: -0.07538441772653824  |  Disctiminator: -244.9920135553503\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Total time elapsed: 02:39:17\n",
            "0% [] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:04:48\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " | Validation Loss: Generator: -0.07023869422199909\n",
            "Finished Epoch[27/90]\n",
            "Starting Epoch[28/90]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "0% [] 100% | ETA: 00:00:00"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " | Train Loss: Generator: -0.036059386844847476  |  Disctiminator: -240.7046226423151\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Total time elapsed: 01:10:02\n",
            "0% [] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:00:48\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " | Validation Loss: Generator: -0.19356585787272865\n",
            "Finished Epoch[28/90]\n",
            "Starting Epoch[29/90]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "0% [] 100% | ETA: 00:00:00"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " | Train Loss: Generator: -0.0636371394320836  |  Disctiminator: -242.54500389005872\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Total time elapsed: 01:04:36\n",
            "0% [] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:00:47\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " | Validation Loss: Generator: -0.05802334587382475\n",
            "Finished Epoch[29/90]\n",
            "Starting Epoch[30/90]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-96b6b8100c3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mepoch_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtrain_epoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_local_dis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_global_dis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterionL1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_epoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" | Train Loss: Generator: {0}  |  Disctiminator: {1}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_epoch_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_epoch_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'd'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-c391c5e333a4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net_gen, net_local_dis, net_global_dis, iterator, optimizer_g, optimizer_d, criterionL1)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m####################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/memory.py\u001b[0m in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m     \"\"\"\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_emptyCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Svotzr8EVsYf"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPbOGlG6QQHJ"
      },
      "source": [
        "preds = []\n",
        "\n",
        "for idx, ground_truth in enumerate(testloader, 1):\n",
        "    batch_size = ground_truth.size(0)\n",
        "    bboxes = random_bbox(batch_size=batch_size)\n",
        "    x, mask = mask_image(ground_truth, bboxes)\n",
        "\n",
        "    ground_truth = ground_truth.to(device)\n",
        "    x = x.to(device)\n",
        "    mask = mask.to(device)\n",
        "\n",
        "    x1, x2, offset_flow = net_gen(x.float(), mask)\n",
        "    x2_inpaint = x2 * mask + x * (1. - mask)\n",
        "\n",
        "    for index in range(batch_size):\n",
        "        ground = ground_truth[index].detach().cpu().numpy()\n",
        "        masked = x[index].detach().cpu().numpy()\n",
        "        image = x2_inpaint[index].detach().cpu().numpy()\n",
        "        \n",
        "        ground = np.moveaxis(ground, 0, -1)*255\n",
        "        masked = np.moveaxis(masked, 0, -1)*255\n",
        "        image = np.moveaxis(image, 0, -1)*255\n",
        "\n",
        "        preds.append({'ground':ground, 'masked':masked, 'image':image})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "745_SBAcVnpy"
      },
      "source": [
        "for index in range(len(preds)):\n",
        "    render = preds[index]\n",
        "    cv2_imshow(np.concatenate((render['ground'], render['masked'], render['image']), axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGEfxCqtWiwh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}