{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "n--ufW4iXwsx"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p48STurnMgGV"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import gc"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03Pdt_gjreh6"
      },
      "source": [
        "!mkdir squad\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json -O squad/train-v2.0.json\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json -O squad/dev-v2.0.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXU1jxD_rfzw"
      },
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def read_squad(path):\n",
        "    path = Path(path)\n",
        "    with open(path, 'rb') as f:\n",
        "        squad_dict = json.load(f)\n",
        "\n",
        "    contexts = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "    for group in squad_dict['data']:\n",
        "        for passage in group['paragraphs']:\n",
        "            context = passage['context']\n",
        "            for qa in passage['qas']:\n",
        "                question = qa['question']\n",
        "                for answer in qa['answers']:\n",
        "                    contexts.append(context)\n",
        "                    questions.append(question)\n",
        "                    answers.append(answer)\n",
        "\n",
        "    return contexts, questions, answers\n",
        "\n",
        "train_contexts, train_questions, train_answers = read_squad('/content/squad/train-v2.0.json')\n",
        "val_contexts, val_questions, val_answers = read_squad('/content/squad/dev-v2.0.json')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0Tte9OKrqdl"
      },
      "source": [
        "def add_end_idx(answers, contexts):\n",
        "    for answer, context in zip(answers, contexts):\n",
        "        gold_text = answer['text']\n",
        "        start_idx = answer['answer_start']\n",
        "        end_idx = start_idx + len(gold_text)\n",
        "\n",
        "        if context[start_idx:end_idx] == gold_text:\n",
        "            answer['answer_end'] = end_idx\n",
        "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
        "            answer['answer_start'] = start_idx - 1\n",
        "            answer['answer_end'] = end_idx - 1 \n",
        "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
        "            answer['answer_start'] = start_idx - 2\n",
        "            answer['answer_end'] = end_idx - 2 \n",
        "\n",
        "add_end_idx(train_answers, train_contexts)\n",
        "add_end_idx(val_answers, val_contexts)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbuaoYF4t2Ir"
      },
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
        "gc.collect()\n",
        "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOkdHSXyutvc"
      },
      "source": [
        "def add_token_positions(encodings, answers):\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    for i in range(len(answers)):\n",
        "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
        "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
        "\n",
        "        if start_positions[-1] is None:\n",
        "            start_positions[-1] = tokenizer.model_max_length\n",
        "        if end_positions[-1] is None:\n",
        "            end_positions[-1] = tokenizer.model_max_length\n",
        "\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "\n",
        "add_token_positions(train_encodings, train_answers)\n",
        "add_token_positions(val_encodings, val_answers)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OStiggEzvItT"
      },
      "source": [
        "import torch\n",
        "\n",
        "class SquadDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "train_dataset = SquadDataset(train_encodings)\n",
        "val_dataset = SquadDataset(val_encodings)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgYaPEuavyxR"
      },
      "source": [
        "from transformers import DistilBertForQuestionAnswering\n",
        "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFrya1pTv1N7",
        "outputId": "a1ebddb5-4b53-4c36-99f7-da6ede46c343"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "from tqdm import trange\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "optim = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "epoch_loss = []\n",
        "for epoch in range(3):\n",
        "    for idx, batch in enumerate(train_loader):\n",
        "        optim.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_positions = batch['start_positions'].to(device)\n",
        "        end_positions = batch['end_positions'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        epoch_loss.append(loss.item())\n",
        "        print(\"Batch: {0}    Epoch[{1}/3]    Training Loss: {2}\".format(idx, epoch+1, epoch_loss[-1]))\n",
        "    \n",
        "\n",
        "\n",
        "model.eval()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Batch: 427    Epoch[3/3]    Training Loss: 0.460075318813324\n",
            "Batch: 428    Epoch[3/3]    Training Loss: 0.35041433572769165\n",
            "Batch: 429    Epoch[3/3]    Training Loss: 0.40419939160346985\n",
            "Batch: 430    Epoch[3/3]    Training Loss: 0.8635175228118896\n",
            "Batch: 431    Epoch[3/3]    Training Loss: 0.36274808645248413\n",
            "Batch: 432    Epoch[3/3]    Training Loss: 0.23200422525405884\n",
            "Batch: 433    Epoch[3/3]    Training Loss: 0.45207974314689636\n",
            "Batch: 434    Epoch[3/3]    Training Loss: 0.33109182119369507\n",
            "Batch: 435    Epoch[3/3]    Training Loss: 0.15788090229034424\n",
            "Batch: 436    Epoch[3/3]    Training Loss: 0.3248405158519745\n",
            "Batch: 437    Epoch[3/3]    Training Loss: 0.31180256605148315\n",
            "Batch: 438    Epoch[3/3]    Training Loss: 0.5565935373306274\n",
            "Batch: 439    Epoch[3/3]    Training Loss: 0.7169691324234009\n",
            "Batch: 440    Epoch[3/3]    Training Loss: 0.3736518621444702\n",
            "Batch: 441    Epoch[3/3]    Training Loss: 0.3590708076953888\n",
            "Batch: 442    Epoch[3/3]    Training Loss: 0.605109691619873\n",
            "Batch: 443    Epoch[3/3]    Training Loss: 0.37757742404937744\n",
            "Batch: 444    Epoch[3/3]    Training Loss: 0.4163696765899658\n",
            "Batch: 445    Epoch[3/3]    Training Loss: 1.0369493961334229\n",
            "Batch: 446    Epoch[3/3]    Training Loss: 0.32883530855178833\n",
            "Batch: 447    Epoch[3/3]    Training Loss: 0.7199709415435791\n",
            "Batch: 448    Epoch[3/3]    Training Loss: 0.6546262502670288\n",
            "Batch: 449    Epoch[3/3]    Training Loss: 0.2056710571050644\n",
            "Batch: 450    Epoch[3/3]    Training Loss: 0.971337080001831\n",
            "Batch: 451    Epoch[3/3]    Training Loss: 0.6726647615432739\n",
            "Batch: 452    Epoch[3/3]    Training Loss: 0.4911448657512665\n",
            "Batch: 453    Epoch[3/3]    Training Loss: 0.703916609287262\n",
            "Batch: 454    Epoch[3/3]    Training Loss: 0.6610887050628662\n",
            "Batch: 455    Epoch[3/3]    Training Loss: 0.7378441691398621\n",
            "Batch: 456    Epoch[3/3]    Training Loss: 0.6794056296348572\n",
            "Batch: 457    Epoch[3/3]    Training Loss: 0.43524184823036194\n",
            "Batch: 458    Epoch[3/3]    Training Loss: 0.12942156195640564\n",
            "Batch: 459    Epoch[3/3]    Training Loss: 1.1039073467254639\n",
            "Batch: 460    Epoch[3/3]    Training Loss: 0.5952537655830383\n",
            "Batch: 461    Epoch[3/3]    Training Loss: 0.8048995733261108\n",
            "Batch: 462    Epoch[3/3]    Training Loss: 0.438340425491333\n",
            "Batch: 463    Epoch[3/3]    Training Loss: 0.27466750144958496\n",
            "Batch: 464    Epoch[3/3]    Training Loss: 0.45535725355148315\n",
            "Batch: 465    Epoch[3/3]    Training Loss: 0.5170595049858093\n",
            "Batch: 466    Epoch[3/3]    Training Loss: 0.7982381582260132\n",
            "Batch: 467    Epoch[3/3]    Training Loss: 0.35583001375198364\n",
            "Batch: 468    Epoch[3/3]    Training Loss: 1.0124775171279907\n",
            "Batch: 469    Epoch[3/3]    Training Loss: 0.6192445158958435\n",
            "Batch: 470    Epoch[3/3]    Training Loss: 0.5854418277740479\n",
            "Batch: 471    Epoch[3/3]    Training Loss: 0.2422352135181427\n",
            "Batch: 472    Epoch[3/3]    Training Loss: 0.7430395483970642\n",
            "Batch: 473    Epoch[3/3]    Training Loss: 0.4115411639213562\n",
            "Batch: 474    Epoch[3/3]    Training Loss: 0.2406342625617981\n",
            "Batch: 475    Epoch[3/3]    Training Loss: 0.4218723773956299\n",
            "Batch: 476    Epoch[3/3]    Training Loss: 0.4771588444709778\n",
            "Batch: 477    Epoch[3/3]    Training Loss: 0.6093053221702576\n",
            "Batch: 478    Epoch[3/3]    Training Loss: 0.532853364944458\n",
            "Batch: 479    Epoch[3/3]    Training Loss: 0.7898922562599182\n",
            "Batch: 480    Epoch[3/3]    Training Loss: 0.4566860795021057\n",
            "Batch: 481    Epoch[3/3]    Training Loss: 0.6542359590530396\n",
            "Batch: 482    Epoch[3/3]    Training Loss: 0.6108945608139038\n",
            "Batch: 483    Epoch[3/3]    Training Loss: 0.6164274215698242\n",
            "Batch: 484    Epoch[3/3]    Training Loss: 0.25484269857406616\n",
            "Batch: 485    Epoch[3/3]    Training Loss: 1.1427751779556274\n",
            "Batch: 486    Epoch[3/3]    Training Loss: 0.6055636405944824\n",
            "Batch: 487    Epoch[3/3]    Training Loss: 0.2630813717842102\n",
            "Batch: 488    Epoch[3/3]    Training Loss: 0.6975088119506836\n",
            "Batch: 489    Epoch[3/3]    Training Loss: 0.4293261468410492\n",
            "Batch: 490    Epoch[3/3]    Training Loss: 0.3081143796443939\n",
            "Batch: 491    Epoch[3/3]    Training Loss: 0.5883861780166626\n",
            "Batch: 492    Epoch[3/3]    Training Loss: 0.21971508860588074\n",
            "Batch: 493    Epoch[3/3]    Training Loss: 0.5520524978637695\n",
            "Batch: 494    Epoch[3/3]    Training Loss: 0.703933835029602\n",
            "Batch: 495    Epoch[3/3]    Training Loss: 0.6288158297538757\n",
            "Batch: 496    Epoch[3/3]    Training Loss: 0.4818289279937744\n",
            "Batch: 497    Epoch[3/3]    Training Loss: 0.4139992594718933\n",
            "Batch: 498    Epoch[3/3]    Training Loss: 0.1690424680709839\n",
            "Batch: 499    Epoch[3/3]    Training Loss: 0.4689151346683502\n",
            "Batch: 500    Epoch[3/3]    Training Loss: 0.5280067920684814\n",
            "Batch: 501    Epoch[3/3]    Training Loss: 0.5981934070587158\n",
            "Batch: 502    Epoch[3/3]    Training Loss: 0.6820462346076965\n",
            "Batch: 503    Epoch[3/3]    Training Loss: 1.5117652416229248\n",
            "Batch: 504    Epoch[3/3]    Training Loss: 0.5726312398910522\n",
            "Batch: 505    Epoch[3/3]    Training Loss: 0.3487165570259094\n",
            "Batch: 506    Epoch[3/3]    Training Loss: 0.9703543186187744\n",
            "Batch: 507    Epoch[3/3]    Training Loss: 0.5013882517814636\n",
            "Batch: 508    Epoch[3/3]    Training Loss: 0.8116238713264465\n",
            "Batch: 509    Epoch[3/3]    Training Loss: 0.507166862487793\n",
            "Batch: 510    Epoch[3/3]    Training Loss: 0.897205114364624\n",
            "Batch: 511    Epoch[3/3]    Training Loss: 0.30708661675453186\n",
            "Batch: 512    Epoch[3/3]    Training Loss: 0.3098612427711487\n",
            "Batch: 513    Epoch[3/3]    Training Loss: 0.5464819073677063\n",
            "Batch: 514    Epoch[3/3]    Training Loss: 0.6076209545135498\n",
            "Batch: 515    Epoch[3/3]    Training Loss: 0.23426613211631775\n",
            "Batch: 516    Epoch[3/3]    Training Loss: 0.5598336458206177\n",
            "Batch: 517    Epoch[3/3]    Training Loss: 0.3459254205226898\n",
            "Batch: 518    Epoch[3/3]    Training Loss: 0.39342620968818665\n",
            "Batch: 519    Epoch[3/3]    Training Loss: 0.21256813406944275\n",
            "Batch: 520    Epoch[3/3]    Training Loss: 0.5629199147224426\n",
            "Batch: 521    Epoch[3/3]    Training Loss: 0.6728379726409912\n",
            "Batch: 522    Epoch[3/3]    Training Loss: 0.30703240633010864\n",
            "Batch: 523    Epoch[3/3]    Training Loss: 0.8892697095870972\n",
            "Batch: 524    Epoch[3/3]    Training Loss: 0.27294638752937317\n",
            "Batch: 525    Epoch[3/3]    Training Loss: 0.3707898259162903\n",
            "Batch: 526    Epoch[3/3]    Training Loss: 0.8897805213928223\n",
            "Batch: 527    Epoch[3/3]    Training Loss: 0.45550400018692017\n",
            "Batch: 528    Epoch[3/3]    Training Loss: 1.0308252573013306\n",
            "Batch: 529    Epoch[3/3]    Training Loss: 0.2592896819114685\n",
            "Batch: 530    Epoch[3/3]    Training Loss: 0.3387948274612427\n",
            "Batch: 531    Epoch[3/3]    Training Loss: 0.41660213470458984\n",
            "Batch: 532    Epoch[3/3]    Training Loss: 0.7720491886138916\n",
            "Batch: 533    Epoch[3/3]    Training Loss: 0.4059993028640747\n",
            "Batch: 534    Epoch[3/3]    Training Loss: 0.6064184904098511\n",
            "Batch: 535    Epoch[3/3]    Training Loss: 0.4557231664657593\n",
            "Batch: 536    Epoch[3/3]    Training Loss: 0.4579680860042572\n",
            "Batch: 537    Epoch[3/3]    Training Loss: 0.2153243124485016\n",
            "Batch: 538    Epoch[3/3]    Training Loss: 0.3230983018875122\n",
            "Batch: 539    Epoch[3/3]    Training Loss: 0.6412006616592407\n",
            "Batch: 540    Epoch[3/3]    Training Loss: 0.5353282690048218\n",
            "Batch: 541    Epoch[3/3]    Training Loss: 0.6036032438278198\n",
            "Batch: 542    Epoch[3/3]    Training Loss: 0.7195268869400024\n",
            "Batch: 543    Epoch[3/3]    Training Loss: 0.2911708652973175\n",
            "Batch: 544    Epoch[3/3]    Training Loss: 0.7273150682449341\n",
            "Batch: 545    Epoch[3/3]    Training Loss: 0.5456368327140808\n",
            "Batch: 546    Epoch[3/3]    Training Loss: 0.19969546794891357\n",
            "Batch: 547    Epoch[3/3]    Training Loss: 0.35730794072151184\n",
            "Batch: 548    Epoch[3/3]    Training Loss: 0.5410681962966919\n",
            "Batch: 549    Epoch[3/3]    Training Loss: 0.27770382165908813\n",
            "Batch: 550    Epoch[3/3]    Training Loss: 0.580640971660614\n",
            "Batch: 551    Epoch[3/3]    Training Loss: 0.17471739649772644\n",
            "Batch: 552    Epoch[3/3]    Training Loss: 0.3735216557979584\n",
            "Batch: 553    Epoch[3/3]    Training Loss: 0.741034209728241\n",
            "Batch: 554    Epoch[3/3]    Training Loss: 0.5614234805107117\n",
            "Batch: 555    Epoch[3/3]    Training Loss: 0.251490980386734\n",
            "Batch: 556    Epoch[3/3]    Training Loss: 0.5563454627990723\n",
            "Batch: 557    Epoch[3/3]    Training Loss: 0.47503554821014404\n",
            "Batch: 558    Epoch[3/3]    Training Loss: 0.7846531867980957\n",
            "Batch: 559    Epoch[3/3]    Training Loss: 0.5504316091537476\n",
            "Batch: 560    Epoch[3/3]    Training Loss: 0.33069950342178345\n",
            "Batch: 561    Epoch[3/3]    Training Loss: 0.531530499458313\n",
            "Batch: 562    Epoch[3/3]    Training Loss: 0.5081495046615601\n",
            "Batch: 563    Epoch[3/3]    Training Loss: 0.6671452522277832\n",
            "Batch: 564    Epoch[3/3]    Training Loss: 0.24651256203651428\n",
            "Batch: 565    Epoch[3/3]    Training Loss: 0.7779682874679565\n",
            "Batch: 566    Epoch[3/3]    Training Loss: 0.4564484655857086\n",
            "Batch: 567    Epoch[3/3]    Training Loss: 0.33063656091690063\n",
            "Batch: 568    Epoch[3/3]    Training Loss: 0.7366304397583008\n",
            "Batch: 569    Epoch[3/3]    Training Loss: 0.3263702094554901\n",
            "Batch: 570    Epoch[3/3]    Training Loss: 0.5628102421760559\n",
            "Batch: 571    Epoch[3/3]    Training Loss: 0.45771825313568115\n",
            "Batch: 572    Epoch[3/3]    Training Loss: 0.655437171459198\n",
            "Batch: 573    Epoch[3/3]    Training Loss: 0.6757200956344604\n",
            "Batch: 574    Epoch[3/3]    Training Loss: 0.4446728527545929\n",
            "Batch: 575    Epoch[3/3]    Training Loss: 0.35754677653312683\n",
            "Batch: 576    Epoch[3/3]    Training Loss: 0.7296895384788513\n",
            "Batch: 577    Epoch[3/3]    Training Loss: 0.39135289192199707\n",
            "Batch: 578    Epoch[3/3]    Training Loss: 0.42077013850212097\n",
            "Batch: 579    Epoch[3/3]    Training Loss: 0.5565451383590698\n",
            "Batch: 580    Epoch[3/3]    Training Loss: 0.3954971730709076\n",
            "Batch: 581    Epoch[3/3]    Training Loss: 0.6370503902435303\n",
            "Batch: 582    Epoch[3/3]    Training Loss: 0.41111570596694946\n",
            "Batch: 583    Epoch[3/3]    Training Loss: 0.6941584348678589\n",
            "Batch: 584    Epoch[3/3]    Training Loss: 0.8782351016998291\n",
            "Batch: 585    Epoch[3/3]    Training Loss: 0.5803583860397339\n",
            "Batch: 586    Epoch[3/3]    Training Loss: 0.6217889785766602\n",
            "Batch: 587    Epoch[3/3]    Training Loss: 0.24294263124465942\n",
            "Batch: 588    Epoch[3/3]    Training Loss: 0.7091476917266846\n",
            "Batch: 589    Epoch[3/3]    Training Loss: 0.9215853810310364\n",
            "Batch: 590    Epoch[3/3]    Training Loss: 0.9252647161483765\n",
            "Batch: 591    Epoch[3/3]    Training Loss: 0.4035184979438782\n",
            "Batch: 592    Epoch[3/3]    Training Loss: 0.68282550573349\n",
            "Batch: 593    Epoch[3/3]    Training Loss: 0.7723098397254944\n",
            "Batch: 594    Epoch[3/3]    Training Loss: 0.7314512729644775\n",
            "Batch: 595    Epoch[3/3]    Training Loss: 0.565575361251831\n",
            "Batch: 596    Epoch[3/3]    Training Loss: 0.5228499174118042\n",
            "Batch: 597    Epoch[3/3]    Training Loss: 0.6324635744094849\n",
            "Batch: 598    Epoch[3/3]    Training Loss: 0.548682451248169\n",
            "Batch: 599    Epoch[3/3]    Training Loss: 0.38793301582336426\n",
            "Batch: 600    Epoch[3/3]    Training Loss: 0.4317099153995514\n",
            "Batch: 601    Epoch[3/3]    Training Loss: 0.7976430654525757\n",
            "Batch: 602    Epoch[3/3]    Training Loss: 0.8119100332260132\n",
            "Batch: 603    Epoch[3/3]    Training Loss: 0.9079089164733887\n",
            "Batch: 604    Epoch[3/3]    Training Loss: 0.9140888452529907\n",
            "Batch: 605    Epoch[3/3]    Training Loss: 0.4145508110523224\n",
            "Batch: 606    Epoch[3/3]    Training Loss: 0.763584554195404\n",
            "Batch: 607    Epoch[3/3]    Training Loss: 0.23198096454143524\n",
            "Batch: 608    Epoch[3/3]    Training Loss: 0.25712716579437256\n",
            "Batch: 609    Epoch[3/3]    Training Loss: 0.7593806385993958\n",
            "Batch: 610    Epoch[3/3]    Training Loss: 0.6473140716552734\n",
            "Batch: 611    Epoch[3/3]    Training Loss: 0.2123158574104309\n",
            "Batch: 612    Epoch[3/3]    Training Loss: 0.28063005208969116\n",
            "Batch: 613    Epoch[3/3]    Training Loss: 0.4324084520339966\n",
            "Batch: 614    Epoch[3/3]    Training Loss: 0.7226508855819702\n",
            "Batch: 615    Epoch[3/3]    Training Loss: 0.630711555480957\n",
            "Batch: 616    Epoch[3/3]    Training Loss: 0.5202293992042542\n",
            "Batch: 617    Epoch[3/3]    Training Loss: 0.967907726764679\n",
            "Batch: 618    Epoch[3/3]    Training Loss: 0.28306782245635986\n",
            "Batch: 619    Epoch[3/3]    Training Loss: 1.0521504878997803\n",
            "Batch: 620    Epoch[3/3]    Training Loss: 0.7883055210113525\n",
            "Batch: 621    Epoch[3/3]    Training Loss: 1.0087509155273438\n",
            "Batch: 622    Epoch[3/3]    Training Loss: 0.465709924697876\n",
            "Batch: 623    Epoch[3/3]    Training Loss: 0.5989488363265991\n",
            "Batch: 624    Epoch[3/3]    Training Loss: 1.031584620475769\n",
            "Batch: 625    Epoch[3/3]    Training Loss: 0.2527707815170288\n",
            "Batch: 626    Epoch[3/3]    Training Loss: 0.4001549482345581\n",
            "Batch: 627    Epoch[3/3]    Training Loss: 1.2842611074447632\n",
            "Batch: 628    Epoch[3/3]    Training Loss: 0.31041789054870605\n",
            "Batch: 629    Epoch[3/3]    Training Loss: 0.5606944561004639\n",
            "Batch: 630    Epoch[3/3]    Training Loss: 0.7619313597679138\n",
            "Batch: 631    Epoch[3/3]    Training Loss: 0.5153375864028931\n",
            "Batch: 632    Epoch[3/3]    Training Loss: 0.46938556432724\n",
            "Batch: 633    Epoch[3/3]    Training Loss: 0.44582363963127136\n",
            "Batch: 634    Epoch[3/3]    Training Loss: 0.8386926054954529\n",
            "Batch: 635    Epoch[3/3]    Training Loss: 0.3829387426376343\n",
            "Batch: 636    Epoch[3/3]    Training Loss: 0.23713955283164978\n",
            "Batch: 637    Epoch[3/3]    Training Loss: 0.704131007194519\n",
            "Batch: 638    Epoch[3/3]    Training Loss: 0.709964394569397\n",
            "Batch: 639    Epoch[3/3]    Training Loss: 0.36274999380111694\n",
            "Batch: 640    Epoch[3/3]    Training Loss: 0.40916022658348083\n",
            "Batch: 641    Epoch[3/3]    Training Loss: 0.3678714632987976\n",
            "Batch: 642    Epoch[3/3]    Training Loss: 0.30166688561439514\n",
            "Batch: 643    Epoch[3/3]    Training Loss: 0.6893788576126099\n",
            "Batch: 644    Epoch[3/3]    Training Loss: 0.44827377796173096\n",
            "Batch: 645    Epoch[3/3]    Training Loss: 0.7131781578063965\n",
            "Batch: 646    Epoch[3/3]    Training Loss: 0.280264675617218\n",
            "Batch: 647    Epoch[3/3]    Training Loss: 0.5625649094581604\n",
            "Batch: 648    Epoch[3/3]    Training Loss: 0.512891948223114\n",
            "Batch: 649    Epoch[3/3]    Training Loss: 0.8656338453292847\n",
            "Batch: 650    Epoch[3/3]    Training Loss: 0.7731177806854248\n",
            "Batch: 651    Epoch[3/3]    Training Loss: 1.1614127159118652\n",
            "Batch: 652    Epoch[3/3]    Training Loss: 0.3378269076347351\n",
            "Batch: 653    Epoch[3/3]    Training Loss: 0.2616245448589325\n",
            "Batch: 654    Epoch[3/3]    Training Loss: 1.1508498191833496\n",
            "Batch: 655    Epoch[3/3]    Training Loss: 0.5289111137390137\n",
            "Batch: 656    Epoch[3/3]    Training Loss: 0.3865167498588562\n",
            "Batch: 657    Epoch[3/3]    Training Loss: 0.9479972124099731\n",
            "Batch: 658    Epoch[3/3]    Training Loss: 0.5250611305236816\n",
            "Batch: 659    Epoch[3/3]    Training Loss: 0.37837958335876465\n",
            "Batch: 660    Epoch[3/3]    Training Loss: 0.8095019459724426\n",
            "Batch: 661    Epoch[3/3]    Training Loss: 0.46069639921188354\n",
            "Batch: 662    Epoch[3/3]    Training Loss: 0.24553290009498596\n",
            "Batch: 663    Epoch[3/3]    Training Loss: 0.4925314784049988\n",
            "Batch: 664    Epoch[3/3]    Training Loss: 0.2950578033924103\n",
            "Batch: 665    Epoch[3/3]    Training Loss: 0.35787874460220337\n",
            "Batch: 666    Epoch[3/3]    Training Loss: 0.31398671865463257\n",
            "Batch: 667    Epoch[3/3]    Training Loss: 0.38783252239227295\n",
            "Batch: 668    Epoch[3/3]    Training Loss: 0.641650378704071\n",
            "Batch: 669    Epoch[3/3]    Training Loss: 0.62790447473526\n",
            "Batch: 670    Epoch[3/3]    Training Loss: 0.10677387565374374\n",
            "Batch: 671    Epoch[3/3]    Training Loss: 0.4187580943107605\n",
            "Batch: 672    Epoch[3/3]    Training Loss: 0.4452536106109619\n",
            "Batch: 673    Epoch[3/3]    Training Loss: 0.4472603499889374\n",
            "Batch: 674    Epoch[3/3]    Training Loss: 0.29480719566345215\n",
            "Batch: 675    Epoch[3/3]    Training Loss: 0.4552091360092163\n",
            "Batch: 676    Epoch[3/3]    Training Loss: 0.43045738339424133\n",
            "Batch: 677    Epoch[3/3]    Training Loss: 1.2690236568450928\n",
            "Batch: 678    Epoch[3/3]    Training Loss: 0.3875047266483307\n",
            "Batch: 679    Epoch[3/3]    Training Loss: 0.4858766794204712\n",
            "Batch: 680    Epoch[3/3]    Training Loss: 0.43785881996154785\n",
            "Batch: 681    Epoch[3/3]    Training Loss: 0.4818193316459656\n",
            "Batch: 682    Epoch[3/3]    Training Loss: 0.41167008876800537\n",
            "Batch: 683    Epoch[3/3]    Training Loss: 0.6989567279815674\n",
            "Batch: 684    Epoch[3/3]    Training Loss: 0.5174840688705444\n",
            "Batch: 685    Epoch[3/3]    Training Loss: 1.0104100704193115\n",
            "Batch: 686    Epoch[3/3]    Training Loss: 0.7101551294326782\n",
            "Batch: 687    Epoch[3/3]    Training Loss: 0.3062424659729004\n",
            "Batch: 688    Epoch[3/3]    Training Loss: 0.4781801998615265\n",
            "Batch: 689    Epoch[3/3]    Training Loss: 0.7538648843765259\n",
            "Batch: 690    Epoch[3/3]    Training Loss: 0.4947166442871094\n",
            "Batch: 691    Epoch[3/3]    Training Loss: 0.8981950283050537\n",
            "Batch: 692    Epoch[3/3]    Training Loss: 0.7941206097602844\n",
            "Batch: 693    Epoch[3/3]    Training Loss: 0.2844947576522827\n",
            "Batch: 694    Epoch[3/3]    Training Loss: 0.5526353716850281\n",
            "Batch: 695    Epoch[3/3]    Training Loss: 0.938551127910614\n",
            "Batch: 696    Epoch[3/3]    Training Loss: 0.6678445339202881\n",
            "Batch: 697    Epoch[3/3]    Training Loss: 0.5780328512191772\n",
            "Batch: 698    Epoch[3/3]    Training Loss: 0.7803652286529541\n",
            "Batch: 699    Epoch[3/3]    Training Loss: 0.5029107332229614\n",
            "Batch: 700    Epoch[3/3]    Training Loss: 1.1990008354187012\n",
            "Batch: 701    Epoch[3/3]    Training Loss: 1.065385103225708\n",
            "Batch: 702    Epoch[3/3]    Training Loss: 0.6018489599227905\n",
            "Batch: 703    Epoch[3/3]    Training Loss: 0.3147091865539551\n",
            "Batch: 704    Epoch[3/3]    Training Loss: 0.2866268754005432\n",
            "Batch: 705    Epoch[3/3]    Training Loss: 0.3315485417842865\n",
            "Batch: 706    Epoch[3/3]    Training Loss: 0.6617907285690308\n",
            "Batch: 707    Epoch[3/3]    Training Loss: 0.8396500945091248\n",
            "Batch: 708    Epoch[3/3]    Training Loss: 0.38910287618637085\n",
            "Batch: 709    Epoch[3/3]    Training Loss: 0.6236237287521362\n",
            "Batch: 710    Epoch[3/3]    Training Loss: 0.3561221957206726\n",
            "Batch: 711    Epoch[3/3]    Training Loss: 0.2580053508281708\n",
            "Batch: 712    Epoch[3/3]    Training Loss: 0.40185266733169556\n",
            "Batch: 713    Epoch[3/3]    Training Loss: 0.5032944083213806\n",
            "Batch: 714    Epoch[3/3]    Training Loss: 0.4583004117012024\n",
            "Batch: 715    Epoch[3/3]    Training Loss: 0.4833548069000244\n",
            "Batch: 716    Epoch[3/3]    Training Loss: 0.4565492868423462\n",
            "Batch: 717    Epoch[3/3]    Training Loss: 0.4572799801826477\n",
            "Batch: 718    Epoch[3/3]    Training Loss: 0.6765073537826538\n",
            "Batch: 719    Epoch[3/3]    Training Loss: 0.6511651873588562\n",
            "Batch: 720    Epoch[3/3]    Training Loss: 0.5775176882743835\n",
            "Batch: 721    Epoch[3/3]    Training Loss: 0.7048415541648865\n",
            "Batch: 722    Epoch[3/3]    Training Loss: 0.8381245136260986\n",
            "Batch: 723    Epoch[3/3]    Training Loss: 0.8722899556159973\n",
            "Batch: 724    Epoch[3/3]    Training Loss: 0.6305368542671204\n",
            "Batch: 725    Epoch[3/3]    Training Loss: 0.46443283557891846\n",
            "Batch: 726    Epoch[3/3]    Training Loss: 0.3856154978275299\n",
            "Batch: 727    Epoch[3/3]    Training Loss: 0.977776050567627\n",
            "Batch: 728    Epoch[3/3]    Training Loss: 0.3539279103279114\n",
            "Batch: 729    Epoch[3/3]    Training Loss: 0.7811349630355835\n",
            "Batch: 730    Epoch[3/3]    Training Loss: 0.5018641948699951\n",
            "Batch: 731    Epoch[3/3]    Training Loss: 0.749973714351654\n",
            "Batch: 732    Epoch[3/3]    Training Loss: 0.21942836046218872\n",
            "Batch: 733    Epoch[3/3]    Training Loss: 1.1905736923217773\n",
            "Batch: 734    Epoch[3/3]    Training Loss: 0.46282872557640076\n",
            "Batch: 735    Epoch[3/3]    Training Loss: 0.6191280484199524\n",
            "Batch: 736    Epoch[3/3]    Training Loss: 0.7147891521453857\n",
            "Batch: 737    Epoch[3/3]    Training Loss: 0.5169974565505981\n",
            "Batch: 738    Epoch[3/3]    Training Loss: 0.5134522318840027\n",
            "Batch: 739    Epoch[3/3]    Training Loss: 1.3913565874099731\n",
            "Batch: 740    Epoch[3/3]    Training Loss: 1.140831470489502\n",
            "Batch: 741    Epoch[3/3]    Training Loss: 0.6612197160720825\n",
            "Batch: 742    Epoch[3/3]    Training Loss: 0.5461596250534058\n",
            "Batch: 743    Epoch[3/3]    Training Loss: 0.4110901653766632\n",
            "Batch: 744    Epoch[3/3]    Training Loss: 0.9999115467071533\n",
            "Batch: 745    Epoch[3/3]    Training Loss: 0.4663652181625366\n",
            "Batch: 746    Epoch[3/3]    Training Loss: 0.5070480108261108\n",
            "Batch: 747    Epoch[3/3]    Training Loss: 0.2555321156978607\n",
            "Batch: 748    Epoch[3/3]    Training Loss: 0.26632195711135864\n",
            "Batch: 749    Epoch[3/3]    Training Loss: 0.3396848142147064\n",
            "Batch: 750    Epoch[3/3]    Training Loss: 0.8231630325317383\n",
            "Batch: 751    Epoch[3/3]    Training Loss: 0.5515154600143433\n",
            "Batch: 752    Epoch[3/3]    Training Loss: 0.59718918800354\n",
            "Batch: 753    Epoch[3/3]    Training Loss: 0.7263182401657104\n",
            "Batch: 754    Epoch[3/3]    Training Loss: 0.5635189414024353\n",
            "Batch: 755    Epoch[3/3]    Training Loss: 0.7235934138298035\n",
            "Batch: 756    Epoch[3/3]    Training Loss: 0.6232620477676392\n",
            "Batch: 757    Epoch[3/3]    Training Loss: 0.5051873922348022\n",
            "Batch: 758    Epoch[3/3]    Training Loss: 0.4452206492424011\n",
            "Batch: 759    Epoch[3/3]    Training Loss: 0.3150135278701782\n",
            "Batch: 760    Epoch[3/3]    Training Loss: 0.32800552248954773\n",
            "Batch: 761    Epoch[3/3]    Training Loss: 0.6071724891662598\n",
            "Batch: 762    Epoch[3/3]    Training Loss: 0.39827653765678406\n",
            "Batch: 763    Epoch[3/3]    Training Loss: 0.42437830567359924\n",
            "Batch: 764    Epoch[3/3]    Training Loss: 0.41620171070098877\n",
            "Batch: 765    Epoch[3/3]    Training Loss: 0.8400242924690247\n",
            "Batch: 766    Epoch[3/3]    Training Loss: 0.23599235713481903\n",
            "Batch: 767    Epoch[3/3]    Training Loss: 0.8065295815467834\n",
            "Batch: 768    Epoch[3/3]    Training Loss: 0.6587195992469788\n",
            "Batch: 769    Epoch[3/3]    Training Loss: 0.18882934749126434\n",
            "Batch: 770    Epoch[3/3]    Training Loss: 0.8902580738067627\n",
            "Batch: 771    Epoch[3/3]    Training Loss: 0.4550707936286926\n",
            "Batch: 772    Epoch[3/3]    Training Loss: 0.39623695611953735\n",
            "Batch: 773    Epoch[3/3]    Training Loss: 0.610632598400116\n",
            "Batch: 774    Epoch[3/3]    Training Loss: 0.22701582312583923\n",
            "Batch: 775    Epoch[3/3]    Training Loss: 0.25899142026901245\n",
            "Batch: 776    Epoch[3/3]    Training Loss: 0.296810507774353\n",
            "Batch: 777    Epoch[3/3]    Training Loss: 0.5195363759994507\n",
            "Batch: 778    Epoch[3/3]    Training Loss: 0.41824615001678467\n",
            "Batch: 779    Epoch[3/3]    Training Loss: 0.9217174053192139\n",
            "Batch: 780    Epoch[3/3]    Training Loss: 0.15372523665428162\n",
            "Batch: 781    Epoch[3/3]    Training Loss: 0.7174777388572693\n",
            "Batch: 782    Epoch[3/3]    Training Loss: 0.7464817762374878\n",
            "Batch: 783    Epoch[3/3]    Training Loss: 0.491468220949173\n",
            "Batch: 784    Epoch[3/3]    Training Loss: 0.47454842925071716\n",
            "Batch: 785    Epoch[3/3]    Training Loss: 0.665490984916687\n",
            "Batch: 786    Epoch[3/3]    Training Loss: 0.2633793354034424\n",
            "Batch: 787    Epoch[3/3]    Training Loss: 0.5933057069778442\n",
            "Batch: 788    Epoch[3/3]    Training Loss: 0.44865506887435913\n",
            "Batch: 789    Epoch[3/3]    Training Loss: 0.6832635998725891\n",
            "Batch: 790    Epoch[3/3]    Training Loss: 0.4186263978481293\n",
            "Batch: 791    Epoch[3/3]    Training Loss: 0.5265441536903381\n",
            "Batch: 792    Epoch[3/3]    Training Loss: 0.9904617071151733\n",
            "Batch: 793    Epoch[3/3]    Training Loss: 0.3794551491737366\n",
            "Batch: 794    Epoch[3/3]    Training Loss: 0.5308277606964111\n",
            "Batch: 795    Epoch[3/3]    Training Loss: 0.5058378577232361\n",
            "Batch: 796    Epoch[3/3]    Training Loss: 0.5083036422729492\n",
            "Batch: 797    Epoch[3/3]    Training Loss: 0.4961406886577606\n",
            "Batch: 798    Epoch[3/3]    Training Loss: 0.43418657779693604\n",
            "Batch: 799    Epoch[3/3]    Training Loss: 0.6805832982063293\n",
            "Batch: 800    Epoch[3/3]    Training Loss: 0.8356262445449829\n",
            "Batch: 801    Epoch[3/3]    Training Loss: 0.4791088104248047\n",
            "Batch: 802    Epoch[3/3]    Training Loss: 0.2699454426765442\n",
            "Batch: 803    Epoch[3/3]    Training Loss: 0.8835744857788086\n",
            "Batch: 804    Epoch[3/3]    Training Loss: 0.41768747568130493\n",
            "Batch: 805    Epoch[3/3]    Training Loss: 0.5580703020095825\n",
            "Batch: 806    Epoch[3/3]    Training Loss: 0.3851483464241028\n",
            "Batch: 807    Epoch[3/3]    Training Loss: 0.41258275508880615\n",
            "Batch: 808    Epoch[3/3]    Training Loss: 0.30665451288223267\n",
            "Batch: 809    Epoch[3/3]    Training Loss: 0.992912769317627\n",
            "Batch: 810    Epoch[3/3]    Training Loss: 0.5710495710372925\n",
            "Batch: 811    Epoch[3/3]    Training Loss: 0.6750214099884033\n",
            "Batch: 812    Epoch[3/3]    Training Loss: 0.8908970355987549\n",
            "Batch: 813    Epoch[3/3]    Training Loss: 1.0468087196350098\n",
            "Batch: 814    Epoch[3/3]    Training Loss: 1.275029182434082\n",
            "Batch: 815    Epoch[3/3]    Training Loss: 0.6274302005767822\n",
            "Batch: 816    Epoch[3/3]    Training Loss: 0.46920809149742126\n",
            "Batch: 817    Epoch[3/3]    Training Loss: 0.6553822755813599\n",
            "Batch: 818    Epoch[3/3]    Training Loss: 0.9205194711685181\n",
            "Batch: 819    Epoch[3/3]    Training Loss: 0.445232093334198\n",
            "Batch: 820    Epoch[3/3]    Training Loss: 0.6532875299453735\n",
            "Batch: 821    Epoch[3/3]    Training Loss: 0.31587958335876465\n",
            "Batch: 822    Epoch[3/3]    Training Loss: 0.5512593388557434\n",
            "Batch: 823    Epoch[3/3]    Training Loss: 0.6946348547935486\n",
            "Batch: 824    Epoch[3/3]    Training Loss: 1.0757896900177002\n",
            "Batch: 825    Epoch[3/3]    Training Loss: 0.6974011659622192\n",
            "Batch: 826    Epoch[3/3]    Training Loss: 0.268163800239563\n",
            "Batch: 827    Epoch[3/3]    Training Loss: 0.5805571675300598\n",
            "Batch: 828    Epoch[3/3]    Training Loss: 0.5303242206573486\n",
            "Batch: 829    Epoch[3/3]    Training Loss: 0.3087782859802246\n",
            "Batch: 830    Epoch[3/3]    Training Loss: 1.190464735031128\n",
            "Batch: 831    Epoch[3/3]    Training Loss: 0.6248411536216736\n",
            "Batch: 832    Epoch[3/3]    Training Loss: 0.8346242904663086\n",
            "Batch: 833    Epoch[3/3]    Training Loss: 0.5243034958839417\n",
            "Batch: 834    Epoch[3/3]    Training Loss: 0.21864846348762512\n",
            "Batch: 835    Epoch[3/3]    Training Loss: 0.4068034887313843\n",
            "Batch: 836    Epoch[3/3]    Training Loss: 0.32737892866134644\n",
            "Batch: 837    Epoch[3/3]    Training Loss: 0.34406614303588867\n",
            "Batch: 838    Epoch[3/3]    Training Loss: 0.6176645755767822\n",
            "Batch: 839    Epoch[3/3]    Training Loss: 0.5803368091583252\n",
            "Batch: 840    Epoch[3/3]    Training Loss: 0.5845531225204468\n",
            "Batch: 841    Epoch[3/3]    Training Loss: 0.6892462968826294\n",
            "Batch: 842    Epoch[3/3]    Training Loss: 0.3235452175140381\n",
            "Batch: 843    Epoch[3/3]    Training Loss: 0.6980017423629761\n",
            "Batch: 844    Epoch[3/3]    Training Loss: 0.45080041885375977\n",
            "Batch: 845    Epoch[3/3]    Training Loss: 0.28451356291770935\n",
            "Batch: 846    Epoch[3/3]    Training Loss: 0.7380715608596802\n",
            "Batch: 847    Epoch[3/3]    Training Loss: 0.574082612991333\n",
            "Batch: 848    Epoch[3/3]    Training Loss: 0.5166863203048706\n",
            "Batch: 849    Epoch[3/3]    Training Loss: 0.7540080547332764\n",
            "Batch: 850    Epoch[3/3]    Training Loss: 0.5302789807319641\n",
            "Batch: 851    Epoch[3/3]    Training Loss: 0.7684102058410645\n",
            "Batch: 852    Epoch[3/3]    Training Loss: 0.4621826410293579\n",
            "Batch: 853    Epoch[3/3]    Training Loss: 0.8690099716186523\n",
            "Batch: 854    Epoch[3/3]    Training Loss: 0.3898632526397705\n",
            "Batch: 855    Epoch[3/3]    Training Loss: 0.4980509877204895\n",
            "Batch: 856    Epoch[3/3]    Training Loss: 0.6426001191139221\n",
            "Batch: 857    Epoch[3/3]    Training Loss: 0.46746188402175903\n",
            "Batch: 858    Epoch[3/3]    Training Loss: 0.3412790894508362\n",
            "Batch: 859    Epoch[3/3]    Training Loss: 0.7124249935150146\n",
            "Batch: 860    Epoch[3/3]    Training Loss: 0.735607385635376\n",
            "Batch: 861    Epoch[3/3]    Training Loss: 0.38664209842681885\n",
            "Batch: 862    Epoch[3/3]    Training Loss: 0.32580649852752686\n",
            "Batch: 863    Epoch[3/3]    Training Loss: 0.4200228154659271\n",
            "Batch: 864    Epoch[3/3]    Training Loss: 0.735133171081543\n",
            "Batch: 865    Epoch[3/3]    Training Loss: 0.20550891757011414\n",
            "Batch: 866    Epoch[3/3]    Training Loss: 0.44851434230804443\n",
            "Batch: 867    Epoch[3/3]    Training Loss: 0.6511996984481812\n",
            "Batch: 868    Epoch[3/3]    Training Loss: 0.7916457653045654\n",
            "Batch: 869    Epoch[3/3]    Training Loss: 0.37338000535964966\n",
            "Batch: 870    Epoch[3/3]    Training Loss: 0.2715069055557251\n",
            "Batch: 871    Epoch[3/3]    Training Loss: 0.9531759023666382\n",
            "Batch: 872    Epoch[3/3]    Training Loss: 0.48149949312210083\n",
            "Batch: 873    Epoch[3/3]    Training Loss: 0.7126761078834534\n",
            "Batch: 874    Epoch[3/3]    Training Loss: 0.53786700963974\n",
            "Batch: 875    Epoch[3/3]    Training Loss: 0.5609623193740845\n",
            "Batch: 876    Epoch[3/3]    Training Loss: 0.34492161870002747\n",
            "Batch: 877    Epoch[3/3]    Training Loss: 0.535486102104187\n",
            "Batch: 878    Epoch[3/3]    Training Loss: 0.7013134360313416\n",
            "Batch: 879    Epoch[3/3]    Training Loss: 0.749015212059021\n",
            "Batch: 880    Epoch[3/3]    Training Loss: 0.6817601323127747\n",
            "Batch: 881    Epoch[3/3]    Training Loss: 0.658333420753479\n",
            "Batch: 882    Epoch[3/3]    Training Loss: 1.101606845855713\n",
            "Batch: 883    Epoch[3/3]    Training Loss: 0.3596397638320923\n",
            "Batch: 884    Epoch[3/3]    Training Loss: 0.32231754064559937\n",
            "Batch: 885    Epoch[3/3]    Training Loss: 0.8231337070465088\n",
            "Batch: 886    Epoch[3/3]    Training Loss: 0.26890838146209717\n",
            "Batch: 887    Epoch[3/3]    Training Loss: 0.4096454977989197\n",
            "Batch: 888    Epoch[3/3]    Training Loss: 0.29631462693214417\n",
            "Batch: 889    Epoch[3/3]    Training Loss: 0.34979933500289917\n",
            "Batch: 890    Epoch[3/3]    Training Loss: 0.6738659739494324\n",
            "Batch: 891    Epoch[3/3]    Training Loss: 0.4632694125175476\n",
            "Batch: 892    Epoch[3/3]    Training Loss: 1.2554478645324707\n",
            "Batch: 893    Epoch[3/3]    Training Loss: 0.7238327264785767\n",
            "Batch: 894    Epoch[3/3]    Training Loss: 0.3926658630371094\n",
            "Batch: 895    Epoch[3/3]    Training Loss: 0.6308841705322266\n",
            "Batch: 896    Epoch[3/3]    Training Loss: 0.5584070682525635\n",
            "Batch: 897    Epoch[3/3]    Training Loss: 0.8494265079498291\n",
            "Batch: 898    Epoch[3/3]    Training Loss: 0.516474723815918\n",
            "Batch: 899    Epoch[3/3]    Training Loss: 0.5517827272415161\n",
            "Batch: 900    Epoch[3/3]    Training Loss: 0.6817456483840942\n",
            "Batch: 901    Epoch[3/3]    Training Loss: 0.6772788763046265\n",
            "Batch: 902    Epoch[3/3]    Training Loss: 0.2119539976119995\n",
            "Batch: 903    Epoch[3/3]    Training Loss: 0.24815493822097778\n",
            "Batch: 904    Epoch[3/3]    Training Loss: 0.3679346442222595\n",
            "Batch: 905    Epoch[3/3]    Training Loss: 0.7472661733627319\n",
            "Batch: 906    Epoch[3/3]    Training Loss: 0.4291554391384125\n",
            "Batch: 907    Epoch[3/3]    Training Loss: 0.2636343836784363\n",
            "Batch: 908    Epoch[3/3]    Training Loss: 0.40773528814315796\n",
            "Batch: 909    Epoch[3/3]    Training Loss: 0.3459630012512207\n",
            "Batch: 910    Epoch[3/3]    Training Loss: 0.8816527724266052\n",
            "Batch: 911    Epoch[3/3]    Training Loss: 0.3618205189704895\n",
            "Batch: 912    Epoch[3/3]    Training Loss: 0.49185317754745483\n",
            "Batch: 913    Epoch[3/3]    Training Loss: 0.7017139196395874\n",
            "Batch: 914    Epoch[3/3]    Training Loss: 0.1804269254207611\n",
            "Batch: 915    Epoch[3/3]    Training Loss: 0.38475292921066284\n",
            "Batch: 916    Epoch[3/3]    Training Loss: 0.5088820457458496\n",
            "Batch: 917    Epoch[3/3]    Training Loss: 0.8092786073684692\n",
            "Batch: 918    Epoch[3/3]    Training Loss: 0.3110513687133789\n",
            "Batch: 919    Epoch[3/3]    Training Loss: 0.44287386536598206\n",
            "Batch: 920    Epoch[3/3]    Training Loss: 1.3630279302597046\n",
            "Batch: 921    Epoch[3/3]    Training Loss: 0.8977923393249512\n",
            "Batch: 922    Epoch[3/3]    Training Loss: 0.5182098746299744\n",
            "Batch: 923    Epoch[3/3]    Training Loss: 0.5753265023231506\n",
            "Batch: 924    Epoch[3/3]    Training Loss: 0.7682685256004333\n",
            "Batch: 925    Epoch[3/3]    Training Loss: 0.7129313945770264\n",
            "Batch: 926    Epoch[3/3]    Training Loss: 0.38680583238601685\n",
            "Batch: 927    Epoch[3/3]    Training Loss: 0.23472464084625244\n",
            "Batch: 928    Epoch[3/3]    Training Loss: 0.2541666030883789\n",
            "Batch: 929    Epoch[3/3]    Training Loss: 0.6762489080429077\n",
            "Batch: 930    Epoch[3/3]    Training Loss: 0.5739948749542236\n",
            "Batch: 931    Epoch[3/3]    Training Loss: 0.3682538866996765\n",
            "Batch: 932    Epoch[3/3]    Training Loss: 0.9342488050460815\n",
            "Batch: 933    Epoch[3/3]    Training Loss: 0.384816437959671\n",
            "Batch: 934    Epoch[3/3]    Training Loss: 0.6814796924591064\n",
            "Batch: 935    Epoch[3/3]    Training Loss: 0.3908432126045227\n",
            "Batch: 936    Epoch[3/3]    Training Loss: 0.29974937438964844\n",
            "Batch: 937    Epoch[3/3]    Training Loss: 0.646359920501709\n",
            "Batch: 938    Epoch[3/3]    Training Loss: 0.44904324412345886\n",
            "Batch: 939    Epoch[3/3]    Training Loss: 0.319441020488739\n",
            "Batch: 940    Epoch[3/3]    Training Loss: 0.6522840857505798\n",
            "Batch: 941    Epoch[3/3]    Training Loss: 0.338800847530365\n",
            "Batch: 942    Epoch[3/3]    Training Loss: 0.5402522683143616\n",
            "Batch: 943    Epoch[3/3]    Training Loss: 0.8338656425476074\n",
            "Batch: 944    Epoch[3/3]    Training Loss: 0.48212000727653503\n",
            "Batch: 945    Epoch[3/3]    Training Loss: 0.6224613189697266\n",
            "Batch: 946    Epoch[3/3]    Training Loss: 0.5797520875930786\n",
            "Batch: 947    Epoch[3/3]    Training Loss: 1.004032850265503\n",
            "Batch: 948    Epoch[3/3]    Training Loss: 0.6058794260025024\n",
            "Batch: 949    Epoch[3/3]    Training Loss: 0.2872673571109772\n",
            "Batch: 950    Epoch[3/3]    Training Loss: 0.791115939617157\n",
            "Batch: 951    Epoch[3/3]    Training Loss: 0.62420254945755\n",
            "Batch: 952    Epoch[3/3]    Training Loss: 0.3633438050746918\n",
            "Batch: 953    Epoch[3/3]    Training Loss: 0.46425580978393555\n",
            "Batch: 954    Epoch[3/3]    Training Loss: 0.3423072099685669\n",
            "Batch: 955    Epoch[3/3]    Training Loss: 0.8205907344818115\n",
            "Batch: 956    Epoch[3/3]    Training Loss: 0.76033616065979\n",
            "Batch: 957    Epoch[3/3]    Training Loss: 0.6834880113601685\n",
            "Batch: 958    Epoch[3/3]    Training Loss: 0.6977672576904297\n",
            "Batch: 959    Epoch[3/3]    Training Loss: 0.4213159382343292\n",
            "Batch: 960    Epoch[3/3]    Training Loss: 0.36602383852005005\n",
            "Batch: 961    Epoch[3/3]    Training Loss: 0.4796808362007141\n",
            "Batch: 962    Epoch[3/3]    Training Loss: 0.8276761770248413\n",
            "Batch: 963    Epoch[3/3]    Training Loss: 0.8853799104690552\n",
            "Batch: 964    Epoch[3/3]    Training Loss: 0.2983156442642212\n",
            "Batch: 965    Epoch[3/3]    Training Loss: 0.48957812786102295\n",
            "Batch: 966    Epoch[3/3]    Training Loss: 0.6254042387008667\n",
            "Batch: 967    Epoch[3/3]    Training Loss: 0.3934439420700073\n",
            "Batch: 968    Epoch[3/3]    Training Loss: 0.38821810483932495\n",
            "Batch: 969    Epoch[3/3]    Training Loss: 0.4894122779369354\n",
            "Batch: 970    Epoch[3/3]    Training Loss: 0.8270477056503296\n",
            "Batch: 971    Epoch[3/3]    Training Loss: 0.65349942445755\n",
            "Batch: 972    Epoch[3/3]    Training Loss: 0.6392204165458679\n",
            "Batch: 973    Epoch[3/3]    Training Loss: 0.561648964881897\n",
            "Batch: 974    Epoch[3/3]    Training Loss: 0.3238335847854614\n",
            "Batch: 975    Epoch[3/3]    Training Loss: 0.3775498569011688\n",
            "Batch: 976    Epoch[3/3]    Training Loss: 0.7691378593444824\n",
            "Batch: 977    Epoch[3/3]    Training Loss: 0.5290231704711914\n",
            "Batch: 978    Epoch[3/3]    Training Loss: 0.35598212480545044\n",
            "Batch: 979    Epoch[3/3]    Training Loss: 0.2670362591743469\n",
            "Batch: 980    Epoch[3/3]    Training Loss: 0.7501367330551147\n",
            "Batch: 981    Epoch[3/3]    Training Loss: 0.37786418199539185\n",
            "Batch: 982    Epoch[3/3]    Training Loss: 0.5716262459754944\n",
            "Batch: 983    Epoch[3/3]    Training Loss: 0.8596386909484863\n",
            "Batch: 984    Epoch[3/3]    Training Loss: 0.693293035030365\n",
            "Batch: 985    Epoch[3/3]    Training Loss: 0.6453475952148438\n",
            "Batch: 986    Epoch[3/3]    Training Loss: 0.3322875499725342\n",
            "Batch: 987    Epoch[3/3]    Training Loss: 0.854638934135437\n",
            "Batch: 988    Epoch[3/3]    Training Loss: 0.2524331212043762\n",
            "Batch: 989    Epoch[3/3]    Training Loss: 0.09249595552682877\n",
            "Batch: 990    Epoch[3/3]    Training Loss: 0.673485279083252\n",
            "Batch: 991    Epoch[3/3]    Training Loss: 0.796034574508667\n",
            "Batch: 992    Epoch[3/3]    Training Loss: 0.3897223472595215\n",
            "Batch: 993    Epoch[3/3]    Training Loss: 0.468030720949173\n",
            "Batch: 994    Epoch[3/3]    Training Loss: 0.6682291030883789\n",
            "Batch: 995    Epoch[3/3]    Training Loss: 0.878419041633606\n",
            "Batch: 996    Epoch[3/3]    Training Loss: 0.43533623218536377\n",
            "Batch: 997    Epoch[3/3]    Training Loss: 0.23730753362178802\n",
            "Batch: 998    Epoch[3/3]    Training Loss: 0.6747627854347229\n",
            "Batch: 999    Epoch[3/3]    Training Loss: 0.5279086828231812\n",
            "Batch: 1000    Epoch[3/3]    Training Loss: 0.560295581817627\n",
            "Batch: 1001    Epoch[3/3]    Training Loss: 0.4087607264518738\n",
            "Batch: 1002    Epoch[3/3]    Training Loss: 0.8651156425476074\n",
            "Batch: 1003    Epoch[3/3]    Training Loss: 0.4153628349304199\n",
            "Batch: 1004    Epoch[3/3]    Training Loss: 0.566503643989563\n",
            "Batch: 1005    Epoch[3/3]    Training Loss: 0.8477349877357483\n",
            "Batch: 1006    Epoch[3/3]    Training Loss: 0.694572925567627\n",
            "Batch: 1007    Epoch[3/3]    Training Loss: 0.3041146695613861\n",
            "Batch: 1008    Epoch[3/3]    Training Loss: 0.5688064098358154\n",
            "Batch: 1009    Epoch[3/3]    Training Loss: 1.3203256130218506\n",
            "Batch: 1010    Epoch[3/3]    Training Loss: 0.26699143648147583\n",
            "Batch: 1011    Epoch[3/3]    Training Loss: 1.0807346105575562\n",
            "Batch: 1012    Epoch[3/3]    Training Loss: 1.107621192932129\n",
            "Batch: 1013    Epoch[3/3]    Training Loss: 0.491788387298584\n",
            "Batch: 1014    Epoch[3/3]    Training Loss: 0.7035650014877319\n",
            "Batch: 1015    Epoch[3/3]    Training Loss: 0.6606224775314331\n",
            "Batch: 1016    Epoch[3/3]    Training Loss: 0.29654332995414734\n",
            "Batch: 1017    Epoch[3/3]    Training Loss: 0.5617560148239136\n",
            "Batch: 1018    Epoch[3/3]    Training Loss: 0.8224596977233887\n",
            "Batch: 1019    Epoch[3/3]    Training Loss: 0.6344490647315979\n",
            "Batch: 1020    Epoch[3/3]    Training Loss: 0.6181729435920715\n",
            "Batch: 1021    Epoch[3/3]    Training Loss: 0.46451786160469055\n",
            "Batch: 1022    Epoch[3/3]    Training Loss: 1.58222234249115\n",
            "Batch: 1023    Epoch[3/3]    Training Loss: 0.6259450316429138\n",
            "Batch: 1024    Epoch[3/3]    Training Loss: 0.529431939125061\n",
            "Batch: 1025    Epoch[3/3]    Training Loss: 0.29490894079208374\n",
            "Batch: 1026    Epoch[3/3]    Training Loss: 0.5482010841369629\n",
            "Batch: 1027    Epoch[3/3]    Training Loss: 0.6160922050476074\n",
            "Batch: 1028    Epoch[3/3]    Training Loss: 0.597551703453064\n",
            "Batch: 1029    Epoch[3/3]    Training Loss: 0.7739591598510742\n",
            "Batch: 1030    Epoch[3/3]    Training Loss: 0.31945812702178955\n",
            "Batch: 1031    Epoch[3/3]    Training Loss: 0.5057609677314758\n",
            "Batch: 1032    Epoch[3/3]    Training Loss: 0.36925697326660156\n",
            "Batch: 1033    Epoch[3/3]    Training Loss: 0.8398172855377197\n",
            "Batch: 1034    Epoch[3/3]    Training Loss: 0.6723862886428833\n",
            "Batch: 1035    Epoch[3/3]    Training Loss: 0.6892475485801697\n",
            "Batch: 1036    Epoch[3/3]    Training Loss: 0.9409658908843994\n",
            "Batch: 1037    Epoch[3/3]    Training Loss: 0.36996904015541077\n",
            "Batch: 1038    Epoch[3/3]    Training Loss: 0.7666288614273071\n",
            "Batch: 1039    Epoch[3/3]    Training Loss: 0.5055450797080994\n",
            "Batch: 1040    Epoch[3/3]    Training Loss: 0.6642066836357117\n",
            "Batch: 1041    Epoch[3/3]    Training Loss: 0.30965456366539\n",
            "Batch: 1042    Epoch[3/3]    Training Loss: 0.7473804950714111\n",
            "Batch: 1043    Epoch[3/3]    Training Loss: 0.8537240028381348\n",
            "Batch: 1044    Epoch[3/3]    Training Loss: 0.30475273728370667\n",
            "Batch: 1045    Epoch[3/3]    Training Loss: 0.73676598072052\n",
            "Batch: 1046    Epoch[3/3]    Training Loss: 0.811072051525116\n",
            "Batch: 1047    Epoch[3/3]    Training Loss: 0.9715533256530762\n",
            "Batch: 1048    Epoch[3/3]    Training Loss: 0.3848927617073059\n",
            "Batch: 1049    Epoch[3/3]    Training Loss: 0.7286444306373596\n",
            "Batch: 1050    Epoch[3/3]    Training Loss: 0.3654174208641052\n",
            "Batch: 1051    Epoch[3/3]    Training Loss: 0.6367364525794983\n",
            "Batch: 1052    Epoch[3/3]    Training Loss: 0.5032711029052734\n",
            "Batch: 1053    Epoch[3/3]    Training Loss: 0.603206217288971\n",
            "Batch: 1054    Epoch[3/3]    Training Loss: 0.32331550121307373\n",
            "Batch: 1055    Epoch[3/3]    Training Loss: 0.7783876657485962\n",
            "Batch: 1056    Epoch[3/3]    Training Loss: 0.6903278827667236\n",
            "Batch: 1057    Epoch[3/3]    Training Loss: 0.3702195882797241\n",
            "Batch: 1058    Epoch[3/3]    Training Loss: 0.2648749351501465\n",
            "Batch: 1059    Epoch[3/3]    Training Loss: 1.0974791049957275\n",
            "Batch: 1060    Epoch[3/3]    Training Loss: 0.5248688459396362\n",
            "Batch: 1061    Epoch[3/3]    Training Loss: 0.48259061574935913\n",
            "Batch: 1062    Epoch[3/3]    Training Loss: 0.37679535150527954\n",
            "Batch: 1063    Epoch[3/3]    Training Loss: 0.7097035050392151\n",
            "Batch: 1064    Epoch[3/3]    Training Loss: 0.7524838447570801\n",
            "Batch: 1065    Epoch[3/3]    Training Loss: 0.49469539523124695\n",
            "Batch: 1066    Epoch[3/3]    Training Loss: 0.6967750787734985\n",
            "Batch: 1067    Epoch[3/3]    Training Loss: 0.645520806312561\n",
            "Batch: 1068    Epoch[3/3]    Training Loss: 0.6635019779205322\n",
            "Batch: 1069    Epoch[3/3]    Training Loss: 0.17894387245178223\n",
            "Batch: 1070    Epoch[3/3]    Training Loss: 0.7066609859466553\n",
            "Batch: 1071    Epoch[3/3]    Training Loss: 0.6618907451629639\n",
            "Batch: 1072    Epoch[3/3]    Training Loss: 0.5922993421554565\n",
            "Batch: 1073    Epoch[3/3]    Training Loss: 0.8034352660179138\n",
            "Batch: 1074    Epoch[3/3]    Training Loss: 0.6592541933059692\n",
            "Batch: 1075    Epoch[3/3]    Training Loss: 0.418119877576828\n",
            "Batch: 1076    Epoch[3/3]    Training Loss: 0.78730309009552\n",
            "Batch: 1077    Epoch[3/3]    Training Loss: 0.3647523522377014\n",
            "Batch: 1078    Epoch[3/3]    Training Loss: 0.7550895810127258\n",
            "Batch: 1079    Epoch[3/3]    Training Loss: 0.28916430473327637\n",
            "Batch: 1080    Epoch[3/3]    Training Loss: 1.0566920042037964\n",
            "Batch: 1081    Epoch[3/3]    Training Loss: 0.2111179381608963\n",
            "Batch: 1082    Epoch[3/3]    Training Loss: 0.4158315360546112\n",
            "Batch: 1083    Epoch[3/3]    Training Loss: 0.5921547412872314\n",
            "Batch: 1084    Epoch[3/3]    Training Loss: 1.2527639865875244\n",
            "Batch: 1085    Epoch[3/3]    Training Loss: 0.5278153419494629\n",
            "Batch: 1086    Epoch[3/3]    Training Loss: 0.2612605094909668\n",
            "Batch: 1087    Epoch[3/3]    Training Loss: 0.8156650066375732\n",
            "Batch: 1088    Epoch[3/3]    Training Loss: 0.24426493048667908\n",
            "Batch: 1089    Epoch[3/3]    Training Loss: 0.5119640827178955\n",
            "Batch: 1090    Epoch[3/3]    Training Loss: 0.7698699235916138\n",
            "Batch: 1091    Epoch[3/3]    Training Loss: 0.4058820605278015\n",
            "Batch: 1092    Epoch[3/3]    Training Loss: 0.3739725947380066\n",
            "Batch: 1093    Epoch[3/3]    Training Loss: 0.602935791015625\n",
            "Batch: 1094    Epoch[3/3]    Training Loss: 0.7552108764648438\n",
            "Batch: 1095    Epoch[3/3]    Training Loss: 0.30871227383613586\n",
            "Batch: 1096    Epoch[3/3]    Training Loss: 0.22839687764644623\n",
            "Batch: 1097    Epoch[3/3]    Training Loss: 0.3254707157611847\n",
            "Batch: 1098    Epoch[3/3]    Training Loss: 0.30063673853874207\n",
            "Batch: 1099    Epoch[3/3]    Training Loss: 0.4247734546661377\n",
            "Batch: 1100    Epoch[3/3]    Training Loss: 0.6242037415504456\n",
            "Batch: 1101    Epoch[3/3]    Training Loss: 0.5679559707641602\n",
            "Batch: 1102    Epoch[3/3]    Training Loss: 0.5607758164405823\n",
            "Batch: 1103    Epoch[3/3]    Training Loss: 0.6016592383384705\n",
            "Batch: 1104    Epoch[3/3]    Training Loss: 0.326530396938324\n",
            "Batch: 1105    Epoch[3/3]    Training Loss: 0.3632659316062927\n",
            "Batch: 1106    Epoch[3/3]    Training Loss: 0.7238146066665649\n",
            "Batch: 1107    Epoch[3/3]    Training Loss: 0.26732414960861206\n",
            "Batch: 1108    Epoch[3/3]    Training Loss: 0.42107051610946655\n",
            "Batch: 1109    Epoch[3/3]    Training Loss: 0.4251609742641449\n",
            "Batch: 1110    Epoch[3/3]    Training Loss: 0.5167238712310791\n",
            "Batch: 1111    Epoch[3/3]    Training Loss: 0.34491339325904846\n",
            "Batch: 1112    Epoch[3/3]    Training Loss: 0.5740947723388672\n",
            "Batch: 1113    Epoch[3/3]    Training Loss: 0.4941364526748657\n",
            "Batch: 1114    Epoch[3/3]    Training Loss: 0.48562365770339966\n",
            "Batch: 1115    Epoch[3/3]    Training Loss: 0.3343413472175598\n",
            "Batch: 1116    Epoch[3/3]    Training Loss: 0.4616438150405884\n",
            "Batch: 1117    Epoch[3/3]    Training Loss: 0.517920196056366\n",
            "Batch: 1118    Epoch[3/3]    Training Loss: 0.40028297901153564\n",
            "Batch: 1119    Epoch[3/3]    Training Loss: 0.5411808490753174\n",
            "Batch: 1120    Epoch[3/3]    Training Loss: 1.1343953609466553\n",
            "Batch: 1121    Epoch[3/3]    Training Loss: 0.35857367515563965\n",
            "Batch: 1122    Epoch[3/3]    Training Loss: 0.601811945438385\n",
            "Batch: 1123    Epoch[3/3]    Training Loss: 0.5416793823242188\n",
            "Batch: 1124    Epoch[3/3]    Training Loss: 0.6632704138755798\n",
            "Batch: 1125    Epoch[3/3]    Training Loss: 0.4845343828201294\n",
            "Batch: 1126    Epoch[3/3]    Training Loss: 0.3944181799888611\n",
            "Batch: 1127    Epoch[3/3]    Training Loss: 1.030419111251831\n",
            "Batch: 1128    Epoch[3/3]    Training Loss: 0.32691648602485657\n",
            "Batch: 1129    Epoch[3/3]    Training Loss: 0.9315138459205627\n",
            "Batch: 1130    Epoch[3/3]    Training Loss: 0.9592411518096924\n",
            "Batch: 1131    Epoch[3/3]    Training Loss: 0.3941264748573303\n",
            "Batch: 1132    Epoch[3/3]    Training Loss: 0.27048736810684204\n",
            "Batch: 1133    Epoch[3/3]    Training Loss: 0.972968339920044\n",
            "Batch: 1134    Epoch[3/3]    Training Loss: 0.9647122621536255\n",
            "Batch: 1135    Epoch[3/3]    Training Loss: 1.1232826709747314\n",
            "Batch: 1136    Epoch[3/3]    Training Loss: 0.2650916874408722\n",
            "Batch: 1137    Epoch[3/3]    Training Loss: 0.261446088552475\n",
            "Batch: 1138    Epoch[3/3]    Training Loss: 0.16011108458042145\n",
            "Batch: 1139    Epoch[3/3]    Training Loss: 0.39034128189086914\n",
            "Batch: 1140    Epoch[3/3]    Training Loss: 0.7762020826339722\n",
            "Batch: 1141    Epoch[3/3]    Training Loss: 0.7580322623252869\n",
            "Batch: 1142    Epoch[3/3]    Training Loss: 0.5413336157798767\n",
            "Batch: 1143    Epoch[3/3]    Training Loss: 0.5294169783592224\n",
            "Batch: 1144    Epoch[3/3]    Training Loss: 0.7855516672134399\n",
            "Batch: 1145    Epoch[3/3]    Training Loss: 0.6151925325393677\n",
            "Batch: 1146    Epoch[3/3]    Training Loss: 0.5230079889297485\n",
            "Batch: 1147    Epoch[3/3]    Training Loss: 0.3370615839958191\n",
            "Batch: 1148    Epoch[3/3]    Training Loss: 0.8925167322158813\n",
            "Batch: 1149    Epoch[3/3]    Training Loss: 0.44414210319519043\n",
            "Batch: 1150    Epoch[3/3]    Training Loss: 0.9261786937713623\n",
            "Batch: 1151    Epoch[3/3]    Training Loss: 0.37426894903182983\n",
            "Batch: 1152    Epoch[3/3]    Training Loss: 0.3645956814289093\n",
            "Batch: 1153    Epoch[3/3]    Training Loss: 0.34866684675216675\n",
            "Batch: 1154    Epoch[3/3]    Training Loss: 0.676773726940155\n",
            "Batch: 1155    Epoch[3/3]    Training Loss: 0.4208650290966034\n",
            "Batch: 1156    Epoch[3/3]    Training Loss: 0.18375074863433838\n",
            "Batch: 1157    Epoch[3/3]    Training Loss: 0.48058873414993286\n",
            "Batch: 1158    Epoch[3/3]    Training Loss: 0.5202041864395142\n",
            "Batch: 1159    Epoch[3/3]    Training Loss: 0.46382027864456177\n",
            "Batch: 1160    Epoch[3/3]    Training Loss: 0.3168175518512726\n",
            "Batch: 1161    Epoch[3/3]    Training Loss: 0.3659423887729645\n",
            "Batch: 1162    Epoch[3/3]    Training Loss: 1.2363848686218262\n",
            "Batch: 1163    Epoch[3/3]    Training Loss: 0.519201934337616\n",
            "Batch: 1164    Epoch[3/3]    Training Loss: 0.36288467049598694\n",
            "Batch: 1165    Epoch[3/3]    Training Loss: 0.2834859788417816\n",
            "Batch: 1166    Epoch[3/3]    Training Loss: 0.3803770840167999\n",
            "Batch: 1167    Epoch[3/3]    Training Loss: 0.4433509111404419\n",
            "Batch: 1168    Epoch[3/3]    Training Loss: 0.589953601360321\n",
            "Batch: 1169    Epoch[3/3]    Training Loss: 0.4857289791107178\n",
            "Batch: 1170    Epoch[3/3]    Training Loss: 0.733318030834198\n",
            "Batch: 1171    Epoch[3/3]    Training Loss: 0.7755118608474731\n",
            "Batch: 1172    Epoch[3/3]    Training Loss: 0.46432411670684814\n",
            "Batch: 1173    Epoch[3/3]    Training Loss: 0.5941136479377747\n",
            "Batch: 1174    Epoch[3/3]    Training Loss: 0.6276683807373047\n",
            "Batch: 1175    Epoch[3/3]    Training Loss: 0.2573639154434204\n",
            "Batch: 1176    Epoch[3/3]    Training Loss: 0.4501052498817444\n",
            "Batch: 1177    Epoch[3/3]    Training Loss: 1.2468898296356201\n",
            "Batch: 1178    Epoch[3/3]    Training Loss: 0.6237349510192871\n",
            "Batch: 1179    Epoch[3/3]    Training Loss: 1.0788511037826538\n",
            "Batch: 1180    Epoch[3/3]    Training Loss: 0.28208255767822266\n",
            "Batch: 1181    Epoch[3/3]    Training Loss: 0.37523436546325684\n",
            "Batch: 1182    Epoch[3/3]    Training Loss: 0.2581910490989685\n",
            "Batch: 1183    Epoch[3/3]    Training Loss: 0.88547283411026\n",
            "Batch: 1184    Epoch[3/3]    Training Loss: 0.5018077492713928\n",
            "Batch: 1185    Epoch[3/3]    Training Loss: 0.5293476581573486\n",
            "Batch: 1186    Epoch[3/3]    Training Loss: 0.2584354877471924\n",
            "Batch: 1187    Epoch[3/3]    Training Loss: 0.7559372782707214\n",
            "Batch: 1188    Epoch[3/3]    Training Loss: 0.16463956236839294\n",
            "Batch: 1189    Epoch[3/3]    Training Loss: 0.9867860078811646\n",
            "Batch: 1190    Epoch[3/3]    Training Loss: 0.6680864095687866\n",
            "Batch: 1191    Epoch[3/3]    Training Loss: 0.82886803150177\n",
            "Batch: 1192    Epoch[3/3]    Training Loss: 0.5815321207046509\n",
            "Batch: 1193    Epoch[3/3]    Training Loss: 0.4727492332458496\n",
            "Batch: 1194    Epoch[3/3]    Training Loss: 0.6656012535095215\n",
            "Batch: 1195    Epoch[3/3]    Training Loss: 0.40356674790382385\n",
            "Batch: 1196    Epoch[3/3]    Training Loss: 0.7561602592468262\n",
            "Batch: 1197    Epoch[3/3]    Training Loss: 0.8643480539321899\n",
            "Batch: 1198    Epoch[3/3]    Training Loss: 0.286117285490036\n",
            "Batch: 1199    Epoch[3/3]    Training Loss: 0.4649791717529297\n",
            "Batch: 1200    Epoch[3/3]    Training Loss: 0.938274085521698\n",
            "Batch: 1201    Epoch[3/3]    Training Loss: 0.43096500635147095\n",
            "Batch: 1202    Epoch[3/3]    Training Loss: 0.6891859769821167\n",
            "Batch: 1203    Epoch[3/3]    Training Loss: 0.7303196787834167\n",
            "Batch: 1204    Epoch[3/3]    Training Loss: 0.3735595941543579\n",
            "Batch: 1205    Epoch[3/3]    Training Loss: 0.8159959316253662\n",
            "Batch: 1206    Epoch[3/3]    Training Loss: 0.7364703416824341\n",
            "Batch: 1207    Epoch[3/3]    Training Loss: 0.8082162141799927\n",
            "Batch: 1208    Epoch[3/3]    Training Loss: 0.9704607129096985\n",
            "Batch: 1209    Epoch[3/3]    Training Loss: 0.8924916386604309\n",
            "Batch: 1210    Epoch[3/3]    Training Loss: 0.6284224987030029\n",
            "Batch: 1211    Epoch[3/3]    Training Loss: 0.4582234025001526\n",
            "Batch: 1212    Epoch[3/3]    Training Loss: 0.3367854356765747\n",
            "Batch: 1213    Epoch[3/3]    Training Loss: 0.5217305421829224\n",
            "Batch: 1214    Epoch[3/3]    Training Loss: 0.30502474308013916\n",
            "Batch: 1215    Epoch[3/3]    Training Loss: 0.35536831617355347\n",
            "Batch: 1216    Epoch[3/3]    Training Loss: 0.4574015736579895\n",
            "Batch: 1217    Epoch[3/3]    Training Loss: 0.5774053931236267\n",
            "Batch: 1218    Epoch[3/3]    Training Loss: 0.23581087589263916\n",
            "Batch: 1219    Epoch[3/3]    Training Loss: 1.1353678703308105\n",
            "Batch: 1220    Epoch[3/3]    Training Loss: 0.9655047059059143\n",
            "Batch: 1221    Epoch[3/3]    Training Loss: 0.47920775413513184\n",
            "Batch: 1222    Epoch[3/3]    Training Loss: 0.8220745325088501\n",
            "Batch: 1223    Epoch[3/3]    Training Loss: 0.2888314127922058\n",
            "Batch: 1224    Epoch[3/3]    Training Loss: 0.3256293535232544\n",
            "Batch: 1225    Epoch[3/3]    Training Loss: 0.7116182446479797\n",
            "Batch: 1226    Epoch[3/3]    Training Loss: 0.7126012444496155\n",
            "Batch: 1227    Epoch[3/3]    Training Loss: 0.5671248435974121\n",
            "Batch: 1228    Epoch[3/3]    Training Loss: 0.8840612173080444\n",
            "Batch: 1229    Epoch[3/3]    Training Loss: 0.4622054696083069\n",
            "Batch: 1230    Epoch[3/3]    Training Loss: 0.3120993673801422\n",
            "Batch: 1231    Epoch[3/3]    Training Loss: 0.5339133739471436\n",
            "Batch: 1232    Epoch[3/3]    Training Loss: 0.5003860592842102\n",
            "Batch: 1233    Epoch[3/3]    Training Loss: 0.822655200958252\n",
            "Batch: 1234    Epoch[3/3]    Training Loss: 0.5688918828964233\n",
            "Batch: 1235    Epoch[3/3]    Training Loss: 0.3691743016242981\n",
            "Batch: 1236    Epoch[3/3]    Training Loss: 0.6965729594230652\n",
            "Batch: 1237    Epoch[3/3]    Training Loss: 0.4957866668701172\n",
            "Batch: 1238    Epoch[3/3]    Training Loss: 0.35508766770362854\n",
            "Batch: 1239    Epoch[3/3]    Training Loss: 0.8512017130851746\n",
            "Batch: 1240    Epoch[3/3]    Training Loss: 0.45349475741386414\n",
            "Batch: 1241    Epoch[3/3]    Training Loss: 0.2224489152431488\n",
            "Batch: 1242    Epoch[3/3]    Training Loss: 0.21557748317718506\n",
            "Batch: 1243    Epoch[3/3]    Training Loss: 0.7535676956176758\n",
            "Batch: 1244    Epoch[3/3]    Training Loss: 0.2936655879020691\n",
            "Batch: 1245    Epoch[3/3]    Training Loss: 0.6511504650115967\n",
            "Batch: 1246    Epoch[3/3]    Training Loss: 0.8957763314247131\n",
            "Batch: 1247    Epoch[3/3]    Training Loss: 0.6426186561584473\n",
            "Batch: 1248    Epoch[3/3]    Training Loss: 0.6099752187728882\n",
            "Batch: 1249    Epoch[3/3]    Training Loss: 0.36973345279693604\n",
            "Batch: 1250    Epoch[3/3]    Training Loss: 0.36482077836990356\n",
            "Batch: 1251    Epoch[3/3]    Training Loss: 1.377583384513855\n",
            "Batch: 1252    Epoch[3/3]    Training Loss: 0.3934866189956665\n",
            "Batch: 1253    Epoch[3/3]    Training Loss: 0.4900244176387787\n",
            "Batch: 1254    Epoch[3/3]    Training Loss: 1.0838834047317505\n",
            "Batch: 1255    Epoch[3/3]    Training Loss: 0.47072935104370117\n",
            "Batch: 1256    Epoch[3/3]    Training Loss: 0.7085868120193481\n",
            "Batch: 1257    Epoch[3/3]    Training Loss: 0.6205578446388245\n",
            "Batch: 1258    Epoch[3/3]    Training Loss: 0.9506310224533081\n",
            "Batch: 1259    Epoch[3/3]    Training Loss: 0.5003992319107056\n",
            "Batch: 1260    Epoch[3/3]    Training Loss: 0.46871986985206604\n",
            "Batch: 1261    Epoch[3/3]    Training Loss: 0.7552382946014404\n",
            "Batch: 1262    Epoch[3/3]    Training Loss: 0.49209171533584595\n",
            "Batch: 1263    Epoch[3/3]    Training Loss: 0.9036756753921509\n",
            "Batch: 1264    Epoch[3/3]    Training Loss: 0.6348691582679749\n",
            "Batch: 1265    Epoch[3/3]    Training Loss: 0.9749004244804382\n",
            "Batch: 1266    Epoch[3/3]    Training Loss: 0.42330995202064514\n",
            "Batch: 1267    Epoch[3/3]    Training Loss: 0.7415851354598999\n",
            "Batch: 1268    Epoch[3/3]    Training Loss: 0.6856114268302917\n",
            "Batch: 1269    Epoch[3/3]    Training Loss: 0.6327123641967773\n",
            "Batch: 1270    Epoch[3/3]    Training Loss: 0.4938521981239319\n",
            "Batch: 1271    Epoch[3/3]    Training Loss: 0.5918654799461365\n",
            "Batch: 1272    Epoch[3/3]    Training Loss: 0.3671702742576599\n",
            "Batch: 1273    Epoch[3/3]    Training Loss: 0.28120648860931396\n",
            "Batch: 1274    Epoch[3/3]    Training Loss: 0.25784263014793396\n",
            "Batch: 1275    Epoch[3/3]    Training Loss: 0.6102625131607056\n",
            "Batch: 1276    Epoch[3/3]    Training Loss: 0.6592393517494202\n",
            "Batch: 1277    Epoch[3/3]    Training Loss: 0.2555587589740753\n",
            "Batch: 1278    Epoch[3/3]    Training Loss: 0.5516370534896851\n",
            "Batch: 1279    Epoch[3/3]    Training Loss: 0.6573355793952942\n",
            "Batch: 1280    Epoch[3/3]    Training Loss: 0.28341948986053467\n",
            "Batch: 1281    Epoch[3/3]    Training Loss: 0.764090895652771\n",
            "Batch: 1282    Epoch[3/3]    Training Loss: 0.43358314037323\n",
            "Batch: 1283    Epoch[3/3]    Training Loss: 0.9527498483657837\n",
            "Batch: 1284    Epoch[3/3]    Training Loss: 0.5069794058799744\n",
            "Batch: 1285    Epoch[3/3]    Training Loss: 0.4028700292110443\n",
            "Batch: 1286    Epoch[3/3]    Training Loss: 0.642781138420105\n",
            "Batch: 1287    Epoch[3/3]    Training Loss: 0.5232778191566467\n",
            "Batch: 1288    Epoch[3/3]    Training Loss: 0.813694179058075\n",
            "Batch: 1289    Epoch[3/3]    Training Loss: 0.734076738357544\n",
            "Batch: 1290    Epoch[3/3]    Training Loss: 0.5887502431869507\n",
            "Batch: 1291    Epoch[3/3]    Training Loss: 0.8206015825271606\n",
            "Batch: 1292    Epoch[3/3]    Training Loss: 0.5839373469352722\n",
            "Batch: 1293    Epoch[3/3]    Training Loss: 0.5017272233963013\n",
            "Batch: 1294    Epoch[3/3]    Training Loss: 0.6773386597633362\n",
            "Batch: 1295    Epoch[3/3]    Training Loss: 0.2867435812950134\n",
            "Batch: 1296    Epoch[3/3]    Training Loss: 0.389488160610199\n",
            "Batch: 1297    Epoch[3/3]    Training Loss: 0.3380439281463623\n",
            "Batch: 1298    Epoch[3/3]    Training Loss: 0.4576062560081482\n",
            "Batch: 1299    Epoch[3/3]    Training Loss: 0.5236835479736328\n",
            "Batch: 1300    Epoch[3/3]    Training Loss: 0.5893948078155518\n",
            "Batch: 1301    Epoch[3/3]    Training Loss: 0.5918394923210144\n",
            "Batch: 1302    Epoch[3/3]    Training Loss: 0.5711544752120972\n",
            "Batch: 1303    Epoch[3/3]    Training Loss: 0.20600822567939758\n",
            "Batch: 1304    Epoch[3/3]    Training Loss: 1.4659357070922852\n",
            "Batch: 1305    Epoch[3/3]    Training Loss: 0.5092600584030151\n",
            "Batch: 1306    Epoch[3/3]    Training Loss: 0.6214147806167603\n",
            "Batch: 1307    Epoch[3/3]    Training Loss: 0.37703901529312134\n",
            "Batch: 1308    Epoch[3/3]    Training Loss: 0.3675442934036255\n",
            "Batch: 1309    Epoch[3/3]    Training Loss: 0.5401960611343384\n",
            "Batch: 1310    Epoch[3/3]    Training Loss: 0.61969393491745\n",
            "Batch: 1311    Epoch[3/3]    Training Loss: 0.46816563606262207\n",
            "Batch: 1312    Epoch[3/3]    Training Loss: 0.8045806288719177\n",
            "Batch: 1313    Epoch[3/3]    Training Loss: 1.0374791622161865\n",
            "Batch: 1314    Epoch[3/3]    Training Loss: 0.9534239172935486\n",
            "Batch: 1315    Epoch[3/3]    Training Loss: 0.9773843288421631\n",
            "Batch: 1316    Epoch[3/3]    Training Loss: 0.5359953045845032\n",
            "Batch: 1317    Epoch[3/3]    Training Loss: 0.8252754211425781\n",
            "Batch: 1318    Epoch[3/3]    Training Loss: 0.5224679708480835\n",
            "Batch: 1319    Epoch[3/3]    Training Loss: 0.4112083613872528\n",
            "Batch: 1320    Epoch[3/3]    Training Loss: 0.5134762525558472\n",
            "Batch: 1321    Epoch[3/3]    Training Loss: 0.6743209362030029\n",
            "Batch: 1322    Epoch[3/3]    Training Loss: 0.5317149758338928\n",
            "Batch: 1323    Epoch[3/3]    Training Loss: 0.4368259012699127\n",
            "Batch: 1324    Epoch[3/3]    Training Loss: 0.8146406412124634\n",
            "Batch: 1325    Epoch[3/3]    Training Loss: 0.6902307868003845\n",
            "Batch: 1326    Epoch[3/3]    Training Loss: 0.4887547194957733\n",
            "Batch: 1327    Epoch[3/3]    Training Loss: 0.5444607734680176\n",
            "Batch: 1328    Epoch[3/3]    Training Loss: 0.6851832866668701\n",
            "Batch: 1329    Epoch[3/3]    Training Loss: 0.2532327175140381\n",
            "Batch: 1330    Epoch[3/3]    Training Loss: 0.5051360726356506\n",
            "Batch: 1331    Epoch[3/3]    Training Loss: 0.429738849401474\n",
            "Batch: 1332    Epoch[3/3]    Training Loss: 0.4067693054676056\n",
            "Batch: 1333    Epoch[3/3]    Training Loss: 0.39602506160736084\n",
            "Batch: 1334    Epoch[3/3]    Training Loss: 0.5016492605209351\n",
            "Batch: 1335    Epoch[3/3]    Training Loss: 0.5590335130691528\n",
            "Batch: 1336    Epoch[3/3]    Training Loss: 0.18601199984550476\n",
            "Batch: 1337    Epoch[3/3]    Training Loss: 0.4393378496170044\n",
            "Batch: 1338    Epoch[3/3]    Training Loss: 0.5343024730682373\n",
            "Batch: 1339    Epoch[3/3]    Training Loss: 0.711907684803009\n",
            "Batch: 1340    Epoch[3/3]    Training Loss: 0.9232203960418701\n",
            "Batch: 1341    Epoch[3/3]    Training Loss: 0.5556446313858032\n",
            "Batch: 1342    Epoch[3/3]    Training Loss: 0.38282689452171326\n",
            "Batch: 1343    Epoch[3/3]    Training Loss: 0.44272276759147644\n",
            "Batch: 1344    Epoch[3/3]    Training Loss: 0.5293271541595459\n",
            "Batch: 1345    Epoch[3/3]    Training Loss: 0.3161545395851135\n",
            "Batch: 1346    Epoch[3/3]    Training Loss: 0.7640754580497742\n",
            "Batch: 1347    Epoch[3/3]    Training Loss: 0.6860841512680054\n",
            "Batch: 1348    Epoch[3/3]    Training Loss: 0.6644224524497986\n",
            "Batch: 1349    Epoch[3/3]    Training Loss: 0.3545386493206024\n",
            "Batch: 1350    Epoch[3/3]    Training Loss: 0.9524897336959839\n",
            "Batch: 1351    Epoch[3/3]    Training Loss: 1.122249722480774\n",
            "Batch: 1352    Epoch[3/3]    Training Loss: 0.34724506735801697\n",
            "Batch: 1353    Epoch[3/3]    Training Loss: 0.6659539937973022\n",
            "Batch: 1354    Epoch[3/3]    Training Loss: 0.6395499110221863\n",
            "Batch: 1355    Epoch[3/3]    Training Loss: 0.4255082309246063\n",
            "Batch: 1356    Epoch[3/3]    Training Loss: 0.20751023292541504\n",
            "Batch: 1357    Epoch[3/3]    Training Loss: 0.4254307746887207\n",
            "Batch: 1358    Epoch[3/3]    Training Loss: 0.7124383449554443\n",
            "Batch: 1359    Epoch[3/3]    Training Loss: 0.6320890784263611\n",
            "Batch: 1360    Epoch[3/3]    Training Loss: 0.3301902711391449\n",
            "Batch: 1361    Epoch[3/3]    Training Loss: 0.7816582322120667\n",
            "Batch: 1362    Epoch[3/3]    Training Loss: 0.7177714109420776\n",
            "Batch: 1363    Epoch[3/3]    Training Loss: 0.6805564165115356\n",
            "Batch: 1364    Epoch[3/3]    Training Loss: 0.5314918756484985\n",
            "Batch: 1365    Epoch[3/3]    Training Loss: 0.5184050798416138\n",
            "Batch: 1366    Epoch[3/3]    Training Loss: 0.706876277923584\n",
            "Batch: 1367    Epoch[3/3]    Training Loss: 0.45213061571121216\n",
            "Batch: 1368    Epoch[3/3]    Training Loss: 0.44980669021606445\n",
            "Batch: 1369    Epoch[3/3]    Training Loss: 0.4965200424194336\n",
            "Batch: 1370    Epoch[3/3]    Training Loss: 0.408910870552063\n",
            "Batch: 1371    Epoch[3/3]    Training Loss: 0.40440160036087036\n",
            "Batch: 1372    Epoch[3/3]    Training Loss: 0.29826146364212036\n",
            "Batch: 1373    Epoch[3/3]    Training Loss: 0.39878198504447937\n",
            "Batch: 1374    Epoch[3/3]    Training Loss: 0.4164298474788666\n",
            "Batch: 1375    Epoch[3/3]    Training Loss: 0.4252103269100189\n",
            "Batch: 1376    Epoch[3/3]    Training Loss: 0.5135877728462219\n",
            "Batch: 1377    Epoch[3/3]    Training Loss: 1.0213853120803833\n",
            "Batch: 1378    Epoch[3/3]    Training Loss: 0.8854676485061646\n",
            "Batch: 1379    Epoch[3/3]    Training Loss: 0.3552955389022827\n",
            "Batch: 1380    Epoch[3/3]    Training Loss: 0.6933562159538269\n",
            "Batch: 1381    Epoch[3/3]    Training Loss: 0.4504637122154236\n",
            "Batch: 1382    Epoch[3/3]    Training Loss: 0.6033779382705688\n",
            "Batch: 1383    Epoch[3/3]    Training Loss: 0.25347211956977844\n",
            "Batch: 1384    Epoch[3/3]    Training Loss: 0.5156165361404419\n",
            "Batch: 1385    Epoch[3/3]    Training Loss: 0.5517349243164062\n",
            "Batch: 1386    Epoch[3/3]    Training Loss: 0.3963668644428253\n",
            "Batch: 1387    Epoch[3/3]    Training Loss: 0.5138238072395325\n",
            "Batch: 1388    Epoch[3/3]    Training Loss: 0.42948323488235474\n",
            "Batch: 1389    Epoch[3/3]    Training Loss: 1.04695463180542\n",
            "Batch: 1390    Epoch[3/3]    Training Loss: 0.4249613285064697\n",
            "Batch: 1391    Epoch[3/3]    Training Loss: 0.6578078269958496\n",
            "Batch: 1392    Epoch[3/3]    Training Loss: 0.30850785970687866\n",
            "Batch: 1393    Epoch[3/3]    Training Loss: 0.2683044373989105\n",
            "Batch: 1394    Epoch[3/3]    Training Loss: 0.5443516373634338\n",
            "Batch: 1395    Epoch[3/3]    Training Loss: 0.6012076139450073\n",
            "Batch: 1396    Epoch[3/3]    Training Loss: 0.3712235689163208\n",
            "Batch: 1397    Epoch[3/3]    Training Loss: 1.1428301334381104\n",
            "Batch: 1398    Epoch[3/3]    Training Loss: 0.5005695819854736\n",
            "Batch: 1399    Epoch[3/3]    Training Loss: 0.4966091513633728\n",
            "Batch: 1400    Epoch[3/3]    Training Loss: 0.6693415641784668\n",
            "Batch: 1401    Epoch[3/3]    Training Loss: 0.8348563313484192\n",
            "Batch: 1402    Epoch[3/3]    Training Loss: 0.89337557554245\n",
            "Batch: 1403    Epoch[3/3]    Training Loss: 0.20307205617427826\n",
            "Batch: 1404    Epoch[3/3]    Training Loss: 0.33570918440818787\n",
            "Batch: 1405    Epoch[3/3]    Training Loss: 0.8749027252197266\n",
            "Batch: 1406    Epoch[3/3]    Training Loss: 1.1546950340270996\n",
            "Batch: 1407    Epoch[3/3]    Training Loss: 0.7540149688720703\n",
            "Batch: 1408    Epoch[3/3]    Training Loss: 0.7202063798904419\n",
            "Batch: 1409    Epoch[3/3]    Training Loss: 0.5110807418823242\n",
            "Batch: 1410    Epoch[3/3]    Training Loss: 0.5770108103752136\n",
            "Batch: 1411    Epoch[3/3]    Training Loss: 0.8049598932266235\n",
            "Batch: 1412    Epoch[3/3]    Training Loss: 0.7010128498077393\n",
            "Batch: 1413    Epoch[3/3]    Training Loss: 0.39808815717697144\n",
            "Batch: 1414    Epoch[3/3]    Training Loss: 0.37465471029281616\n",
            "Batch: 1415    Epoch[3/3]    Training Loss: 0.6887810230255127\n",
            "Batch: 1416    Epoch[3/3]    Training Loss: 0.5055407285690308\n",
            "Batch: 1417    Epoch[3/3]    Training Loss: 0.5988565683364868\n",
            "Batch: 1418    Epoch[3/3]    Training Loss: 0.33993202447891235\n",
            "Batch: 1419    Epoch[3/3]    Training Loss: 0.3476840853691101\n",
            "Batch: 1420    Epoch[3/3]    Training Loss: 0.30694934725761414\n",
            "Batch: 1421    Epoch[3/3]    Training Loss: 0.27010393142700195\n",
            "Batch: 1422    Epoch[3/3]    Training Loss: 0.9931358098983765\n",
            "Batch: 1423    Epoch[3/3]    Training Loss: 0.4279283285140991\n",
            "Batch: 1424    Epoch[3/3]    Training Loss: 0.3496263027191162\n",
            "Batch: 1425    Epoch[3/3]    Training Loss: 1.1839375495910645\n",
            "Batch: 1426    Epoch[3/3]    Training Loss: 0.25129079818725586\n",
            "Batch: 1427    Epoch[3/3]    Training Loss: 0.485187828540802\n",
            "Batch: 1428    Epoch[3/3]    Training Loss: 0.2826205790042877\n",
            "Batch: 1429    Epoch[3/3]    Training Loss: 0.33550533652305603\n",
            "Batch: 1430    Epoch[3/3]    Training Loss: 0.37455683946609497\n",
            "Batch: 1431    Epoch[3/3]    Training Loss: 0.5785308480262756\n",
            "Batch: 1432    Epoch[3/3]    Training Loss: 0.4794057011604309\n",
            "Batch: 1433    Epoch[3/3]    Training Loss: 0.2246440052986145\n",
            "Batch: 1434    Epoch[3/3]    Training Loss: 0.6112151741981506\n",
            "Batch: 1435    Epoch[3/3]    Training Loss: 0.6884956359863281\n",
            "Batch: 1436    Epoch[3/3]    Training Loss: 0.4483042359352112\n",
            "Batch: 1437    Epoch[3/3]    Training Loss: 0.7096606492996216\n",
            "Batch: 1438    Epoch[3/3]    Training Loss: 0.6687631011009216\n",
            "Batch: 1439    Epoch[3/3]    Training Loss: 0.5217738151550293\n",
            "Batch: 1440    Epoch[3/3]    Training Loss: 0.5971246957778931\n",
            "Batch: 1441    Epoch[3/3]    Training Loss: 0.3958202600479126\n",
            "Batch: 1442    Epoch[3/3]    Training Loss: 0.5639691948890686\n",
            "Batch: 1443    Epoch[3/3]    Training Loss: 0.2626423239707947\n",
            "Batch: 1444    Epoch[3/3]    Training Loss: 0.5930905342102051\n",
            "Batch: 1445    Epoch[3/3]    Training Loss: 0.6863288879394531\n",
            "Batch: 1446    Epoch[3/3]    Training Loss: 0.6380249261856079\n",
            "Batch: 1447    Epoch[3/3]    Training Loss: 0.30199018120765686\n",
            "Batch: 1448    Epoch[3/3]    Training Loss: 0.4316765069961548\n",
            "Batch: 1449    Epoch[3/3]    Training Loss: 1.2332602739334106\n",
            "Batch: 1450    Epoch[3/3]    Training Loss: 0.4551118016242981\n",
            "Batch: 1451    Epoch[3/3]    Training Loss: 0.5688097476959229\n",
            "Batch: 1452    Epoch[3/3]    Training Loss: 0.7245650291442871\n",
            "Batch: 1453    Epoch[3/3]    Training Loss: 0.6984022855758667\n",
            "Batch: 1454    Epoch[3/3]    Training Loss: 0.6857987642288208\n",
            "Batch: 1455    Epoch[3/3]    Training Loss: 0.46029132604599\n",
            "Batch: 1456    Epoch[3/3]    Training Loss: 0.7565304040908813\n",
            "Batch: 1457    Epoch[3/3]    Training Loss: 0.2581654489040375\n",
            "Batch: 1458    Epoch[3/3]    Training Loss: 0.4061462879180908\n",
            "Batch: 1459    Epoch[3/3]    Training Loss: 0.5868475437164307\n",
            "Batch: 1460    Epoch[3/3]    Training Loss: 0.5540098547935486\n",
            "Batch: 1461    Epoch[3/3]    Training Loss: 0.8327984809875488\n",
            "Batch: 1462    Epoch[3/3]    Training Loss: 0.2705633044242859\n",
            "Batch: 1463    Epoch[3/3]    Training Loss: 0.9264175891876221\n",
            "Batch: 1464    Epoch[3/3]    Training Loss: 0.5711377859115601\n",
            "Batch: 1465    Epoch[3/3]    Training Loss: 1.230412244796753\n",
            "Batch: 1466    Epoch[3/3]    Training Loss: 0.6245027184486389\n",
            "Batch: 1467    Epoch[3/3]    Training Loss: 0.3805615305900574\n",
            "Batch: 1468    Epoch[3/3]    Training Loss: 0.3269163966178894\n",
            "Batch: 1469    Epoch[3/3]    Training Loss: 0.691662073135376\n",
            "Batch: 1470    Epoch[3/3]    Training Loss: 0.6755526661872864\n",
            "Batch: 1471    Epoch[3/3]    Training Loss: 0.7440769672393799\n",
            "Batch: 1472    Epoch[3/3]    Training Loss: 0.3635297417640686\n",
            "Batch: 1473    Epoch[3/3]    Training Loss: 0.788562536239624\n",
            "Batch: 1474    Epoch[3/3]    Training Loss: 0.3963896334171295\n",
            "Batch: 1475    Epoch[3/3]    Training Loss: 0.5537055730819702\n",
            "Batch: 1476    Epoch[3/3]    Training Loss: 0.649748682975769\n",
            "Batch: 1477    Epoch[3/3]    Training Loss: 0.5734995603561401\n",
            "Batch: 1478    Epoch[3/3]    Training Loss: 1.0191607475280762\n",
            "Batch: 1479    Epoch[3/3]    Training Loss: 0.2333831787109375\n",
            "Batch: 1480    Epoch[3/3]    Training Loss: 0.33378201723098755\n",
            "Batch: 1481    Epoch[3/3]    Training Loss: 0.5988463759422302\n",
            "Batch: 1482    Epoch[3/3]    Training Loss: 0.3716046214103699\n",
            "Batch: 1483    Epoch[3/3]    Training Loss: 0.5035402774810791\n",
            "Batch: 1484    Epoch[3/3]    Training Loss: 0.7216058373451233\n",
            "Batch: 1485    Epoch[3/3]    Training Loss: 0.5900574326515198\n",
            "Batch: 1486    Epoch[3/3]    Training Loss: 0.480288028717041\n",
            "Batch: 1487    Epoch[3/3]    Training Loss: 0.3878591060638428\n",
            "Batch: 1488    Epoch[3/3]    Training Loss: 0.4839325547218323\n",
            "Batch: 1489    Epoch[3/3]    Training Loss: 1.348937749862671\n",
            "Batch: 1490    Epoch[3/3]    Training Loss: 0.8276068568229675\n",
            "Batch: 1491    Epoch[3/3]    Training Loss: 0.7934350967407227\n",
            "Batch: 1492    Epoch[3/3]    Training Loss: 1.0440587997436523\n",
            "Batch: 1493    Epoch[3/3]    Training Loss: 0.7099925875663757\n",
            "Batch: 1494    Epoch[3/3]    Training Loss: 0.6696048974990845\n",
            "Batch: 1495    Epoch[3/3]    Training Loss: 0.4257047176361084\n",
            "Batch: 1496    Epoch[3/3]    Training Loss: 0.5655864477157593\n",
            "Batch: 1497    Epoch[3/3]    Training Loss: 0.47723907232284546\n",
            "Batch: 1498    Epoch[3/3]    Training Loss: 0.5172269344329834\n",
            "Batch: 1499    Epoch[3/3]    Training Loss: 0.3219011723995209\n",
            "Batch: 1500    Epoch[3/3]    Training Loss: 0.9987908601760864\n",
            "Batch: 1501    Epoch[3/3]    Training Loss: 0.5732951164245605\n",
            "Batch: 1502    Epoch[3/3]    Training Loss: 0.5212849974632263\n",
            "Batch: 1503    Epoch[3/3]    Training Loss: 0.594414234161377\n",
            "Batch: 1504    Epoch[3/3]    Training Loss: 0.5808165669441223\n",
            "Batch: 1505    Epoch[3/3]    Training Loss: 0.5684255361557007\n",
            "Batch: 1506    Epoch[3/3]    Training Loss: 0.479341983795166\n",
            "Batch: 1507    Epoch[3/3]    Training Loss: 0.6593390703201294\n",
            "Batch: 1508    Epoch[3/3]    Training Loss: 0.98968905210495\n",
            "Batch: 1509    Epoch[3/3]    Training Loss: 0.26939657330513\n",
            "Batch: 1510    Epoch[3/3]    Training Loss: 0.733299970626831\n",
            "Batch: 1511    Epoch[3/3]    Training Loss: 1.0404243469238281\n",
            "Batch: 1512    Epoch[3/3]    Training Loss: 0.6389627456665039\n",
            "Batch: 1513    Epoch[3/3]    Training Loss: 0.9354581236839294\n",
            "Batch: 1514    Epoch[3/3]    Training Loss: 0.2581992745399475\n",
            "Batch: 1515    Epoch[3/3]    Training Loss: 0.6897796988487244\n",
            "Batch: 1516    Epoch[3/3]    Training Loss: 0.7809522151947021\n",
            "Batch: 1517    Epoch[3/3]    Training Loss: 0.6709741353988647\n",
            "Batch: 1518    Epoch[3/3]    Training Loss: 0.4594898819923401\n",
            "Batch: 1519    Epoch[3/3]    Training Loss: 0.543692409992218\n",
            "Batch: 1520    Epoch[3/3]    Training Loss: 0.7473775744438171\n",
            "Batch: 1521    Epoch[3/3]    Training Loss: 0.42496800422668457\n",
            "Batch: 1522    Epoch[3/3]    Training Loss: 0.4519551694393158\n",
            "Batch: 1523    Epoch[3/3]    Training Loss: 0.4559085965156555\n",
            "Batch: 1524    Epoch[3/3]    Training Loss: 0.42953091859817505\n",
            "Batch: 1525    Epoch[3/3]    Training Loss: 0.9149554371833801\n",
            "Batch: 1526    Epoch[3/3]    Training Loss: 0.469720721244812\n",
            "Batch: 1527    Epoch[3/3]    Training Loss: 0.4999260902404785\n",
            "Batch: 1528    Epoch[3/3]    Training Loss: 0.7120057940483093\n",
            "Batch: 1529    Epoch[3/3]    Training Loss: 1.1233022212982178\n",
            "Batch: 1530    Epoch[3/3]    Training Loss: 0.36760586500167847\n",
            "Batch: 1531    Epoch[3/3]    Training Loss: 0.6087058782577515\n",
            "Batch: 1532    Epoch[3/3]    Training Loss: 0.9572668671607971\n",
            "Batch: 1533    Epoch[3/3]    Training Loss: 0.40948486328125\n",
            "Batch: 1534    Epoch[3/3]    Training Loss: 0.37391045689582825\n",
            "Batch: 1535    Epoch[3/3]    Training Loss: 0.5274398326873779\n",
            "Batch: 1536    Epoch[3/3]    Training Loss: 0.5234986543655396\n",
            "Batch: 1537    Epoch[3/3]    Training Loss: 0.9173556566238403\n",
            "Batch: 1538    Epoch[3/3]    Training Loss: 0.39473921060562134\n",
            "Batch: 1539    Epoch[3/3]    Training Loss: 0.49417680501937866\n",
            "Batch: 1540    Epoch[3/3]    Training Loss: 0.7472858428955078\n",
            "Batch: 1541    Epoch[3/3]    Training Loss: 0.9913939833641052\n",
            "Batch: 1542    Epoch[3/3]    Training Loss: 0.29412662982940674\n",
            "Batch: 1543    Epoch[3/3]    Training Loss: 0.8653080463409424\n",
            "Batch: 1544    Epoch[3/3]    Training Loss: 1.2111482620239258\n",
            "Batch: 1545    Epoch[3/3]    Training Loss: 0.8742315769195557\n",
            "Batch: 1546    Epoch[3/3]    Training Loss: 0.5982246994972229\n",
            "Batch: 1547    Epoch[3/3]    Training Loss: 0.21377116441726685\n",
            "Batch: 1548    Epoch[3/3]    Training Loss: 0.24938350915908813\n",
            "Batch: 1549    Epoch[3/3]    Training Loss: 0.3929538428783417\n",
            "Batch: 1550    Epoch[3/3]    Training Loss: 1.0049865245819092\n",
            "Batch: 1551    Epoch[3/3]    Training Loss: 0.43230342864990234\n",
            "Batch: 1552    Epoch[3/3]    Training Loss: 0.5580148696899414\n",
            "Batch: 1553    Epoch[3/3]    Training Loss: 0.5095027089118958\n",
            "Batch: 1554    Epoch[3/3]    Training Loss: 0.8508305549621582\n",
            "Batch: 1555    Epoch[3/3]    Training Loss: 0.6987420320510864\n",
            "Batch: 1556    Epoch[3/3]    Training Loss: 0.49225008487701416\n",
            "Batch: 1557    Epoch[3/3]    Training Loss: 0.5983781814575195\n",
            "Batch: 1558    Epoch[3/3]    Training Loss: 0.32148611545562744\n",
            "Batch: 1559    Epoch[3/3]    Training Loss: 0.4940822124481201\n",
            "Batch: 1560    Epoch[3/3]    Training Loss: 0.5727192163467407\n",
            "Batch: 1561    Epoch[3/3]    Training Loss: 0.5270406603813171\n",
            "Batch: 1562    Epoch[3/3]    Training Loss: 0.6563601493835449\n",
            "Batch: 1563    Epoch[3/3]    Training Loss: 0.5177061557769775\n",
            "Batch: 1564    Epoch[3/3]    Training Loss: 0.38131099939346313\n",
            "Batch: 1565    Epoch[3/3]    Training Loss: 0.7396209239959717\n",
            "Batch: 1566    Epoch[3/3]    Training Loss: 0.4025672674179077\n",
            "Batch: 1567    Epoch[3/3]    Training Loss: 0.29039469361305237\n",
            "Batch: 1568    Epoch[3/3]    Training Loss: 0.27725568413734436\n",
            "Batch: 1569    Epoch[3/3]    Training Loss: 0.6944071054458618\n",
            "Batch: 1570    Epoch[3/3]    Training Loss: 0.463920533657074\n",
            "Batch: 1571    Epoch[3/3]    Training Loss: 0.5160756707191467\n",
            "Batch: 1572    Epoch[3/3]    Training Loss: 0.4498592019081116\n",
            "Batch: 1573    Epoch[3/3]    Training Loss: 0.5215924382209778\n",
            "Batch: 1574    Epoch[3/3]    Training Loss: 0.42199602723121643\n",
            "Batch: 1575    Epoch[3/3]    Training Loss: 0.3342864215373993\n",
            "Batch: 1576    Epoch[3/3]    Training Loss: 0.24461308121681213\n",
            "Batch: 1577    Epoch[3/3]    Training Loss: 0.5030326247215271\n",
            "Batch: 1578    Epoch[3/3]    Training Loss: 0.47172629833221436\n",
            "Batch: 1579    Epoch[3/3]    Training Loss: 0.6449126601219177\n",
            "Batch: 1580    Epoch[3/3]    Training Loss: 0.19278182089328766\n",
            "Batch: 1581    Epoch[3/3]    Training Loss: 0.6582033634185791\n",
            "Batch: 1582    Epoch[3/3]    Training Loss: 1.4161103963851929\n",
            "Batch: 1583    Epoch[3/3]    Training Loss: 0.7085528373718262\n",
            "Batch: 1584    Epoch[3/3]    Training Loss: 1.064868688583374\n",
            "Batch: 1585    Epoch[3/3]    Training Loss: 0.5658406615257263\n",
            "Batch: 1586    Epoch[3/3]    Training Loss: 0.9550479650497437\n",
            "Batch: 1587    Epoch[3/3]    Training Loss: 0.5663597583770752\n",
            "Batch: 1588    Epoch[3/3]    Training Loss: 0.43592220544815063\n",
            "Batch: 1589    Epoch[3/3]    Training Loss: 0.444482684135437\n",
            "Batch: 1590    Epoch[3/3]    Training Loss: 0.7097258567810059\n",
            "Batch: 1591    Epoch[3/3]    Training Loss: 0.4339701235294342\n",
            "Batch: 1592    Epoch[3/3]    Training Loss: 0.8339414596557617\n",
            "Batch: 1593    Epoch[3/3]    Training Loss: 0.33148249983787537\n",
            "Batch: 1594    Epoch[3/3]    Training Loss: 0.38970938324928284\n",
            "Batch: 1595    Epoch[3/3]    Training Loss: 0.685031533241272\n",
            "Batch: 1596    Epoch[3/3]    Training Loss: 0.5900917649269104\n",
            "Batch: 1597    Epoch[3/3]    Training Loss: 0.4504554271697998\n",
            "Batch: 1598    Epoch[3/3]    Training Loss: 0.5168771743774414\n",
            "Batch: 1599    Epoch[3/3]    Training Loss: 0.18557915091514587\n",
            "Batch: 1600    Epoch[3/3]    Training Loss: 0.5094685554504395\n",
            "Batch: 1601    Epoch[3/3]    Training Loss: 0.1745356023311615\n",
            "Batch: 1602    Epoch[3/3]    Training Loss: 0.4587864875793457\n",
            "Batch: 1603    Epoch[3/3]    Training Loss: 0.7527011632919312\n",
            "Batch: 1604    Epoch[3/3]    Training Loss: 0.18333792686462402\n",
            "Batch: 1605    Epoch[3/3]    Training Loss: 0.3684196472167969\n",
            "Batch: 1606    Epoch[3/3]    Training Loss: 0.4756503105163574\n",
            "Batch: 1607    Epoch[3/3]    Training Loss: 0.13105075061321259\n",
            "Batch: 1608    Epoch[3/3]    Training Loss: 0.5057135820388794\n",
            "Batch: 1609    Epoch[3/3]    Training Loss: 0.3864351511001587\n",
            "Batch: 1610    Epoch[3/3]    Training Loss: 1.3922511339187622\n",
            "Batch: 1611    Epoch[3/3]    Training Loss: 0.7474223375320435\n",
            "Batch: 1612    Epoch[3/3]    Training Loss: 0.5209066271781921\n",
            "Batch: 1613    Epoch[3/3]    Training Loss: 0.6275216341018677\n",
            "Batch: 1614    Epoch[3/3]    Training Loss: 0.8706151247024536\n",
            "Batch: 1615    Epoch[3/3]    Training Loss: 0.43365663290023804\n",
            "Batch: 1616    Epoch[3/3]    Training Loss: 0.5111823678016663\n",
            "Batch: 1617    Epoch[3/3]    Training Loss: 0.4529200792312622\n",
            "Batch: 1618    Epoch[3/3]    Training Loss: 0.46663016080856323\n",
            "Batch: 1619    Epoch[3/3]    Training Loss: 0.622333824634552\n",
            "Batch: 1620    Epoch[3/3]    Training Loss: 0.35492563247680664\n",
            "Batch: 1621    Epoch[3/3]    Training Loss: 0.7951838374137878\n",
            "Batch: 1622    Epoch[3/3]    Training Loss: 0.4817395806312561\n",
            "Batch: 1623    Epoch[3/3]    Training Loss: 0.5577126741409302\n",
            "Batch: 1624    Epoch[3/3]    Training Loss: 0.7647302150726318\n",
            "Batch: 1625    Epoch[3/3]    Training Loss: 0.6866194605827332\n",
            "Batch: 1626    Epoch[3/3]    Training Loss: 0.5319148302078247\n",
            "Batch: 1627    Epoch[3/3]    Training Loss: 0.8528218269348145\n",
            "Batch: 1628    Epoch[3/3]    Training Loss: 0.7643551230430603\n",
            "Batch: 1629    Epoch[3/3]    Training Loss: 0.9474474191665649\n",
            "Batch: 1630    Epoch[3/3]    Training Loss: 0.8297730684280396\n",
            "Batch: 1631    Epoch[3/3]    Training Loss: 0.7005560398101807\n",
            "Batch: 1632    Epoch[3/3]    Training Loss: 0.39360374212265015\n",
            "Batch: 1633    Epoch[3/3]    Training Loss: 0.7560839653015137\n",
            "Batch: 1634    Epoch[3/3]    Training Loss: 0.4059763550758362\n",
            "Batch: 1635    Epoch[3/3]    Training Loss: 0.5804349184036255\n",
            "Batch: 1636    Epoch[3/3]    Training Loss: 0.25340279936790466\n",
            "Batch: 1637    Epoch[3/3]    Training Loss: 0.4771813750267029\n",
            "Batch: 1638    Epoch[3/3]    Training Loss: 0.6153278350830078\n",
            "Batch: 1639    Epoch[3/3]    Training Loss: 0.777306318283081\n",
            "Batch: 1640    Epoch[3/3]    Training Loss: 0.7586312890052795\n",
            "Batch: 1641    Epoch[3/3]    Training Loss: 1.0698862075805664\n",
            "Batch: 1642    Epoch[3/3]    Training Loss: 0.7079688310623169\n",
            "Batch: 1643    Epoch[3/3]    Training Loss: 0.4217708706855774\n",
            "Batch: 1644    Epoch[3/3]    Training Loss: 0.9084632396697998\n",
            "Batch: 1645    Epoch[3/3]    Training Loss: 0.41198134422302246\n",
            "Batch: 1646    Epoch[3/3]    Training Loss: 0.6365723609924316\n",
            "Batch: 1647    Epoch[3/3]    Training Loss: 0.9488695859909058\n",
            "Batch: 1648    Epoch[3/3]    Training Loss: 0.45812100172042847\n",
            "Batch: 1649    Epoch[3/3]    Training Loss: 0.2792308032512665\n",
            "Batch: 1650    Epoch[3/3]    Training Loss: 0.3505769968032837\n",
            "Batch: 1651    Epoch[3/3]    Training Loss: 0.9542231559753418\n",
            "Batch: 1652    Epoch[3/3]    Training Loss: 0.43843209743499756\n",
            "Batch: 1653    Epoch[3/3]    Training Loss: 0.6180336475372314\n",
            "Batch: 1654    Epoch[3/3]    Training Loss: 0.5388269424438477\n",
            "Batch: 1655    Epoch[3/3]    Training Loss: 0.4162229895591736\n",
            "Batch: 1656    Epoch[3/3]    Training Loss: 0.751739501953125\n",
            "Batch: 1657    Epoch[3/3]    Training Loss: 0.5540611147880554\n",
            "Batch: 1658    Epoch[3/3]    Training Loss: 0.7827783226966858\n",
            "Batch: 1659    Epoch[3/3]    Training Loss: 0.44568386673927307\n",
            "Batch: 1660    Epoch[3/3]    Training Loss: 0.35984188318252563\n",
            "Batch: 1661    Epoch[3/3]    Training Loss: 0.5476413369178772\n",
            "Batch: 1662    Epoch[3/3]    Training Loss: 0.7150841951370239\n",
            "Batch: 1663    Epoch[3/3]    Training Loss: 0.7860132455825806\n",
            "Batch: 1664    Epoch[3/3]    Training Loss: 0.6640715599060059\n",
            "Batch: 1665    Epoch[3/3]    Training Loss: 0.2839958071708679\n",
            "Batch: 1666    Epoch[3/3]    Training Loss: 0.8932421207427979\n",
            "Batch: 1667    Epoch[3/3]    Training Loss: 0.2996630072593689\n",
            "Batch: 1668    Epoch[3/3]    Training Loss: 0.40613487362861633\n",
            "Batch: 1669    Epoch[3/3]    Training Loss: 0.4597456157207489\n",
            "Batch: 1670    Epoch[3/3]    Training Loss: 0.762714147567749\n",
            "Batch: 1671    Epoch[3/3]    Training Loss: 0.5432407855987549\n",
            "Batch: 1672    Epoch[3/3]    Training Loss: 0.47258633375167847\n",
            "Batch: 1673    Epoch[3/3]    Training Loss: 0.6036887168884277\n",
            "Batch: 1674    Epoch[3/3]    Training Loss: 0.859849750995636\n",
            "Batch: 1675    Epoch[3/3]    Training Loss: 0.6066921353340149\n",
            "Batch: 1676    Epoch[3/3]    Training Loss: 0.3237369954586029\n",
            "Batch: 1677    Epoch[3/3]    Training Loss: 0.3881964683532715\n",
            "Batch: 1678    Epoch[3/3]    Training Loss: 1.2714760303497314\n",
            "Batch: 1679    Epoch[3/3]    Training Loss: 0.48042911291122437\n",
            "Batch: 1680    Epoch[3/3]    Training Loss: 1.0675404071807861\n",
            "Batch: 1681    Epoch[3/3]    Training Loss: 0.6523927450180054\n",
            "Batch: 1682    Epoch[3/3]    Training Loss: 0.834740400314331\n",
            "Batch: 1683    Epoch[3/3]    Training Loss: 0.6772915124893188\n",
            "Batch: 1684    Epoch[3/3]    Training Loss: 0.9040517807006836\n",
            "Batch: 1685    Epoch[3/3]    Training Loss: 0.4373037815093994\n",
            "Batch: 1686    Epoch[3/3]    Training Loss: 0.8913281559944153\n",
            "Batch: 1687    Epoch[3/3]    Training Loss: 0.638690173625946\n",
            "Batch: 1688    Epoch[3/3]    Training Loss: 0.7080158591270447\n",
            "Batch: 1689    Epoch[3/3]    Training Loss: 1.0512250661849976\n",
            "Batch: 1690    Epoch[3/3]    Training Loss: 0.5633209943771362\n",
            "Batch: 1691    Epoch[3/3]    Training Loss: 0.7327952980995178\n",
            "Batch: 1692    Epoch[3/3]    Training Loss: 0.43647873401641846\n",
            "Batch: 1693    Epoch[3/3]    Training Loss: 0.3568829894065857\n",
            "Batch: 1694    Epoch[3/3]    Training Loss: 0.2513929605484009\n",
            "Batch: 1695    Epoch[3/3]    Training Loss: 0.15180785953998566\n",
            "Batch: 1696    Epoch[3/3]    Training Loss: 0.789124608039856\n",
            "Batch: 1697    Epoch[3/3]    Training Loss: 1.2395094633102417\n",
            "Batch: 1698    Epoch[3/3]    Training Loss: 0.6275988817214966\n",
            "Batch: 1699    Epoch[3/3]    Training Loss: 0.30608922243118286\n",
            "Batch: 1700    Epoch[3/3]    Training Loss: 0.5123962163925171\n",
            "Batch: 1701    Epoch[3/3]    Training Loss: 0.49650734663009644\n",
            "Batch: 1702    Epoch[3/3]    Training Loss: 0.24503785371780396\n",
            "Batch: 1703    Epoch[3/3]    Training Loss: 0.2788468897342682\n",
            "Batch: 1704    Epoch[3/3]    Training Loss: 0.7348487973213196\n",
            "Batch: 1705    Epoch[3/3]    Training Loss: 0.40548211336135864\n",
            "Batch: 1706    Epoch[3/3]    Training Loss: 0.48635247349739075\n",
            "Batch: 1707    Epoch[3/3]    Training Loss: 0.4004060626029968\n",
            "Batch: 1708    Epoch[3/3]    Training Loss: 0.6006242036819458\n",
            "Batch: 1709    Epoch[3/3]    Training Loss: 0.41185206174850464\n",
            "Batch: 1710    Epoch[3/3]    Training Loss: 0.5312908291816711\n",
            "Batch: 1711    Epoch[3/3]    Training Loss: 0.6881709694862366\n",
            "Batch: 1712    Epoch[3/3]    Training Loss: 0.6765410900115967\n",
            "Batch: 1713    Epoch[3/3]    Training Loss: 0.4852830767631531\n",
            "Batch: 1714    Epoch[3/3]    Training Loss: 0.2675108015537262\n",
            "Batch: 1715    Epoch[3/3]    Training Loss: 0.6892455220222473\n",
            "Batch: 1716    Epoch[3/3]    Training Loss: 0.6789729595184326\n",
            "Batch: 1717    Epoch[3/3]    Training Loss: 0.2745863199234009\n",
            "Batch: 1718    Epoch[3/3]    Training Loss: 0.6870189905166626\n",
            "Batch: 1719    Epoch[3/3]    Training Loss: 0.4545249342918396\n",
            "Batch: 1720    Epoch[3/3]    Training Loss: 0.5498618483543396\n",
            "Batch: 1721    Epoch[3/3]    Training Loss: 0.33134010434150696\n",
            "Batch: 1722    Epoch[3/3]    Training Loss: 0.670819103717804\n",
            "Batch: 1723    Epoch[3/3]    Training Loss: 0.1730595976114273\n",
            "Batch: 1724    Epoch[3/3]    Training Loss: 0.45716315507888794\n",
            "Batch: 1725    Epoch[3/3]    Training Loss: 0.33940255641937256\n",
            "Batch: 1726    Epoch[3/3]    Training Loss: 0.6645948886871338\n",
            "Batch: 1727    Epoch[3/3]    Training Loss: 0.6043075323104858\n",
            "Batch: 1728    Epoch[3/3]    Training Loss: 0.5541201829910278\n",
            "Batch: 1729    Epoch[3/3]    Training Loss: 0.39221417903900146\n",
            "Batch: 1730    Epoch[3/3]    Training Loss: 0.618950605392456\n",
            "Batch: 1731    Epoch[3/3]    Training Loss: 1.1571290493011475\n",
            "Batch: 1732    Epoch[3/3]    Training Loss: 0.33182501792907715\n",
            "Batch: 1733    Epoch[3/3]    Training Loss: 0.09243964403867722\n",
            "Batch: 1734    Epoch[3/3]    Training Loss: 0.3958214521408081\n",
            "Batch: 1735    Epoch[3/3]    Training Loss: 0.393960177898407\n",
            "Batch: 1736    Epoch[3/3]    Training Loss: 0.6889888048171997\n",
            "Batch: 1737    Epoch[3/3]    Training Loss: 0.37247005105018616\n",
            "Batch: 1738    Epoch[3/3]    Training Loss: 0.22141790390014648\n",
            "Batch: 1739    Epoch[3/3]    Training Loss: 0.5946022272109985\n",
            "Batch: 1740    Epoch[3/3]    Training Loss: 0.7025948762893677\n",
            "Batch: 1741    Epoch[3/3]    Training Loss: 0.5875521898269653\n",
            "Batch: 1742    Epoch[3/3]    Training Loss: 1.266467571258545\n",
            "Batch: 1743    Epoch[3/3]    Training Loss: 0.5636720657348633\n",
            "Batch: 1744    Epoch[3/3]    Training Loss: 0.5980271100997925\n",
            "Batch: 1745    Epoch[3/3]    Training Loss: 0.6328741908073425\n",
            "Batch: 1746    Epoch[3/3]    Training Loss: 0.5760393142700195\n",
            "Batch: 1747    Epoch[3/3]    Training Loss: 0.28361576795578003\n",
            "Batch: 1748    Epoch[3/3]    Training Loss: 0.4236791133880615\n",
            "Batch: 1749    Epoch[3/3]    Training Loss: 0.5042283535003662\n",
            "Batch: 1750    Epoch[3/3]    Training Loss: 0.664740800857544\n",
            "Batch: 1751    Epoch[3/3]    Training Loss: 0.6379531621932983\n",
            "Batch: 1752    Epoch[3/3]    Training Loss: 1.0079035758972168\n",
            "Batch: 1753    Epoch[3/3]    Training Loss: 0.3641113042831421\n",
            "Batch: 1754    Epoch[3/3]    Training Loss: 0.4783165752887726\n",
            "Batch: 1755    Epoch[3/3]    Training Loss: 0.965074360370636\n",
            "Batch: 1756    Epoch[3/3]    Training Loss: 0.6323572993278503\n",
            "Batch: 1757    Epoch[3/3]    Training Loss: 0.46163827180862427\n",
            "Batch: 1758    Epoch[3/3]    Training Loss: 0.5062946081161499\n",
            "Batch: 1759    Epoch[3/3]    Training Loss: 0.588018536567688\n",
            "Batch: 1760    Epoch[3/3]    Training Loss: 0.7107200622558594\n",
            "Batch: 1761    Epoch[3/3]    Training Loss: 0.9916104674339294\n",
            "Batch: 1762    Epoch[3/3]    Training Loss: 0.45698609948158264\n",
            "Batch: 1763    Epoch[3/3]    Training Loss: 0.3527022898197174\n",
            "Batch: 1764    Epoch[3/3]    Training Loss: 0.5735759139060974\n",
            "Batch: 1765    Epoch[3/3]    Training Loss: 0.608070969581604\n",
            "Batch: 1766    Epoch[3/3]    Training Loss: 0.37270092964172363\n",
            "Batch: 1767    Epoch[3/3]    Training Loss: 0.7793375253677368\n",
            "Batch: 1768    Epoch[3/3]    Training Loss: 0.424136221408844\n",
            "Batch: 1769    Epoch[3/3]    Training Loss: 0.5155127644538879\n",
            "Batch: 1770    Epoch[3/3]    Training Loss: 0.36843591928482056\n",
            "Batch: 1771    Epoch[3/3]    Training Loss: 0.3952336311340332\n",
            "Batch: 1772    Epoch[3/3]    Training Loss: 0.5073841214179993\n",
            "Batch: 1773    Epoch[3/3]    Training Loss: 0.387300044298172\n",
            "Batch: 1774    Epoch[3/3]    Training Loss: 0.641083836555481\n",
            "Batch: 1775    Epoch[3/3]    Training Loss: 0.15788277983665466\n",
            "Batch: 1776    Epoch[3/3]    Training Loss: 0.22213949263095856\n",
            "Batch: 1777    Epoch[3/3]    Training Loss: 0.5310378670692444\n",
            "Batch: 1778    Epoch[3/3]    Training Loss: 0.43414342403411865\n",
            "Batch: 1779    Epoch[3/3]    Training Loss: 0.31209737062454224\n",
            "Batch: 1780    Epoch[3/3]    Training Loss: 0.33892783522605896\n",
            "Batch: 1781    Epoch[3/3]    Training Loss: 0.5288415551185608\n",
            "Batch: 1782    Epoch[3/3]    Training Loss: 0.3459479808807373\n",
            "Batch: 1783    Epoch[3/3]    Training Loss: 0.684804379940033\n",
            "Batch: 1784    Epoch[3/3]    Training Loss: 0.4763818383216858\n",
            "Batch: 1785    Epoch[3/3]    Training Loss: 0.15790396928787231\n",
            "Batch: 1786    Epoch[3/3]    Training Loss: 0.33255428075790405\n",
            "Batch: 1787    Epoch[3/3]    Training Loss: 0.26843637228012085\n",
            "Batch: 1788    Epoch[3/3]    Training Loss: 0.5707592964172363\n",
            "Batch: 1789    Epoch[3/3]    Training Loss: 0.7454948425292969\n",
            "Batch: 1790    Epoch[3/3]    Training Loss: 0.4815652370452881\n",
            "Batch: 1791    Epoch[3/3]    Training Loss: 0.7186163067817688\n",
            "Batch: 1792    Epoch[3/3]    Training Loss: 0.3723895251750946\n",
            "Batch: 1793    Epoch[3/3]    Training Loss: 1.3232319355010986\n",
            "Batch: 1794    Epoch[3/3]    Training Loss: 0.7569347620010376\n",
            "Batch: 1795    Epoch[3/3]    Training Loss: 0.38460755348205566\n",
            "Batch: 1796    Epoch[3/3]    Training Loss: 0.7846571803092957\n",
            "Batch: 1797    Epoch[3/3]    Training Loss: 0.7605969905853271\n",
            "Batch: 1798    Epoch[3/3]    Training Loss: 1.005050778388977\n",
            "Batch: 1799    Epoch[3/3]    Training Loss: 0.46909070014953613\n",
            "Batch: 1800    Epoch[3/3]    Training Loss: 0.43634724617004395\n",
            "Batch: 1801    Epoch[3/3]    Training Loss: 0.5529875755310059\n",
            "Batch: 1802    Epoch[3/3]    Training Loss: 0.6671806573867798\n",
            "Batch: 1803    Epoch[3/3]    Training Loss: 0.5206600427627563\n",
            "Batch: 1804    Epoch[3/3]    Training Loss: 0.7734476327896118\n",
            "Batch: 1805    Epoch[3/3]    Training Loss: 0.8618422150611877\n",
            "Batch: 1806    Epoch[3/3]    Training Loss: 0.58647620677948\n",
            "Batch: 1807    Epoch[3/3]    Training Loss: 0.37911781668663025\n",
            "Batch: 1808    Epoch[3/3]    Training Loss: 0.46243178844451904\n",
            "Batch: 1809    Epoch[3/3]    Training Loss: 0.6432884335517883\n",
            "Batch: 1810    Epoch[3/3]    Training Loss: 0.3576135039329529\n",
            "Batch: 1811    Epoch[3/3]    Training Loss: 0.5261986255645752\n",
            "Batch: 1812    Epoch[3/3]    Training Loss: 0.38102003931999207\n",
            "Batch: 1813    Epoch[3/3]    Training Loss: 1.0503770112991333\n",
            "Batch: 1814    Epoch[3/3]    Training Loss: 0.5514904260635376\n",
            "Batch: 1815    Epoch[3/3]    Training Loss: 0.5888190269470215\n",
            "Batch: 1816    Epoch[3/3]    Training Loss: 0.9164813756942749\n",
            "Batch: 1817    Epoch[3/3]    Training Loss: 0.5745350122451782\n",
            "Batch: 1818    Epoch[3/3]    Training Loss: 0.23073670268058777\n",
            "Batch: 1819    Epoch[3/3]    Training Loss: 0.7977485656738281\n",
            "Batch: 1820    Epoch[3/3]    Training Loss: 0.6530736684799194\n",
            "Batch: 1821    Epoch[3/3]    Training Loss: 0.5910147428512573\n",
            "Batch: 1822    Epoch[3/3]    Training Loss: 0.4360474646091461\n",
            "Batch: 1823    Epoch[3/3]    Training Loss: 0.6697056889533997\n",
            "Batch: 1824    Epoch[3/3]    Training Loss: 0.4366665780544281\n",
            "Batch: 1825    Epoch[3/3]    Training Loss: 0.9149888753890991\n",
            "Batch: 1826    Epoch[3/3]    Training Loss: 0.5684599876403809\n",
            "Batch: 1827    Epoch[3/3]    Training Loss: 0.5510804653167725\n",
            "Batch: 1828    Epoch[3/3]    Training Loss: 0.9693350791931152\n",
            "Batch: 1829    Epoch[3/3]    Training Loss: 0.28168752789497375\n",
            "Batch: 1830    Epoch[3/3]    Training Loss: 0.3315127193927765\n",
            "Batch: 1831    Epoch[3/3]    Training Loss: 0.8893657326698303\n",
            "Batch: 1832    Epoch[3/3]    Training Loss: 0.3142729699611664\n",
            "Batch: 1833    Epoch[3/3]    Training Loss: 0.5318830013275146\n",
            "Batch: 1834    Epoch[3/3]    Training Loss: 0.1473081409931183\n",
            "Batch: 1835    Epoch[3/3]    Training Loss: 0.4936465620994568\n",
            "Batch: 1836    Epoch[3/3]    Training Loss: 0.6277631521224976\n",
            "Batch: 1837    Epoch[3/3]    Training Loss: 0.6394041776657104\n",
            "Batch: 1838    Epoch[3/3]    Training Loss: 0.9118496179580688\n",
            "Batch: 1839    Epoch[3/3]    Training Loss: 0.7203606367111206\n",
            "Batch: 1840    Epoch[3/3]    Training Loss: 0.28687676787376404\n",
            "Batch: 1841    Epoch[3/3]    Training Loss: 0.37719061970710754\n",
            "Batch: 1842    Epoch[3/3]    Training Loss: 0.6926866769790649\n",
            "Batch: 1843    Epoch[3/3]    Training Loss: 0.6834734082221985\n",
            "Batch: 1844    Epoch[3/3]    Training Loss: 0.45322197675704956\n",
            "Batch: 1845    Epoch[3/3]    Training Loss: 0.6366485357284546\n",
            "Batch: 1846    Epoch[3/3]    Training Loss: 0.6013579368591309\n",
            "Batch: 1847    Epoch[3/3]    Training Loss: 0.4827185273170471\n",
            "Batch: 1848    Epoch[3/3]    Training Loss: 0.22690057754516602\n",
            "Batch: 1849    Epoch[3/3]    Training Loss: 0.6642285585403442\n",
            "Batch: 1850    Epoch[3/3]    Training Loss: 0.5047003030776978\n",
            "Batch: 1851    Epoch[3/3]    Training Loss: 0.4679582417011261\n",
            "Batch: 1852    Epoch[3/3]    Training Loss: 0.718863844871521\n",
            "Batch: 1853    Epoch[3/3]    Training Loss: 0.2736088037490845\n",
            "Batch: 1854    Epoch[3/3]    Training Loss: 0.843666672706604\n",
            "Batch: 1855    Epoch[3/3]    Training Loss: 0.5883285999298096\n",
            "Batch: 1856    Epoch[3/3]    Training Loss: 0.3099753260612488\n",
            "Batch: 1857    Epoch[3/3]    Training Loss: 0.696357011795044\n",
            "Batch: 1858    Epoch[3/3]    Training Loss: 0.6093265414237976\n",
            "Batch: 1859    Epoch[3/3]    Training Loss: 1.3679273128509521\n",
            "Batch: 1860    Epoch[3/3]    Training Loss: 0.9419066309928894\n",
            "Batch: 1861    Epoch[3/3]    Training Loss: 0.2458985149860382\n",
            "Batch: 1862    Epoch[3/3]    Training Loss: 0.5173138380050659\n",
            "Batch: 1863    Epoch[3/3]    Training Loss: 0.8852408528327942\n",
            "Batch: 1864    Epoch[3/3]    Training Loss: 0.4653959274291992\n",
            "Batch: 1865    Epoch[3/3]    Training Loss: 0.19030234217643738\n",
            "Batch: 1866    Epoch[3/3]    Training Loss: 0.5392530560493469\n",
            "Batch: 1867    Epoch[3/3]    Training Loss: 0.7802485227584839\n",
            "Batch: 1868    Epoch[3/3]    Training Loss: 0.6712676286697388\n",
            "Batch: 1869    Epoch[3/3]    Training Loss: 0.3449644446372986\n",
            "Batch: 1870    Epoch[3/3]    Training Loss: 0.32772696018218994\n",
            "Batch: 1871    Epoch[3/3]    Training Loss: 0.3283734917640686\n",
            "Batch: 1872    Epoch[3/3]    Training Loss: 0.4475826025009155\n",
            "Batch: 1873    Epoch[3/3]    Training Loss: 0.7717679738998413\n",
            "Batch: 1874    Epoch[3/3]    Training Loss: 0.47379136085510254\n",
            "Batch: 1875    Epoch[3/3]    Training Loss: 0.246281236410141\n",
            "Batch: 1876    Epoch[3/3]    Training Loss: 0.597960889339447\n",
            "Batch: 1877    Epoch[3/3]    Training Loss: 0.44075945019721985\n",
            "Batch: 1878    Epoch[3/3]    Training Loss: 0.224773108959198\n",
            "Batch: 1879    Epoch[3/3]    Training Loss: 0.5243203639984131\n",
            "Batch: 1880    Epoch[3/3]    Training Loss: 0.27423834800720215\n",
            "Batch: 1881    Epoch[3/3]    Training Loss: 1.3749470710754395\n",
            "Batch: 1882    Epoch[3/3]    Training Loss: 0.44560837745666504\n",
            "Batch: 1883    Epoch[3/3]    Training Loss: 0.5846686363220215\n",
            "Batch: 1884    Epoch[3/3]    Training Loss: 0.306776225566864\n",
            "Batch: 1885    Epoch[3/3]    Training Loss: 1.1396872997283936\n",
            "Batch: 1886    Epoch[3/3]    Training Loss: 0.7216517329216003\n",
            "Batch: 1887    Epoch[3/3]    Training Loss: 0.5595008134841919\n",
            "Batch: 1888    Epoch[3/3]    Training Loss: 0.3464863896369934\n",
            "Batch: 1889    Epoch[3/3]    Training Loss: 1.053308367729187\n",
            "Batch: 1890    Epoch[3/3]    Training Loss: 0.4398694336414337\n",
            "Batch: 1891    Epoch[3/3]    Training Loss: 0.26516133546829224\n",
            "Batch: 1892    Epoch[3/3]    Training Loss: 0.437269926071167\n",
            "Batch: 1893    Epoch[3/3]    Training Loss: 0.4127703905105591\n",
            "Batch: 1894    Epoch[3/3]    Training Loss: 0.5938533544540405\n",
            "Batch: 1895    Epoch[3/3]    Training Loss: 0.8263131380081177\n",
            "Batch: 1896    Epoch[3/3]    Training Loss: 1.1520159244537354\n",
            "Batch: 1897    Epoch[3/3]    Training Loss: 0.5938640832901001\n",
            "Batch: 1898    Epoch[3/3]    Training Loss: 0.47610896825790405\n",
            "Batch: 1899    Epoch[3/3]    Training Loss: 0.8979539275169373\n",
            "Batch: 1900    Epoch[3/3]    Training Loss: 0.6851818561553955\n",
            "Batch: 1901    Epoch[3/3]    Training Loss: 0.4512134790420532\n",
            "Batch: 1902    Epoch[3/3]    Training Loss: 0.49528008699417114\n",
            "Batch: 1903    Epoch[3/3]    Training Loss: 0.680175244808197\n",
            "Batch: 1904    Epoch[3/3]    Training Loss: 1.1535981893539429\n",
            "Batch: 1905    Epoch[3/3]    Training Loss: 0.39762210845947266\n",
            "Batch: 1906    Epoch[3/3]    Training Loss: 0.2980993390083313\n",
            "Batch: 1907    Epoch[3/3]    Training Loss: 0.6714435815811157\n",
            "Batch: 1908    Epoch[3/3]    Training Loss: 0.31056711077690125\n",
            "Batch: 1909    Epoch[3/3]    Training Loss: 0.7997435331344604\n",
            "Batch: 1910    Epoch[3/3]    Training Loss: 0.6133458614349365\n",
            "Batch: 1911    Epoch[3/3]    Training Loss: 0.6760691404342651\n",
            "Batch: 1912    Epoch[3/3]    Training Loss: 0.6760613322257996\n",
            "Batch: 1913    Epoch[3/3]    Training Loss: 1.0601999759674072\n",
            "Batch: 1914    Epoch[3/3]    Training Loss: 0.8529232740402222\n",
            "Batch: 1915    Epoch[3/3]    Training Loss: 0.47636929154396057\n",
            "Batch: 1916    Epoch[3/3]    Training Loss: 0.7603224515914917\n",
            "Batch: 1917    Epoch[3/3]    Training Loss: 0.5240986943244934\n",
            "Batch: 1918    Epoch[3/3]    Training Loss: 0.5842441320419312\n",
            "Batch: 1919    Epoch[3/3]    Training Loss: 0.6406649351119995\n",
            "Batch: 1920    Epoch[3/3]    Training Loss: 0.8789041638374329\n",
            "Batch: 1921    Epoch[3/3]    Training Loss: 1.0450787544250488\n",
            "Batch: 1922    Epoch[3/3]    Training Loss: 0.5373701453208923\n",
            "Batch: 1923    Epoch[3/3]    Training Loss: 0.6462283134460449\n",
            "Batch: 1924    Epoch[3/3]    Training Loss: 1.119842767715454\n",
            "Batch: 1925    Epoch[3/3]    Training Loss: 0.6093310117721558\n",
            "Batch: 1926    Epoch[3/3]    Training Loss: 0.2794981002807617\n",
            "Batch: 1927    Epoch[3/3]    Training Loss: 0.4919614791870117\n",
            "Batch: 1928    Epoch[3/3]    Training Loss: 0.24531951546669006\n",
            "Batch: 1929    Epoch[3/3]    Training Loss: 1.1857619285583496\n",
            "Batch: 1930    Epoch[3/3]    Training Loss: 0.5590192079544067\n",
            "Batch: 1931    Epoch[3/3]    Training Loss: 0.6832943558692932\n",
            "Batch: 1932    Epoch[3/3]    Training Loss: 0.8286687135696411\n",
            "Batch: 1933    Epoch[3/3]    Training Loss: 0.4339166581630707\n",
            "Batch: 1934    Epoch[3/3]    Training Loss: 1.0852776765823364\n",
            "Batch: 1935    Epoch[3/3]    Training Loss: 0.4283981919288635\n",
            "Batch: 1936    Epoch[3/3]    Training Loss: 0.438782662153244\n",
            "Batch: 1937    Epoch[3/3]    Training Loss: 0.30914413928985596\n",
            "Batch: 1938    Epoch[3/3]    Training Loss: 0.49755269289016724\n",
            "Batch: 1939    Epoch[3/3]    Training Loss: 0.38342732191085815\n",
            "Batch: 1940    Epoch[3/3]    Training Loss: 0.5244125127792358\n",
            "Batch: 1941    Epoch[3/3]    Training Loss: 0.307181715965271\n",
            "Batch: 1942    Epoch[3/3]    Training Loss: 1.0064271688461304\n",
            "Batch: 1943    Epoch[3/3]    Training Loss: 0.44440001249313354\n",
            "Batch: 1944    Epoch[3/3]    Training Loss: 1.2371641397476196\n",
            "Batch: 1945    Epoch[3/3]    Training Loss: 0.27121126651763916\n",
            "Batch: 1946    Epoch[3/3]    Training Loss: 0.34771794080734253\n",
            "Batch: 1947    Epoch[3/3]    Training Loss: 0.8208715319633484\n",
            "Batch: 1948    Epoch[3/3]    Training Loss: 0.35886600613594055\n",
            "Batch: 1949    Epoch[3/3]    Training Loss: 0.46011561155319214\n",
            "Batch: 1950    Epoch[3/3]    Training Loss: 0.8084807395935059\n",
            "Batch: 1951    Epoch[3/3]    Training Loss: 0.7915917634963989\n",
            "Batch: 1952    Epoch[3/3]    Training Loss: 0.2993527054786682\n",
            "Batch: 1953    Epoch[3/3]    Training Loss: 1.0638878345489502\n",
            "Batch: 1954    Epoch[3/3]    Training Loss: 0.7060530185699463\n",
            "Batch: 1955    Epoch[3/3]    Training Loss: 0.6703200340270996\n",
            "Batch: 1956    Epoch[3/3]    Training Loss: 0.7393321990966797\n",
            "Batch: 1957    Epoch[3/3]    Training Loss: 0.73260098695755\n",
            "Batch: 1958    Epoch[3/3]    Training Loss: 0.17793944478034973\n",
            "Batch: 1959    Epoch[3/3]    Training Loss: 0.23251968622207642\n",
            "Batch: 1960    Epoch[3/3]    Training Loss: 0.5840257406234741\n",
            "Batch: 1961    Epoch[3/3]    Training Loss: 0.5718300342559814\n",
            "Batch: 1962    Epoch[3/3]    Training Loss: 1.3658063411712646\n",
            "Batch: 1963    Epoch[3/3]    Training Loss: 0.7767480611801147\n",
            "Batch: 1964    Epoch[3/3]    Training Loss: 0.4043411612510681\n",
            "Batch: 1965    Epoch[3/3]    Training Loss: 0.5786492824554443\n",
            "Batch: 1966    Epoch[3/3]    Training Loss: 0.5984904766082764\n",
            "Batch: 1967    Epoch[3/3]    Training Loss: 0.9826899766921997\n",
            "Batch: 1968    Epoch[3/3]    Training Loss: 0.9963062405586243\n",
            "Batch: 1969    Epoch[3/3]    Training Loss: 0.8893921375274658\n",
            "Batch: 1970    Epoch[3/3]    Training Loss: 0.2843749225139618\n",
            "Batch: 1971    Epoch[3/3]    Training Loss: 0.3135647475719452\n",
            "Batch: 1972    Epoch[3/3]    Training Loss: 0.324626088142395\n",
            "Batch: 1973    Epoch[3/3]    Training Loss: 0.33805572986602783\n",
            "Batch: 1974    Epoch[3/3]    Training Loss: 1.109619379043579\n",
            "Batch: 1975    Epoch[3/3]    Training Loss: 0.5946975350379944\n",
            "Batch: 1976    Epoch[3/3]    Training Loss: 0.25225743651390076\n",
            "Batch: 1977    Epoch[3/3]    Training Loss: 0.505139946937561\n",
            "Batch: 1978    Epoch[3/3]    Training Loss: 0.32139790058135986\n",
            "Batch: 1979    Epoch[3/3]    Training Loss: 0.7590005993843079\n",
            "Batch: 1980    Epoch[3/3]    Training Loss: 0.6815128326416016\n",
            "Batch: 1981    Epoch[3/3]    Training Loss: 0.5895849466323853\n",
            "Batch: 1982    Epoch[3/3]    Training Loss: 0.20108863711357117\n",
            "Batch: 1983    Epoch[3/3]    Training Loss: 0.9745484590530396\n",
            "Batch: 1984    Epoch[3/3]    Training Loss: 0.46213704347610474\n",
            "Batch: 1985    Epoch[3/3]    Training Loss: 0.7146772742271423\n",
            "Batch: 1986    Epoch[3/3]    Training Loss: 0.47974181175231934\n",
            "Batch: 1987    Epoch[3/3]    Training Loss: 0.5721439123153687\n",
            "Batch: 1988    Epoch[3/3]    Training Loss: 1.0481880903244019\n",
            "Batch: 1989    Epoch[3/3]    Training Loss: 0.8122867941856384\n",
            "Batch: 1990    Epoch[3/3]    Training Loss: 0.5429750680923462\n",
            "Batch: 1991    Epoch[3/3]    Training Loss: 0.22369860112667084\n",
            "Batch: 1992    Epoch[3/3]    Training Loss: 0.5552355647087097\n",
            "Batch: 1993    Epoch[3/3]    Training Loss: 0.4067528247833252\n",
            "Batch: 1994    Epoch[3/3]    Training Loss: 0.32863670587539673\n",
            "Batch: 1995    Epoch[3/3]    Training Loss: 0.6096290946006775\n",
            "Batch: 1996    Epoch[3/3]    Training Loss: 0.5260478854179382\n",
            "Batch: 1997    Epoch[3/3]    Training Loss: 0.6311677694320679\n",
            "Batch: 1998    Epoch[3/3]    Training Loss: 0.6907159090042114\n",
            "Batch: 1999    Epoch[3/3]    Training Loss: 0.4766588509082794\n",
            "Batch: 2000    Epoch[3/3]    Training Loss: 1.2666168212890625\n",
            "Batch: 2001    Epoch[3/3]    Training Loss: 1.0380122661590576\n",
            "Batch: 2002    Epoch[3/3]    Training Loss: 0.36191326379776\n",
            "Batch: 2003    Epoch[3/3]    Training Loss: 0.3673146069049835\n",
            "Batch: 2004    Epoch[3/3]    Training Loss: 0.9392424821853638\n",
            "Batch: 2005    Epoch[3/3]    Training Loss: 0.8698301315307617\n",
            "Batch: 2006    Epoch[3/3]    Training Loss: 0.48078653216362\n",
            "Batch: 2007    Epoch[3/3]    Training Loss: 0.3511465787887573\n",
            "Batch: 2008    Epoch[3/3]    Training Loss: 0.3174649477005005\n",
            "Batch: 2009    Epoch[3/3]    Training Loss: 0.9780480265617371\n",
            "Batch: 2010    Epoch[3/3]    Training Loss: 0.5202885270118713\n",
            "Batch: 2011    Epoch[3/3]    Training Loss: 0.2823922634124756\n",
            "Batch: 2012    Epoch[3/3]    Training Loss: 0.19614674150943756\n",
            "Batch: 2013    Epoch[3/3]    Training Loss: 0.44818657636642456\n",
            "Batch: 2014    Epoch[3/3]    Training Loss: 0.4650542140007019\n",
            "Batch: 2015    Epoch[3/3]    Training Loss: 0.7158523201942444\n",
            "Batch: 2016    Epoch[3/3]    Training Loss: 0.6589367389678955\n",
            "Batch: 2017    Epoch[3/3]    Training Loss: 1.0164072513580322\n",
            "Batch: 2018    Epoch[3/3]    Training Loss: 1.133150577545166\n",
            "Batch: 2019    Epoch[3/3]    Training Loss: 0.3384121060371399\n",
            "Batch: 2020    Epoch[3/3]    Training Loss: 0.37117916345596313\n",
            "Batch: 2021    Epoch[3/3]    Training Loss: 0.7962372303009033\n",
            "Batch: 2022    Epoch[3/3]    Training Loss: 0.573494553565979\n",
            "Batch: 2023    Epoch[3/3]    Training Loss: 0.7336394786834717\n",
            "Batch: 2024    Epoch[3/3]    Training Loss: 0.47207945585250854\n",
            "Batch: 2025    Epoch[3/3]    Training Loss: 0.6066377758979797\n",
            "Batch: 2026    Epoch[3/3]    Training Loss: 0.19950757920742035\n",
            "Batch: 2027    Epoch[3/3]    Training Loss: 0.6911040544509888\n",
            "Batch: 2028    Epoch[3/3]    Training Loss: 0.6995836496353149\n",
            "Batch: 2029    Epoch[3/3]    Training Loss: 0.5667173862457275\n",
            "Batch: 2030    Epoch[3/3]    Training Loss: 1.2498522996902466\n",
            "Batch: 2031    Epoch[3/3]    Training Loss: 0.5574754476547241\n",
            "Batch: 2032    Epoch[3/3]    Training Loss: 0.36709585785865784\n",
            "Batch: 2033    Epoch[3/3]    Training Loss: 0.46378785371780396\n",
            "Batch: 2034    Epoch[3/3]    Training Loss: 0.7874621748924255\n",
            "Batch: 2035    Epoch[3/3]    Training Loss: 0.310039222240448\n",
            "Batch: 2036    Epoch[3/3]    Training Loss: 1.0690220594406128\n",
            "Batch: 2037    Epoch[3/3]    Training Loss: 1.176773190498352\n",
            "Batch: 2038    Epoch[3/3]    Training Loss: 0.7673799991607666\n",
            "Batch: 2039    Epoch[3/3]    Training Loss: 0.5044283270835876\n",
            "Batch: 2040    Epoch[3/3]    Training Loss: 0.41149091720581055\n",
            "Batch: 2041    Epoch[3/3]    Training Loss: 0.6940019726753235\n",
            "Batch: 2042    Epoch[3/3]    Training Loss: 0.5160372853279114\n",
            "Batch: 2043    Epoch[3/3]    Training Loss: 0.5780532956123352\n",
            "Batch: 2044    Epoch[3/3]    Training Loss: 0.5359153747558594\n",
            "Batch: 2045    Epoch[3/3]    Training Loss: 0.78484046459198\n",
            "Batch: 2046    Epoch[3/3]    Training Loss: 0.35448697209358215\n",
            "Batch: 2047    Epoch[3/3]    Training Loss: 0.8645048141479492\n",
            "Batch: 2048    Epoch[3/3]    Training Loss: 0.8243354558944702\n",
            "Batch: 2049    Epoch[3/3]    Training Loss: 0.4560445249080658\n",
            "Batch: 2050    Epoch[3/3]    Training Loss: 0.3900556266307831\n",
            "Batch: 2051    Epoch[3/3]    Training Loss: 0.5145153999328613\n",
            "Batch: 2052    Epoch[3/3]    Training Loss: 0.6438735127449036\n",
            "Batch: 2053    Epoch[3/3]    Training Loss: 0.17110949754714966\n",
            "Batch: 2054    Epoch[3/3]    Training Loss: 0.6675729155540466\n",
            "Batch: 2055    Epoch[3/3]    Training Loss: 0.29655343294143677\n",
            "Batch: 2056    Epoch[3/3]    Training Loss: 0.3876880407333374\n",
            "Batch: 2057    Epoch[3/3]    Training Loss: 0.5549715757369995\n",
            "Batch: 2058    Epoch[3/3]    Training Loss: 0.30513840913772583\n",
            "Batch: 2059    Epoch[3/3]    Training Loss: 0.7708127498626709\n",
            "Batch: 2060    Epoch[3/3]    Training Loss: 0.5003697872161865\n",
            "Batch: 2061    Epoch[3/3]    Training Loss: 0.29965752363204956\n",
            "Batch: 2062    Epoch[3/3]    Training Loss: 0.36329787969589233\n",
            "Batch: 2063    Epoch[3/3]    Training Loss: 0.5954518914222717\n",
            "Batch: 2064    Epoch[3/3]    Training Loss: 0.556788444519043\n",
            "Batch: 2065    Epoch[3/3]    Training Loss: 0.6070172190666199\n",
            "Batch: 2066    Epoch[3/3]    Training Loss: 0.4196651577949524\n",
            "Batch: 2067    Epoch[3/3]    Training Loss: 0.3650829493999481\n",
            "Batch: 2068    Epoch[3/3]    Training Loss: 0.6269667148590088\n",
            "Batch: 2069    Epoch[3/3]    Training Loss: 0.48929035663604736\n",
            "Batch: 2070    Epoch[3/3]    Training Loss: 0.6950260400772095\n",
            "Batch: 2071    Epoch[3/3]    Training Loss: 0.24916452169418335\n",
            "Batch: 2072    Epoch[3/3]    Training Loss: 0.5238547921180725\n",
            "Batch: 2073    Epoch[3/3]    Training Loss: 0.6376380324363708\n",
            "Batch: 2074    Epoch[3/3]    Training Loss: 0.5276432037353516\n",
            "Batch: 2075    Epoch[3/3]    Training Loss: 0.5724462270736694\n",
            "Batch: 2076    Epoch[3/3]    Training Loss: 0.2846226096153259\n",
            "Batch: 2077    Epoch[3/3]    Training Loss: 1.0525486469268799\n",
            "Batch: 2078    Epoch[3/3]    Training Loss: 0.3485339879989624\n",
            "Batch: 2079    Epoch[3/3]    Training Loss: 0.6415634751319885\n",
            "Batch: 2080    Epoch[3/3]    Training Loss: 0.7765984535217285\n",
            "Batch: 2081    Epoch[3/3]    Training Loss: 0.3271840512752533\n",
            "Batch: 2082    Epoch[3/3]    Training Loss: 0.403320848941803\n",
            "Batch: 2083    Epoch[3/3]    Training Loss: 0.741619348526001\n",
            "Batch: 2084    Epoch[3/3]    Training Loss: 0.5047097206115723\n",
            "Batch: 2085    Epoch[3/3]    Training Loss: 1.2685471773147583\n",
            "Batch: 2086    Epoch[3/3]    Training Loss: 0.7499269247055054\n",
            "Batch: 2087    Epoch[3/3]    Training Loss: 0.7231735587120056\n",
            "Batch: 2088    Epoch[3/3]    Training Loss: 0.4543071985244751\n",
            "Batch: 2089    Epoch[3/3]    Training Loss: 0.2744418680667877\n",
            "Batch: 2090    Epoch[3/3]    Training Loss: 0.9308779239654541\n",
            "Batch: 2091    Epoch[3/3]    Training Loss: 0.21362023055553436\n",
            "Batch: 2092    Epoch[3/3]    Training Loss: 0.9110972285270691\n",
            "Batch: 2093    Epoch[3/3]    Training Loss: 0.7686010003089905\n",
            "Batch: 2094    Epoch[3/3]    Training Loss: 0.4829058051109314\n",
            "Batch: 2095    Epoch[3/3]    Training Loss: 0.634669303894043\n",
            "Batch: 2096    Epoch[3/3]    Training Loss: 0.7257816791534424\n",
            "Batch: 2097    Epoch[3/3]    Training Loss: 0.5833959579467773\n",
            "Batch: 2098    Epoch[3/3]    Training Loss: 0.788953959941864\n",
            "Batch: 2099    Epoch[3/3]    Training Loss: 0.5679422616958618\n",
            "Batch: 2100    Epoch[3/3]    Training Loss: 0.40125417709350586\n",
            "Batch: 2101    Epoch[3/3]    Training Loss: 0.39171111583709717\n",
            "Batch: 2102    Epoch[3/3]    Training Loss: 0.27033406496047974\n",
            "Batch: 2103    Epoch[3/3]    Training Loss: 0.8537434935569763\n",
            "Batch: 2104    Epoch[3/3]    Training Loss: 0.17591440677642822\n",
            "Batch: 2105    Epoch[3/3]    Training Loss: 0.7967050075531006\n",
            "Batch: 2106    Epoch[3/3]    Training Loss: 0.936826229095459\n",
            "Batch: 2107    Epoch[3/3]    Training Loss: 0.6008287668228149\n",
            "Batch: 2108    Epoch[3/3]    Training Loss: 0.26631954312324524\n",
            "Batch: 2109    Epoch[3/3]    Training Loss: 0.619343101978302\n",
            "Batch: 2110    Epoch[3/3]    Training Loss: 0.6809890270233154\n",
            "Batch: 2111    Epoch[3/3]    Training Loss: 0.4218080937862396\n",
            "Batch: 2112    Epoch[3/3]    Training Loss: 0.563592791557312\n",
            "Batch: 2113    Epoch[3/3]    Training Loss: 0.7836892604827881\n",
            "Batch: 2114    Epoch[3/3]    Training Loss: 0.25169819593429565\n",
            "Batch: 2115    Epoch[3/3]    Training Loss: 0.4308127760887146\n",
            "Batch: 2116    Epoch[3/3]    Training Loss: 0.2703254818916321\n",
            "Batch: 2117    Epoch[3/3]    Training Loss: 0.7962578535079956\n",
            "Batch: 2118    Epoch[3/3]    Training Loss: 0.675830602645874\n",
            "Batch: 2119    Epoch[3/3]    Training Loss: 0.46270471811294556\n",
            "Batch: 2120    Epoch[3/3]    Training Loss: 0.431424617767334\n",
            "Batch: 2121    Epoch[3/3]    Training Loss: 0.7293688058853149\n",
            "Batch: 2122    Epoch[3/3]    Training Loss: 0.8608458042144775\n",
            "Batch: 2123    Epoch[3/3]    Training Loss: 0.5982254147529602\n",
            "Batch: 2124    Epoch[3/3]    Training Loss: 0.38507819175720215\n",
            "Batch: 2125    Epoch[3/3]    Training Loss: 1.1027841567993164\n",
            "Batch: 2126    Epoch[3/3]    Training Loss: 0.730751633644104\n",
            "Batch: 2127    Epoch[3/3]    Training Loss: 0.8717229962348938\n",
            "Batch: 2128    Epoch[3/3]    Training Loss: 0.24454352259635925\n",
            "Batch: 2129    Epoch[3/3]    Training Loss: 0.4400097131729126\n",
            "Batch: 2130    Epoch[3/3]    Training Loss: 0.5245680809020996\n",
            "Batch: 2131    Epoch[3/3]    Training Loss: 0.6226715445518494\n",
            "Batch: 2132    Epoch[3/3]    Training Loss: 0.5783524513244629\n",
            "Batch: 2133    Epoch[3/3]    Training Loss: 0.18702438473701477\n",
            "Batch: 2134    Epoch[3/3]    Training Loss: 0.3682013154029846\n",
            "Batch: 2135    Epoch[3/3]    Training Loss: 0.22410888969898224\n",
            "Batch: 2136    Epoch[3/3]    Training Loss: 0.5241367816925049\n",
            "Batch: 2137    Epoch[3/3]    Training Loss: 0.6329227089881897\n",
            "Batch: 2138    Epoch[3/3]    Training Loss: 0.6347992420196533\n",
            "Batch: 2139    Epoch[3/3]    Training Loss: 0.7000654935836792\n",
            "Batch: 2140    Epoch[3/3]    Training Loss: 0.3287709653377533\n",
            "Batch: 2141    Epoch[3/3]    Training Loss: 0.22679026424884796\n",
            "Batch: 2142    Epoch[3/3]    Training Loss: 0.9456437230110168\n",
            "Batch: 2143    Epoch[3/3]    Training Loss: 0.5222802758216858\n",
            "Batch: 2144    Epoch[3/3]    Training Loss: 0.6176936030387878\n",
            "Batch: 2145    Epoch[3/3]    Training Loss: 0.5178735852241516\n",
            "Batch: 2146    Epoch[3/3]    Training Loss: 0.9648549556732178\n",
            "Batch: 2147    Epoch[3/3]    Training Loss: 0.3428901135921478\n",
            "Batch: 2148    Epoch[3/3]    Training Loss: 0.5636162757873535\n",
            "Batch: 2149    Epoch[3/3]    Training Loss: 0.2572084069252014\n",
            "Batch: 2150    Epoch[3/3]    Training Loss: 0.9131043553352356\n",
            "Batch: 2151    Epoch[3/3]    Training Loss: 1.0256764888763428\n",
            "Batch: 2152    Epoch[3/3]    Training Loss: 0.4473578929901123\n",
            "Batch: 2153    Epoch[3/3]    Training Loss: 0.658477783203125\n",
            "Batch: 2154    Epoch[3/3]    Training Loss: 0.46944624185562134\n",
            "Batch: 2155    Epoch[3/3]    Training Loss: 0.7616970539093018\n",
            "Batch: 2156    Epoch[3/3]    Training Loss: 0.2051236927509308\n",
            "Batch: 2157    Epoch[3/3]    Training Loss: 0.1763959676027298\n",
            "Batch: 2158    Epoch[3/3]    Training Loss: 0.21084710955619812\n",
            "Batch: 2159    Epoch[3/3]    Training Loss: 0.5685598850250244\n",
            "Batch: 2160    Epoch[3/3]    Training Loss: 1.090174674987793\n",
            "Batch: 2161    Epoch[3/3]    Training Loss: 0.3721354603767395\n",
            "Batch: 2162    Epoch[3/3]    Training Loss: 0.8519523739814758\n",
            "Batch: 2163    Epoch[3/3]    Training Loss: 0.544179379940033\n",
            "Batch: 2164    Epoch[3/3]    Training Loss: 0.1696670800447464\n",
            "Batch: 2165    Epoch[3/3]    Training Loss: 0.4074261784553528\n",
            "Batch: 2166    Epoch[3/3]    Training Loss: 0.7416588664054871\n",
            "Batch: 2167    Epoch[3/3]    Training Loss: 0.4839097857475281\n",
            "Batch: 2168    Epoch[3/3]    Training Loss: 0.40867936611175537\n",
            "Batch: 2169    Epoch[3/3]    Training Loss: 0.5784591436386108\n",
            "Batch: 2170    Epoch[3/3]    Training Loss: 0.37366360425949097\n",
            "Batch: 2171    Epoch[3/3]    Training Loss: 0.4600592255592346\n",
            "Batch: 2172    Epoch[3/3]    Training Loss: 0.32959216833114624\n",
            "Batch: 2173    Epoch[3/3]    Training Loss: 0.8533222675323486\n",
            "Batch: 2174    Epoch[3/3]    Training Loss: 0.7497206926345825\n",
            "Batch: 2175    Epoch[3/3]    Training Loss: 0.546507716178894\n",
            "Batch: 2176    Epoch[3/3]    Training Loss: 0.48456424474716187\n",
            "Batch: 2177    Epoch[3/3]    Training Loss: 0.5283890962600708\n",
            "Batch: 2178    Epoch[3/3]    Training Loss: 0.8707345128059387\n",
            "Batch: 2179    Epoch[3/3]    Training Loss: 0.27925950288772583\n",
            "Batch: 2180    Epoch[3/3]    Training Loss: 0.8753433227539062\n",
            "Batch: 2181    Epoch[3/3]    Training Loss: 0.6604628562927246\n",
            "Batch: 2182    Epoch[3/3]    Training Loss: 0.33410578966140747\n",
            "Batch: 2183    Epoch[3/3]    Training Loss: 0.4496452212333679\n",
            "Batch: 2184    Epoch[3/3]    Training Loss: 0.5874201655387878\n",
            "Batch: 2185    Epoch[3/3]    Training Loss: 0.3904629349708557\n",
            "Batch: 2186    Epoch[3/3]    Training Loss: 0.4627441167831421\n",
            "Batch: 2187    Epoch[3/3]    Training Loss: 0.6011406183242798\n",
            "Batch: 2188    Epoch[3/3]    Training Loss: 0.21001169085502625\n",
            "Batch: 2189    Epoch[3/3]    Training Loss: 0.436748206615448\n",
            "Batch: 2190    Epoch[3/3]    Training Loss: 0.46361228823661804\n",
            "Batch: 2191    Epoch[3/3]    Training Loss: 0.5244954824447632\n",
            "Batch: 2192    Epoch[3/3]    Training Loss: 1.102294921875\n",
            "Batch: 2193    Epoch[3/3]    Training Loss: 0.7549684643745422\n",
            "Batch: 2194    Epoch[3/3]    Training Loss: 0.44674044847488403\n",
            "Batch: 2195    Epoch[3/3]    Training Loss: 0.4104636311531067\n",
            "Batch: 2196    Epoch[3/3]    Training Loss: 0.3942427933216095\n",
            "Batch: 2197    Epoch[3/3]    Training Loss: 0.49621784687042236\n",
            "Batch: 2198    Epoch[3/3]    Training Loss: 0.6225267648696899\n",
            "Batch: 2199    Epoch[3/3]    Training Loss: 0.8650953769683838\n",
            "Batch: 2200    Epoch[3/3]    Training Loss: 0.8781845569610596\n",
            "Batch: 2201    Epoch[3/3]    Training Loss: 0.32966306805610657\n",
            "Batch: 2202    Epoch[3/3]    Training Loss: 0.38884884119033813\n",
            "Batch: 2203    Epoch[3/3]    Training Loss: 0.3896005153656006\n",
            "Batch: 2204    Epoch[3/3]    Training Loss: 1.1081622838974\n",
            "Batch: 2205    Epoch[3/3]    Training Loss: 1.363114356994629\n",
            "Batch: 2206    Epoch[3/3]    Training Loss: 0.833191990852356\n",
            "Batch: 2207    Epoch[3/3]    Training Loss: 1.4655909538269043\n",
            "Batch: 2208    Epoch[3/3]    Training Loss: 0.5338301658630371\n",
            "Batch: 2209    Epoch[3/3]    Training Loss: 0.8024394512176514\n",
            "Batch: 2210    Epoch[3/3]    Training Loss: 0.5050186514854431\n",
            "Batch: 2211    Epoch[3/3]    Training Loss: 0.5200520753860474\n",
            "Batch: 2212    Epoch[3/3]    Training Loss: 0.3330847918987274\n",
            "Batch: 2213    Epoch[3/3]    Training Loss: 0.6131980419158936\n",
            "Batch: 2214    Epoch[3/3]    Training Loss: 1.246349573135376\n",
            "Batch: 2215    Epoch[3/3]    Training Loss: 0.4018503427505493\n",
            "Batch: 2216    Epoch[3/3]    Training Loss: 0.49312829971313477\n",
            "Batch: 2217    Epoch[3/3]    Training Loss: 0.7461560368537903\n",
            "Batch: 2218    Epoch[3/3]    Training Loss: 0.6461877822875977\n",
            "Batch: 2219    Epoch[3/3]    Training Loss: 0.6012183427810669\n",
            "Batch: 2220    Epoch[3/3]    Training Loss: 0.8291566371917725\n",
            "Batch: 2221    Epoch[3/3]    Training Loss: 0.5105844736099243\n",
            "Batch: 2222    Epoch[3/3]    Training Loss: 0.4522148370742798\n",
            "Batch: 2223    Epoch[3/3]    Training Loss: 0.3356107175350189\n",
            "Batch: 2224    Epoch[3/3]    Training Loss: 0.4798682928085327\n",
            "Batch: 2225    Epoch[3/3]    Training Loss: 0.3777312934398651\n",
            "Batch: 2226    Epoch[3/3]    Training Loss: 0.18676283955574036\n",
            "Batch: 2227    Epoch[3/3]    Training Loss: 0.4025946259498596\n",
            "Batch: 2228    Epoch[3/3]    Training Loss: 0.3956386148929596\n",
            "Batch: 2229    Epoch[3/3]    Training Loss: 0.611721396446228\n",
            "Batch: 2230    Epoch[3/3]    Training Loss: 0.36873334646224976\n",
            "Batch: 2231    Epoch[3/3]    Training Loss: 0.5243208408355713\n",
            "Batch: 2232    Epoch[3/3]    Training Loss: 0.40786975622177124\n",
            "Batch: 2233    Epoch[3/3]    Training Loss: 0.32520633935928345\n",
            "Batch: 2234    Epoch[3/3]    Training Loss: 1.0029722452163696\n",
            "Batch: 2235    Epoch[3/3]    Training Loss: 0.5559528470039368\n",
            "Batch: 2236    Epoch[3/3]    Training Loss: 0.37119260430336\n",
            "Batch: 2237    Epoch[3/3]    Training Loss: 0.3612717390060425\n",
            "Batch: 2238    Epoch[3/3]    Training Loss: 0.70618736743927\n",
            "Batch: 2239    Epoch[3/3]    Training Loss: 0.25128042697906494\n",
            "Batch: 2240    Epoch[3/3]    Training Loss: 0.5973834991455078\n",
            "Batch: 2241    Epoch[3/3]    Training Loss: 0.8118759393692017\n",
            "Batch: 2242    Epoch[3/3]    Training Loss: 0.22167955338954926\n",
            "Batch: 2243    Epoch[3/3]    Training Loss: 0.3417036831378937\n",
            "Batch: 2244    Epoch[3/3]    Training Loss: 1.1261703968048096\n",
            "Batch: 2245    Epoch[3/3]    Training Loss: 1.5213494300842285\n",
            "Batch: 2246    Epoch[3/3]    Training Loss: 0.9212639927864075\n",
            "Batch: 2247    Epoch[3/3]    Training Loss: 0.10065040737390518\n",
            "Batch: 2248    Epoch[3/3]    Training Loss: 0.6420953273773193\n",
            "Batch: 2249    Epoch[3/3]    Training Loss: 0.7532275319099426\n",
            "Batch: 2250    Epoch[3/3]    Training Loss: 0.6258716583251953\n",
            "Batch: 2251    Epoch[3/3]    Training Loss: 0.681727409362793\n",
            "Batch: 2252    Epoch[3/3]    Training Loss: 0.8533948063850403\n",
            "Batch: 2253    Epoch[3/3]    Training Loss: 0.3895006775856018\n",
            "Batch: 2254    Epoch[3/3]    Training Loss: 0.6825459003448486\n",
            "Batch: 2255    Epoch[3/3]    Training Loss: 0.5114443302154541\n",
            "Batch: 2256    Epoch[3/3]    Training Loss: 0.8822816610336304\n",
            "Batch: 2257    Epoch[3/3]    Training Loss: 0.2264830768108368\n",
            "Batch: 2258    Epoch[3/3]    Training Loss: 0.20001249015331268\n",
            "Batch: 2259    Epoch[3/3]    Training Loss: 0.6068167686462402\n",
            "Batch: 2260    Epoch[3/3]    Training Loss: 0.7256747484207153\n",
            "Batch: 2261    Epoch[3/3]    Training Loss: 0.22611244022846222\n",
            "Batch: 2262    Epoch[3/3]    Training Loss: 0.7521288990974426\n",
            "Batch: 2263    Epoch[3/3]    Training Loss: 0.49414369463920593\n",
            "Batch: 2264    Epoch[3/3]    Training Loss: 0.5865570306777954\n",
            "Batch: 2265    Epoch[3/3]    Training Loss: 0.4590710401535034\n",
            "Batch: 2266    Epoch[3/3]    Training Loss: 0.8825255036354065\n",
            "Batch: 2267    Epoch[3/3]    Training Loss: 0.7408086657524109\n",
            "Batch: 2268    Epoch[3/3]    Training Loss: 0.6174030900001526\n",
            "Batch: 2269    Epoch[3/3]    Training Loss: 0.32348543405532837\n",
            "Batch: 2270    Epoch[3/3]    Training Loss: 0.25982654094696045\n",
            "Batch: 2271    Epoch[3/3]    Training Loss: 0.47654104232788086\n",
            "Batch: 2272    Epoch[3/3]    Training Loss: 0.591941237449646\n",
            "Batch: 2273    Epoch[3/3]    Training Loss: 0.5461079478263855\n",
            "Batch: 2274    Epoch[3/3]    Training Loss: 0.6318690776824951\n",
            "Batch: 2275    Epoch[3/3]    Training Loss: 0.9826830625534058\n",
            "Batch: 2276    Epoch[3/3]    Training Loss: 0.5873516201972961\n",
            "Batch: 2277    Epoch[3/3]    Training Loss: 0.9803431034088135\n",
            "Batch: 2278    Epoch[3/3]    Training Loss: 0.5850352048873901\n",
            "Batch: 2279    Epoch[3/3]    Training Loss: 0.715628981590271\n",
            "Batch: 2280    Epoch[3/3]    Training Loss: 0.47613030672073364\n",
            "Batch: 2281    Epoch[3/3]    Training Loss: 0.9912079572677612\n",
            "Batch: 2282    Epoch[3/3]    Training Loss: 0.44336551427841187\n",
            "Batch: 2283    Epoch[3/3]    Training Loss: 0.5697616338729858\n",
            "Batch: 2284    Epoch[3/3]    Training Loss: 1.1223387718200684\n",
            "Batch: 2285    Epoch[3/3]    Training Loss: 0.7175137996673584\n",
            "Batch: 2286    Epoch[3/3]    Training Loss: 0.697767972946167\n",
            "Batch: 2287    Epoch[3/3]    Training Loss: 0.7153019905090332\n",
            "Batch: 2288    Epoch[3/3]    Training Loss: 0.7401494383811951\n",
            "Batch: 2289    Epoch[3/3]    Training Loss: 0.5549384951591492\n",
            "Batch: 2290    Epoch[3/3]    Training Loss: 0.8642942905426025\n",
            "Batch: 2291    Epoch[3/3]    Training Loss: 0.7267781496047974\n",
            "Batch: 2292    Epoch[3/3]    Training Loss: 0.4247191548347473\n",
            "Batch: 2293    Epoch[3/3]    Training Loss: 0.863905668258667\n",
            "Batch: 2294    Epoch[3/3]    Training Loss: 0.3124290704727173\n",
            "Batch: 2295    Epoch[3/3]    Training Loss: 0.7557028532028198\n",
            "Batch: 2296    Epoch[3/3]    Training Loss: 0.37205368280410767\n",
            "Batch: 2297    Epoch[3/3]    Training Loss: 0.45931440591812134\n",
            "Batch: 2298    Epoch[3/3]    Training Loss: 0.22631004452705383\n",
            "Batch: 2299    Epoch[3/3]    Training Loss: 0.7936480641365051\n",
            "Batch: 2300    Epoch[3/3]    Training Loss: 0.6501505970954895\n",
            "Batch: 2301    Epoch[3/3]    Training Loss: 0.39660292863845825\n",
            "Batch: 2302    Epoch[3/3]    Training Loss: 0.7277659773826599\n",
            "Batch: 2303    Epoch[3/3]    Training Loss: 0.562102198600769\n",
            "Batch: 2304    Epoch[3/3]    Training Loss: 0.7251585721969604\n",
            "Batch: 2305    Epoch[3/3]    Training Loss: 0.7167243957519531\n",
            "Batch: 2306    Epoch[3/3]    Training Loss: 0.49654054641723633\n",
            "Batch: 2307    Epoch[3/3]    Training Loss: 1.261542797088623\n",
            "Batch: 2308    Epoch[3/3]    Training Loss: 0.7790433168411255\n",
            "Batch: 2309    Epoch[3/3]    Training Loss: 0.5025243759155273\n",
            "Batch: 2310    Epoch[3/3]    Training Loss: 0.22660773992538452\n",
            "Batch: 2311    Epoch[3/3]    Training Loss: 0.666566014289856\n",
            "Batch: 2312    Epoch[3/3]    Training Loss: 0.4052489399909973\n",
            "Batch: 2313    Epoch[3/3]    Training Loss: 0.3184066116809845\n",
            "Batch: 2314    Epoch[3/3]    Training Loss: 0.420071005821228\n",
            "Batch: 2315    Epoch[3/3]    Training Loss: 0.7735990285873413\n",
            "Batch: 2316    Epoch[3/3]    Training Loss: 0.5560189485549927\n",
            "Batch: 2317    Epoch[3/3]    Training Loss: 0.11680834740400314\n",
            "Batch: 2318    Epoch[3/3]    Training Loss: 0.742251992225647\n",
            "Batch: 2319    Epoch[3/3]    Training Loss: 0.7598729133605957\n",
            "Batch: 2320    Epoch[3/3]    Training Loss: 0.5671567320823669\n",
            "Batch: 2321    Epoch[3/3]    Training Loss: 0.2980247139930725\n",
            "Batch: 2322    Epoch[3/3]    Training Loss: 0.9798298478126526\n",
            "Batch: 2323    Epoch[3/3]    Training Loss: 0.8217591047286987\n",
            "Batch: 2324    Epoch[3/3]    Training Loss: 0.47519439458847046\n",
            "Batch: 2325    Epoch[3/3]    Training Loss: 1.2602159976959229\n",
            "Batch: 2326    Epoch[3/3]    Training Loss: 0.475871205329895\n",
            "Batch: 2327    Epoch[3/3]    Training Loss: 0.6815912127494812\n",
            "Batch: 2328    Epoch[3/3]    Training Loss: 0.7475303411483765\n",
            "Batch: 2329    Epoch[3/3]    Training Loss: 0.22171734273433685\n",
            "Batch: 2330    Epoch[3/3]    Training Loss: 0.7037020325660706\n",
            "Batch: 2331    Epoch[3/3]    Training Loss: 0.6288684606552124\n",
            "Batch: 2332    Epoch[3/3]    Training Loss: 0.771393895149231\n",
            "Batch: 2333    Epoch[3/3]    Training Loss: 0.7870455384254456\n",
            "Batch: 2334    Epoch[3/3]    Training Loss: 0.5009499788284302\n",
            "Batch: 2335    Epoch[3/3]    Training Loss: 0.3604968786239624\n",
            "Batch: 2336    Epoch[3/3]    Training Loss: 0.35029810667037964\n",
            "Batch: 2337    Epoch[3/3]    Training Loss: 0.4502260684967041\n",
            "Batch: 2338    Epoch[3/3]    Training Loss: 0.32933926582336426\n",
            "Batch: 2339    Epoch[3/3]    Training Loss: 0.5820067524909973\n",
            "Batch: 2340    Epoch[3/3]    Training Loss: 0.22697997093200684\n",
            "Batch: 2341    Epoch[3/3]    Training Loss: 0.8893828392028809\n",
            "Batch: 2342    Epoch[3/3]    Training Loss: 0.7750928401947021\n",
            "Batch: 2343    Epoch[3/3]    Training Loss: 1.3527939319610596\n",
            "Batch: 2344    Epoch[3/3]    Training Loss: 0.5157468318939209\n",
            "Batch: 2345    Epoch[3/3]    Training Loss: 0.5509898662567139\n",
            "Batch: 2346    Epoch[3/3]    Training Loss: 0.6598355174064636\n",
            "Batch: 2347    Epoch[3/3]    Training Loss: 0.3214123249053955\n",
            "Batch: 2348    Epoch[3/3]    Training Loss: 0.3562946915626526\n",
            "Batch: 2349    Epoch[3/3]    Training Loss: 0.5924186110496521\n",
            "Batch: 2350    Epoch[3/3]    Training Loss: 0.7483079433441162\n",
            "Batch: 2351    Epoch[3/3]    Training Loss: 0.4040052890777588\n",
            "Batch: 2352    Epoch[3/3]    Training Loss: 0.7555815577507019\n",
            "Batch: 2353    Epoch[3/3]    Training Loss: 0.508541464805603\n",
            "Batch: 2354    Epoch[3/3]    Training Loss: 0.5837432742118835\n",
            "Batch: 2355    Epoch[3/3]    Training Loss: 0.8461917638778687\n",
            "Batch: 2356    Epoch[3/3]    Training Loss: 0.5433108806610107\n",
            "Batch: 2357    Epoch[3/3]    Training Loss: 0.8593894243240356\n",
            "Batch: 2358    Epoch[3/3]    Training Loss: 0.391361266374588\n",
            "Batch: 2359    Epoch[3/3]    Training Loss: 0.6355270147323608\n",
            "Batch: 2360    Epoch[3/3]    Training Loss: 0.6866859197616577\n",
            "Batch: 2361    Epoch[3/3]    Training Loss: 0.4415213465690613\n",
            "Batch: 2362    Epoch[3/3]    Training Loss: 0.868583619594574\n",
            "Batch: 2363    Epoch[3/3]    Training Loss: 0.7764809131622314\n",
            "Batch: 2364    Epoch[3/3]    Training Loss: 0.39052635431289673\n",
            "Batch: 2365    Epoch[3/3]    Training Loss: 0.5271973609924316\n",
            "Batch: 2366    Epoch[3/3]    Training Loss: 0.6193583011627197\n",
            "Batch: 2367    Epoch[3/3]    Training Loss: 0.8592246174812317\n",
            "Batch: 2368    Epoch[3/3]    Training Loss: 0.47168317437171936\n",
            "Batch: 2369    Epoch[3/3]    Training Loss: 1.0197675228118896\n",
            "Batch: 2370    Epoch[3/3]    Training Loss: 0.709097683429718\n",
            "Batch: 2371    Epoch[3/3]    Training Loss: 1.2208014726638794\n",
            "Batch: 2372    Epoch[3/3]    Training Loss: 0.6017873287200928\n",
            "Batch: 2373    Epoch[3/3]    Training Loss: 0.7636125683784485\n",
            "Batch: 2374    Epoch[3/3]    Training Loss: 0.41528207063674927\n",
            "Batch: 2375    Epoch[3/3]    Training Loss: 0.9253814816474915\n",
            "Batch: 2376    Epoch[3/3]    Training Loss: 1.0994162559509277\n",
            "Batch: 2377    Epoch[3/3]    Training Loss: 0.4160795211791992\n",
            "Batch: 2378    Epoch[3/3]    Training Loss: 0.898571252822876\n",
            "Batch: 2379    Epoch[3/3]    Training Loss: 0.8917376399040222\n",
            "Batch: 2380    Epoch[3/3]    Training Loss: 0.3217029571533203\n",
            "Batch: 2381    Epoch[3/3]    Training Loss: 0.41316133737564087\n",
            "Batch: 2382    Epoch[3/3]    Training Loss: 0.5856839418411255\n",
            "Batch: 2383    Epoch[3/3]    Training Loss: 0.557521402835846\n",
            "Batch: 2384    Epoch[3/3]    Training Loss: 0.26736289262771606\n",
            "Batch: 2385    Epoch[3/3]    Training Loss: 1.1786177158355713\n",
            "Batch: 2386    Epoch[3/3]    Training Loss: 1.2217512130737305\n",
            "Batch: 2387    Epoch[3/3]    Training Loss: 1.1607661247253418\n",
            "Batch: 2388    Epoch[3/3]    Training Loss: 0.6044620275497437\n",
            "Batch: 2389    Epoch[3/3]    Training Loss: 0.39130741357803345\n",
            "Batch: 2390    Epoch[3/3]    Training Loss: 0.5471984148025513\n",
            "Batch: 2391    Epoch[3/3]    Training Loss: 1.0695221424102783\n",
            "Batch: 2392    Epoch[3/3]    Training Loss: 0.34130382537841797\n",
            "Batch: 2393    Epoch[3/3]    Training Loss: 0.6909115314483643\n",
            "Batch: 2394    Epoch[3/3]    Training Loss: 0.6562971472740173\n",
            "Batch: 2395    Epoch[3/3]    Training Loss: 0.43917104601860046\n",
            "Batch: 2396    Epoch[3/3]    Training Loss: 0.48893439769744873\n",
            "Batch: 2397    Epoch[3/3]    Training Loss: 0.9611054062843323\n",
            "Batch: 2398    Epoch[3/3]    Training Loss: 0.5960042476654053\n",
            "Batch: 2399    Epoch[3/3]    Training Loss: 0.3690550625324249\n",
            "Batch: 2400    Epoch[3/3]    Training Loss: 0.9684926271438599\n",
            "Batch: 2401    Epoch[3/3]    Training Loss: 1.295372486114502\n",
            "Batch: 2402    Epoch[3/3]    Training Loss: 0.9869331121444702\n",
            "Batch: 2403    Epoch[3/3]    Training Loss: 0.33138394355773926\n",
            "Batch: 2404    Epoch[3/3]    Training Loss: 0.3601384162902832\n",
            "Batch: 2405    Epoch[3/3]    Training Loss: 0.7248635292053223\n",
            "Batch: 2406    Epoch[3/3]    Training Loss: 0.5216192007064819\n",
            "Batch: 2407    Epoch[3/3]    Training Loss: 0.4422563910484314\n",
            "Batch: 2408    Epoch[3/3]    Training Loss: 0.4815412759780884\n",
            "Batch: 2409    Epoch[3/3]    Training Loss: 0.8501627445220947\n",
            "Batch: 2410    Epoch[3/3]    Training Loss: 0.34592941403388977\n",
            "Batch: 2411    Epoch[3/3]    Training Loss: 0.719596803188324\n",
            "Batch: 2412    Epoch[3/3]    Training Loss: 0.8967350125312805\n",
            "Batch: 2413    Epoch[3/3]    Training Loss: 0.7480807304382324\n",
            "Batch: 2414    Epoch[3/3]    Training Loss: 0.5640478134155273\n",
            "Batch: 2415    Epoch[3/3]    Training Loss: 0.3870471119880676\n",
            "Batch: 2416    Epoch[3/3]    Training Loss: 1.449753999710083\n",
            "Batch: 2417    Epoch[3/3]    Training Loss: 0.5320870876312256\n",
            "Batch: 2418    Epoch[3/3]    Training Loss: 0.38909730315208435\n",
            "Batch: 2419    Epoch[3/3]    Training Loss: 0.49486812949180603\n",
            "Batch: 2420    Epoch[3/3]    Training Loss: 0.39376217126846313\n",
            "Batch: 2421    Epoch[3/3]    Training Loss: 0.3822280764579773\n",
            "Batch: 2422    Epoch[3/3]    Training Loss: 0.25421079993247986\n",
            "Batch: 2423    Epoch[3/3]    Training Loss: 0.32328101992607117\n",
            "Batch: 2424    Epoch[3/3]    Training Loss: 0.853783369064331\n",
            "Batch: 2425    Epoch[3/3]    Training Loss: 0.2894649803638458\n",
            "Batch: 2426    Epoch[3/3]    Training Loss: 0.39688026905059814\n",
            "Batch: 2427    Epoch[3/3]    Training Loss: 0.6331871151924133\n",
            "Batch: 2428    Epoch[3/3]    Training Loss: 0.5461124181747437\n",
            "Batch: 2429    Epoch[3/3]    Training Loss: 0.8888618350028992\n",
            "Batch: 2430    Epoch[3/3]    Training Loss: 1.194623351097107\n",
            "Batch: 2431    Epoch[3/3]    Training Loss: 0.18918177485466003\n",
            "Batch: 2432    Epoch[3/3]    Training Loss: 0.8173713684082031\n",
            "Batch: 2433    Epoch[3/3]    Training Loss: 1.0293413400650024\n",
            "Batch: 2434    Epoch[3/3]    Training Loss: 0.42443275451660156\n",
            "Batch: 2435    Epoch[3/3]    Training Loss: 0.4254027307033539\n",
            "Batch: 2436    Epoch[3/3]    Training Loss: 0.9132484197616577\n",
            "Batch: 2437    Epoch[3/3]    Training Loss: 0.40872517228126526\n",
            "Batch: 2438    Epoch[3/3]    Training Loss: 0.5133597254753113\n",
            "Batch: 2439    Epoch[3/3]    Training Loss: 0.6497379541397095\n",
            "Batch: 2440    Epoch[3/3]    Training Loss: 0.27950000762939453\n",
            "Batch: 2441    Epoch[3/3]    Training Loss: 0.9116543531417847\n",
            "Batch: 2442    Epoch[3/3]    Training Loss: 0.26558932662010193\n",
            "Batch: 2443    Epoch[3/3]    Training Loss: 0.35488876700401306\n",
            "Batch: 2444    Epoch[3/3]    Training Loss: 0.6066664457321167\n",
            "Batch: 2445    Epoch[3/3]    Training Loss: 0.8240020275115967\n",
            "Batch: 2446    Epoch[3/3]    Training Loss: 0.3895055651664734\n",
            "Batch: 2447    Epoch[3/3]    Training Loss: 0.6074784994125366\n",
            "Batch: 2448    Epoch[3/3]    Training Loss: 0.5982640981674194\n",
            "Batch: 2449    Epoch[3/3]    Training Loss: 0.8751891851425171\n",
            "Batch: 2450    Epoch[3/3]    Training Loss: 0.5249112844467163\n",
            "Batch: 2451    Epoch[3/3]    Training Loss: 0.5276604294776917\n",
            "Batch: 2452    Epoch[3/3]    Training Loss: 0.9190150499343872\n",
            "Batch: 2453    Epoch[3/3]    Training Loss: 0.44699549674987793\n",
            "Batch: 2454    Epoch[3/3]    Training Loss: 0.42934417724609375\n",
            "Batch: 2455    Epoch[3/3]    Training Loss: 0.21007245779037476\n",
            "Batch: 2456    Epoch[3/3]    Training Loss: 0.4607733488082886\n",
            "Batch: 2457    Epoch[3/3]    Training Loss: 0.9013345241546631\n",
            "Batch: 2458    Epoch[3/3]    Training Loss: 0.6452621221542358\n",
            "Batch: 2459    Epoch[3/3]    Training Loss: 0.5851632952690125\n",
            "Batch: 2460    Epoch[3/3]    Training Loss: 0.5952466726303101\n",
            "Batch: 2461    Epoch[3/3]    Training Loss: 0.42840489745140076\n",
            "Batch: 2462    Epoch[3/3]    Training Loss: 0.5551850199699402\n",
            "Batch: 2463    Epoch[3/3]    Training Loss: 0.37817639112472534\n",
            "Batch: 2464    Epoch[3/3]    Training Loss: 0.777813196182251\n",
            "Batch: 2465    Epoch[3/3]    Training Loss: 0.9483925104141235\n",
            "Batch: 2466    Epoch[3/3]    Training Loss: 0.35263723134994507\n",
            "Batch: 2467    Epoch[3/3]    Training Loss: 0.9450724720954895\n",
            "Batch: 2468    Epoch[3/3]    Training Loss: 0.3443814516067505\n",
            "Batch: 2469    Epoch[3/3]    Training Loss: 0.547939658164978\n",
            "Batch: 2470    Epoch[3/3]    Training Loss: 0.734597384929657\n",
            "Batch: 2471    Epoch[3/3]    Training Loss: 0.3477776050567627\n",
            "Batch: 2472    Epoch[3/3]    Training Loss: 0.33412450551986694\n",
            "Batch: 2473    Epoch[3/3]    Training Loss: 0.7684612274169922\n",
            "Batch: 2474    Epoch[3/3]    Training Loss: 0.6598103046417236\n",
            "Batch: 2475    Epoch[3/3]    Training Loss: 0.5490527153015137\n",
            "Batch: 2476    Epoch[3/3]    Training Loss: 0.8004060983657837\n",
            "Batch: 2477    Epoch[3/3]    Training Loss: 0.8133822679519653\n",
            "Batch: 2478    Epoch[3/3]    Training Loss: 0.18417298793792725\n",
            "Batch: 2479    Epoch[3/3]    Training Loss: 0.30122339725494385\n",
            "Batch: 2480    Epoch[3/3]    Training Loss: 0.9127246141433716\n",
            "Batch: 2481    Epoch[3/3]    Training Loss: 0.5984380841255188\n",
            "Batch: 2482    Epoch[3/3]    Training Loss: 0.9950002431869507\n",
            "Batch: 2483    Epoch[3/3]    Training Loss: 0.5881001949310303\n",
            "Batch: 2484    Epoch[3/3]    Training Loss: 0.7837753295898438\n",
            "Batch: 2485    Epoch[3/3]    Training Loss: 0.6026953458786011\n",
            "Batch: 2486    Epoch[3/3]    Training Loss: 0.3185406029224396\n",
            "Batch: 2487    Epoch[3/3]    Training Loss: 0.3222552537918091\n",
            "Batch: 2488    Epoch[3/3]    Training Loss: 1.2018640041351318\n",
            "Batch: 2489    Epoch[3/3]    Training Loss: 0.6961891651153564\n",
            "Batch: 2490    Epoch[3/3]    Training Loss: 0.6461892127990723\n",
            "Batch: 2491    Epoch[3/3]    Training Loss: 0.485370397567749\n",
            "Batch: 2492    Epoch[3/3]    Training Loss: 0.23623035848140717\n",
            "Batch: 2493    Epoch[3/3]    Training Loss: 0.40443065762519836\n",
            "Batch: 2494    Epoch[3/3]    Training Loss: 0.44379356503486633\n",
            "Batch: 2495    Epoch[3/3]    Training Loss: 0.6078371405601501\n",
            "Batch: 2496    Epoch[3/3]    Training Loss: 0.26878562569618225\n",
            "Batch: 2497    Epoch[3/3]    Training Loss: 0.4799983501434326\n",
            "Batch: 2498    Epoch[3/3]    Training Loss: 0.5017406940460205\n",
            "Batch: 2499    Epoch[3/3]    Training Loss: 0.3087303340435028\n",
            "Batch: 2500    Epoch[3/3]    Training Loss: 0.2914630174636841\n",
            "Batch: 2501    Epoch[3/3]    Training Loss: 0.31046029925346375\n",
            "Batch: 2502    Epoch[3/3]    Training Loss: 0.5342559218406677\n",
            "Batch: 2503    Epoch[3/3]    Training Loss: 0.7524306774139404\n",
            "Batch: 2504    Epoch[3/3]    Training Loss: 0.393929123878479\n",
            "Batch: 2505    Epoch[3/3]    Training Loss: 0.5822939276695251\n",
            "Batch: 2506    Epoch[3/3]    Training Loss: 0.3986051082611084\n",
            "Batch: 2507    Epoch[3/3]    Training Loss: 0.472167432308197\n",
            "Batch: 2508    Epoch[3/3]    Training Loss: 1.8532675504684448\n",
            "Batch: 2509    Epoch[3/3]    Training Loss: 1.016434907913208\n",
            "Batch: 2510    Epoch[3/3]    Training Loss: 0.6203910112380981\n",
            "Batch: 2511    Epoch[3/3]    Training Loss: 1.0260881185531616\n",
            "Batch: 2512    Epoch[3/3]    Training Loss: 0.9639051556587219\n",
            "Batch: 2513    Epoch[3/3]    Training Loss: 0.6789060831069946\n",
            "Batch: 2514    Epoch[3/3]    Training Loss: 0.51929771900177\n",
            "Batch: 2515    Epoch[3/3]    Training Loss: 0.6363621950149536\n",
            "Batch: 2516    Epoch[3/3]    Training Loss: 0.8380929231643677\n",
            "Batch: 2517    Epoch[3/3]    Training Loss: 0.526069164276123\n",
            "Batch: 2518    Epoch[3/3]    Training Loss: 0.2313535064458847\n",
            "Batch: 2519    Epoch[3/3]    Training Loss: 0.6967995762825012\n",
            "Batch: 2520    Epoch[3/3]    Training Loss: 0.2744089663028717\n",
            "Batch: 2521    Epoch[3/3]    Training Loss: 0.41292762756347656\n",
            "Batch: 2522    Epoch[3/3]    Training Loss: 0.9500634670257568\n",
            "Batch: 2523    Epoch[3/3]    Training Loss: 0.47788286209106445\n",
            "Batch: 2524    Epoch[3/3]    Training Loss: 0.283171147108078\n",
            "Batch: 2525    Epoch[3/3]    Training Loss: 0.41709235310554504\n",
            "Batch: 2526    Epoch[3/3]    Training Loss: 0.520936131477356\n",
            "Batch: 2527    Epoch[3/3]    Training Loss: 0.8354427814483643\n",
            "Batch: 2528    Epoch[3/3]    Training Loss: 1.3394051790237427\n",
            "Batch: 2529    Epoch[3/3]    Training Loss: 0.4840732216835022\n",
            "Batch: 2530    Epoch[3/3]    Training Loss: 0.2964290380477905\n",
            "Batch: 2531    Epoch[3/3]    Training Loss: 0.3986036479473114\n",
            "Batch: 2532    Epoch[3/3]    Training Loss: 0.4026947021484375\n",
            "Batch: 2533    Epoch[3/3]    Training Loss: 0.5986677408218384\n",
            "Batch: 2534    Epoch[3/3]    Training Loss: 0.7611573934555054\n",
            "Batch: 2535    Epoch[3/3]    Training Loss: 0.37888550758361816\n",
            "Batch: 2536    Epoch[3/3]    Training Loss: 0.48015648126602173\n",
            "Batch: 2537    Epoch[3/3]    Training Loss: 0.599628210067749\n",
            "Batch: 2538    Epoch[3/3]    Training Loss: 0.958293080329895\n",
            "Batch: 2539    Epoch[3/3]    Training Loss: 0.6424515247344971\n",
            "Batch: 2540    Epoch[3/3]    Training Loss: 0.5276427865028381\n",
            "Batch: 2541    Epoch[3/3]    Training Loss: 0.4160494804382324\n",
            "Batch: 2542    Epoch[3/3]    Training Loss: 0.3963538110256195\n",
            "Batch: 2543    Epoch[3/3]    Training Loss: 0.34410718083381653\n",
            "Batch: 2544    Epoch[3/3]    Training Loss: 0.47473257780075073\n",
            "Batch: 2545    Epoch[3/3]    Training Loss: 0.5403118133544922\n",
            "Batch: 2546    Epoch[3/3]    Training Loss: 0.7529637217521667\n",
            "Batch: 2547    Epoch[3/3]    Training Loss: 0.9498903751373291\n",
            "Batch: 2548    Epoch[3/3]    Training Loss: 0.3911806046962738\n",
            "Batch: 2549    Epoch[3/3]    Training Loss: 0.6263198852539062\n",
            "Batch: 2550    Epoch[3/3]    Training Loss: 0.6662085652351379\n",
            "Batch: 2551    Epoch[3/3]    Training Loss: 1.3136777877807617\n",
            "Batch: 2552    Epoch[3/3]    Training Loss: 0.36070606112480164\n",
            "Batch: 2553    Epoch[3/3]    Training Loss: 0.6990979909896851\n",
            "Batch: 2554    Epoch[3/3]    Training Loss: 0.5399733781814575\n",
            "Batch: 2555    Epoch[3/3]    Training Loss: 0.5271764993667603\n",
            "Batch: 2556    Epoch[3/3]    Training Loss: 0.5119672417640686\n",
            "Batch: 2557    Epoch[3/3]    Training Loss: 0.5984467267990112\n",
            "Batch: 2558    Epoch[3/3]    Training Loss: 0.40432974696159363\n",
            "Batch: 2559    Epoch[3/3]    Training Loss: 0.5149514675140381\n",
            "Batch: 2560    Epoch[3/3]    Training Loss: 0.9820549488067627\n",
            "Batch: 2561    Epoch[3/3]    Training Loss: 0.7223929762840271\n",
            "Batch: 2562    Epoch[3/3]    Training Loss: 0.5683695077896118\n",
            "Batch: 2563    Epoch[3/3]    Training Loss: 0.849858283996582\n",
            "Batch: 2564    Epoch[3/3]    Training Loss: 0.5814551115036011\n",
            "Batch: 2565    Epoch[3/3]    Training Loss: 0.33452361822128296\n",
            "Batch: 2566    Epoch[3/3]    Training Loss: 0.44149714708328247\n",
            "Batch: 2567    Epoch[3/3]    Training Loss: 0.5066514611244202\n",
            "Batch: 2568    Epoch[3/3]    Training Loss: 0.6874064207077026\n",
            "Batch: 2569    Epoch[3/3]    Training Loss: 0.8441718220710754\n",
            "Batch: 2570    Epoch[3/3]    Training Loss: 0.4395398199558258\n",
            "Batch: 2571    Epoch[3/3]    Training Loss: 0.6719663143157959\n",
            "Batch: 2572    Epoch[3/3]    Training Loss: 0.4400426149368286\n",
            "Batch: 2573    Epoch[3/3]    Training Loss: 0.5028895735740662\n",
            "Batch: 2574    Epoch[3/3]    Training Loss: 0.8282197713851929\n",
            "Batch: 2575    Epoch[3/3]    Training Loss: 0.6096758246421814\n",
            "Batch: 2576    Epoch[3/3]    Training Loss: 0.5389565825462341\n",
            "Batch: 2577    Epoch[3/3]    Training Loss: 0.8035851716995239\n",
            "Batch: 2578    Epoch[3/3]    Training Loss: 0.8352823257446289\n",
            "Batch: 2579    Epoch[3/3]    Training Loss: 0.5064260959625244\n",
            "Batch: 2580    Epoch[3/3]    Training Loss: 0.3663870096206665\n",
            "Batch: 2581    Epoch[3/3]    Training Loss: 0.41098761558532715\n",
            "Batch: 2582    Epoch[3/3]    Training Loss: 0.5956311225891113\n",
            "Batch: 2583    Epoch[3/3]    Training Loss: 0.5691589117050171\n",
            "Batch: 2584    Epoch[3/3]    Training Loss: 0.6972500085830688\n",
            "Batch: 2585    Epoch[3/3]    Training Loss: 0.17466334998607635\n",
            "Batch: 2586    Epoch[3/3]    Training Loss: 0.5999206900596619\n",
            "Batch: 2587    Epoch[3/3]    Training Loss: 0.4705933928489685\n",
            "Batch: 2588    Epoch[3/3]    Training Loss: 0.6569266319274902\n",
            "Batch: 2589    Epoch[3/3]    Training Loss: 0.6296095848083496\n",
            "Batch: 2590    Epoch[3/3]    Training Loss: 0.6644147634506226\n",
            "Batch: 2591    Epoch[3/3]    Training Loss: 0.3671390414237976\n",
            "Batch: 2592    Epoch[3/3]    Training Loss: 0.33881860971450806\n",
            "Batch: 2593    Epoch[3/3]    Training Loss: 0.2963579297065735\n",
            "Batch: 2594    Epoch[3/3]    Training Loss: 0.6732770204544067\n",
            "Batch: 2595    Epoch[3/3]    Training Loss: 0.6112856864929199\n",
            "Batch: 2596    Epoch[3/3]    Training Loss: 0.6128264665603638\n",
            "Batch: 2597    Epoch[3/3]    Training Loss: 0.8198598623275757\n",
            "Batch: 2598    Epoch[3/3]    Training Loss: 0.40433168411254883\n",
            "Batch: 2599    Epoch[3/3]    Training Loss: 0.5355713367462158\n",
            "Batch: 2600    Epoch[3/3]    Training Loss: 0.5862627029418945\n",
            "Batch: 2601    Epoch[3/3]    Training Loss: 0.6259215474128723\n",
            "Batch: 2602    Epoch[3/3]    Training Loss: 0.40577322244644165\n",
            "Batch: 2603    Epoch[3/3]    Training Loss: 1.5031981468200684\n",
            "Batch: 2604    Epoch[3/3]    Training Loss: 0.8295883536338806\n",
            "Batch: 2605    Epoch[3/3]    Training Loss: 0.29765287041664124\n",
            "Batch: 2606    Epoch[3/3]    Training Loss: 0.5580933094024658\n",
            "Batch: 2607    Epoch[3/3]    Training Loss: 0.23687992990016937\n",
            "Batch: 2608    Epoch[3/3]    Training Loss: 0.4718751311302185\n",
            "Batch: 2609    Epoch[3/3]    Training Loss: 0.5641949772834778\n",
            "Batch: 2610    Epoch[3/3]    Training Loss: 0.5709831118583679\n",
            "Batch: 2611    Epoch[3/3]    Training Loss: 0.7395972013473511\n",
            "Batch: 2612    Epoch[3/3]    Training Loss: 0.8266780376434326\n",
            "Batch: 2613    Epoch[3/3]    Training Loss: 0.763516902923584\n",
            "Batch: 2614    Epoch[3/3]    Training Loss: 0.2876056730747223\n",
            "Batch: 2615    Epoch[3/3]    Training Loss: 0.7705994844436646\n",
            "Batch: 2616    Epoch[3/3]    Training Loss: 0.706580638885498\n",
            "Batch: 2617    Epoch[3/3]    Training Loss: 0.55364990234375\n",
            "Batch: 2618    Epoch[3/3]    Training Loss: 0.257339209318161\n",
            "Batch: 2619    Epoch[3/3]    Training Loss: 0.39896827936172485\n",
            "Batch: 2620    Epoch[3/3]    Training Loss: 0.4426916241645813\n",
            "Batch: 2621    Epoch[3/3]    Training Loss: 0.6704036593437195\n",
            "Batch: 2622    Epoch[3/3]    Training Loss: 0.4442889392375946\n",
            "Batch: 2623    Epoch[3/3]    Training Loss: 0.8949808478355408\n",
            "Batch: 2624    Epoch[3/3]    Training Loss: 0.6053493022918701\n",
            "Batch: 2625    Epoch[3/3]    Training Loss: 0.33983826637268066\n",
            "Batch: 2626    Epoch[3/3]    Training Loss: 0.8737470507621765\n",
            "Batch: 2627    Epoch[3/3]    Training Loss: 0.36818796396255493\n",
            "Batch: 2628    Epoch[3/3]    Training Loss: 1.0680550336837769\n",
            "Batch: 2629    Epoch[3/3]    Training Loss: 0.7063179612159729\n",
            "Batch: 2630    Epoch[3/3]    Training Loss: 0.46946030855178833\n",
            "Batch: 2631    Epoch[3/3]    Training Loss: 0.4466661214828491\n",
            "Batch: 2632    Epoch[3/3]    Training Loss: 0.6003375053405762\n",
            "Batch: 2633    Epoch[3/3]    Training Loss: 1.1307497024536133\n",
            "Batch: 2634    Epoch[3/3]    Training Loss: 0.6563931703567505\n",
            "Batch: 2635    Epoch[3/3]    Training Loss: 0.8190116882324219\n",
            "Batch: 2636    Epoch[3/3]    Training Loss: 0.366403192281723\n",
            "Batch: 2637    Epoch[3/3]    Training Loss: 0.4454856812953949\n",
            "Batch: 2638    Epoch[3/3]    Training Loss: 0.529022216796875\n",
            "Batch: 2639    Epoch[3/3]    Training Loss: 0.27960604429244995\n",
            "Batch: 2640    Epoch[3/3]    Training Loss: 1.6038117408752441\n",
            "Batch: 2641    Epoch[3/3]    Training Loss: 0.7432740926742554\n",
            "Batch: 2642    Epoch[3/3]    Training Loss: 0.5077016353607178\n",
            "Batch: 2643    Epoch[3/3]    Training Loss: 0.7864742279052734\n",
            "Batch: 2644    Epoch[3/3]    Training Loss: 0.4177319407463074\n",
            "Batch: 2645    Epoch[3/3]    Training Loss: 0.5344998836517334\n",
            "Batch: 2646    Epoch[3/3]    Training Loss: 1.129380702972412\n",
            "Batch: 2647    Epoch[3/3]    Training Loss: 0.6812067031860352\n",
            "Batch: 2648    Epoch[3/3]    Training Loss: 0.45969343185424805\n",
            "Batch: 2649    Epoch[3/3]    Training Loss: 1.0232023000717163\n",
            "Batch: 2650    Epoch[3/3]    Training Loss: 0.6318148970603943\n",
            "Batch: 2651    Epoch[3/3]    Training Loss: 0.7283427715301514\n",
            "Batch: 2652    Epoch[3/3]    Training Loss: 0.5873548984527588\n",
            "Batch: 2653    Epoch[3/3]    Training Loss: 0.8475028276443481\n",
            "Batch: 2654    Epoch[3/3]    Training Loss: 0.6974001526832581\n",
            "Batch: 2655    Epoch[3/3]    Training Loss: 0.3902132511138916\n",
            "Batch: 2656    Epoch[3/3]    Training Loss: 0.4147948622703552\n",
            "Batch: 2657    Epoch[3/3]    Training Loss: 1.4697911739349365\n",
            "Batch: 2658    Epoch[3/3]    Training Loss: 0.40104758739471436\n",
            "Batch: 2659    Epoch[3/3]    Training Loss: 0.8492538928985596\n",
            "Batch: 2660    Epoch[3/3]    Training Loss: 0.5599385499954224\n",
            "Batch: 2661    Epoch[3/3]    Training Loss: 0.31037840247154236\n",
            "Batch: 2662    Epoch[3/3]    Training Loss: 0.6403025388717651\n",
            "Batch: 2663    Epoch[3/3]    Training Loss: 0.8187693357467651\n",
            "Batch: 2664    Epoch[3/3]    Training Loss: 0.7734657526016235\n",
            "Batch: 2665    Epoch[3/3]    Training Loss: 0.6129612326622009\n",
            "Batch: 2666    Epoch[3/3]    Training Loss: 0.3063892424106598\n",
            "Batch: 2667    Epoch[3/3]    Training Loss: 0.5863296389579773\n",
            "Batch: 2668    Epoch[3/3]    Training Loss: 0.84521484375\n",
            "Batch: 2669    Epoch[3/3]    Training Loss: 0.636294424533844\n",
            "Batch: 2670    Epoch[3/3]    Training Loss: 0.459053099155426\n",
            "Batch: 2671    Epoch[3/3]    Training Loss: 0.3501468598842621\n",
            "Batch: 2672    Epoch[3/3]    Training Loss: 0.7437813878059387\n",
            "Batch: 2673    Epoch[3/3]    Training Loss: 0.35303443670272827\n",
            "Batch: 2674    Epoch[3/3]    Training Loss: 0.3071356415748596\n",
            "Batch: 2675    Epoch[3/3]    Training Loss: 0.5093269348144531\n",
            "Batch: 2676    Epoch[3/3]    Training Loss: 0.4341270923614502\n",
            "Batch: 2677    Epoch[3/3]    Training Loss: 0.4467127323150635\n",
            "Batch: 2678    Epoch[3/3]    Training Loss: 0.3921232223510742\n",
            "Batch: 2679    Epoch[3/3]    Training Loss: 0.6298615336418152\n",
            "Batch: 2680    Epoch[3/3]    Training Loss: 0.6016190648078918\n",
            "Batch: 2681    Epoch[3/3]    Training Loss: 0.3666404187679291\n",
            "Batch: 2682    Epoch[3/3]    Training Loss: 0.8077019453048706\n",
            "Batch: 2683    Epoch[3/3]    Training Loss: 0.3955457806587219\n",
            "Batch: 2684    Epoch[3/3]    Training Loss: 0.44999122619628906\n",
            "Batch: 2685    Epoch[3/3]    Training Loss: 1.2373692989349365\n",
            "Batch: 2686    Epoch[3/3]    Training Loss: 0.3685620427131653\n",
            "Batch: 2687    Epoch[3/3]    Training Loss: 0.3838426172733307\n",
            "Batch: 2688    Epoch[3/3]    Training Loss: 0.3413124084472656\n",
            "Batch: 2689    Epoch[3/3]    Training Loss: 0.21015217900276184\n",
            "Batch: 2690    Epoch[3/3]    Training Loss: 0.26953667402267456\n",
            "Batch: 2691    Epoch[3/3]    Training Loss: 0.5356963276863098\n",
            "Batch: 2692    Epoch[3/3]    Training Loss: 0.37862443923950195\n",
            "Batch: 2693    Epoch[3/3]    Training Loss: 0.5869446992874146\n",
            "Batch: 2694    Epoch[3/3]    Training Loss: 0.47493499517440796\n",
            "Batch: 2695    Epoch[3/3]    Training Loss: 0.3032890558242798\n",
            "Batch: 2696    Epoch[3/3]    Training Loss: 0.7841002941131592\n",
            "Batch: 2697    Epoch[3/3]    Training Loss: 1.2968149185180664\n",
            "Batch: 2698    Epoch[3/3]    Training Loss: 0.48511379957199097\n",
            "Batch: 2699    Epoch[3/3]    Training Loss: 0.6065724492073059\n",
            "Batch: 2700    Epoch[3/3]    Training Loss: 0.35925066471099854\n",
            "Batch: 2701    Epoch[3/3]    Training Loss: 0.7689794301986694\n",
            "Batch: 2702    Epoch[3/3]    Training Loss: 0.6794263124465942\n",
            "Batch: 2703    Epoch[3/3]    Training Loss: 0.5526148080825806\n",
            "Batch: 2704    Epoch[3/3]    Training Loss: 0.8576251864433289\n",
            "Batch: 2705    Epoch[3/3]    Training Loss: 0.8866796493530273\n",
            "Batch: 2706    Epoch[3/3]    Training Loss: 0.27727779746055603\n",
            "Batch: 2707    Epoch[3/3]    Training Loss: 0.4407847225666046\n",
            "Batch: 2708    Epoch[3/3]    Training Loss: 0.643775224685669\n",
            "Batch: 2709    Epoch[3/3]    Training Loss: 1.1327508687973022\n",
            "Batch: 2710    Epoch[3/3]    Training Loss: 0.4866805672645569\n",
            "Batch: 2711    Epoch[3/3]    Training Loss: 0.5025478005409241\n",
            "Batch: 2712    Epoch[3/3]    Training Loss: 0.48951032757759094\n",
            "Batch: 2713    Epoch[3/3]    Training Loss: 0.26221030950546265\n",
            "Batch: 2714    Epoch[3/3]    Training Loss: 0.3570977747440338\n",
            "Batch: 2715    Epoch[3/3]    Training Loss: 0.30608946084976196\n",
            "Batch: 2716    Epoch[3/3]    Training Loss: 0.2902102470397949\n",
            "Batch: 2717    Epoch[3/3]    Training Loss: 0.5105981826782227\n",
            "Batch: 2718    Epoch[3/3]    Training Loss: 0.6463798880577087\n",
            "Batch: 2719    Epoch[3/3]    Training Loss: 1.1727838516235352\n",
            "Batch: 2720    Epoch[3/3]    Training Loss: 1.0856873989105225\n",
            "Batch: 2721    Epoch[3/3]    Training Loss: 0.5644799470901489\n",
            "Batch: 2722    Epoch[3/3]    Training Loss: 0.28447413444519043\n",
            "Batch: 2723    Epoch[3/3]    Training Loss: 0.46083390712738037\n",
            "Batch: 2724    Epoch[3/3]    Training Loss: 0.5308581590652466\n",
            "Batch: 2725    Epoch[3/3]    Training Loss: 0.3905484080314636\n",
            "Batch: 2726    Epoch[3/3]    Training Loss: 1.0084205865859985\n",
            "Batch: 2727    Epoch[3/3]    Training Loss: 0.408099889755249\n",
            "Batch: 2728    Epoch[3/3]    Training Loss: 0.6297681331634521\n",
            "Batch: 2729    Epoch[3/3]    Training Loss: 0.8631259202957153\n",
            "Batch: 2730    Epoch[3/3]    Training Loss: 0.464962363243103\n",
            "Batch: 2731    Epoch[3/3]    Training Loss: 0.6034820079803467\n",
            "Batch: 2732    Epoch[3/3]    Training Loss: 0.5059285163879395\n",
            "Batch: 2733    Epoch[3/3]    Training Loss: 0.6861355304718018\n",
            "Batch: 2734    Epoch[3/3]    Training Loss: 0.3544398546218872\n",
            "Batch: 2735    Epoch[3/3]    Training Loss: 0.4868549704551697\n",
            "Batch: 2736    Epoch[3/3]    Training Loss: 0.37544432282447815\n",
            "Batch: 2737    Epoch[3/3]    Training Loss: 0.9915180206298828\n",
            "Batch: 2738    Epoch[3/3]    Training Loss: 0.5381330251693726\n",
            "Batch: 2739    Epoch[3/3]    Training Loss: 0.5908923149108887\n",
            "Batch: 2740    Epoch[3/3]    Training Loss: 0.5984510779380798\n",
            "Batch: 2741    Epoch[3/3]    Training Loss: 0.9199240207672119\n",
            "Batch: 2742    Epoch[3/3]    Training Loss: 0.6090867519378662\n",
            "Batch: 2743    Epoch[3/3]    Training Loss: 0.7335619926452637\n",
            "Batch: 2744    Epoch[3/3]    Training Loss: 0.7336970567703247\n",
            "Batch: 2745    Epoch[3/3]    Training Loss: 0.7979212999343872\n",
            "Batch: 2746    Epoch[3/3]    Training Loss: 0.5013154745101929\n",
            "Batch: 2747    Epoch[3/3]    Training Loss: 0.8764838576316833\n",
            "Batch: 2748    Epoch[3/3]    Training Loss: 1.0506131649017334\n",
            "Batch: 2749    Epoch[3/3]    Training Loss: 1.3192572593688965\n",
            "Batch: 2750    Epoch[3/3]    Training Loss: 0.5692518949508667\n",
            "Batch: 2751    Epoch[3/3]    Training Loss: 0.503781795501709\n",
            "Batch: 2752    Epoch[3/3]    Training Loss: 0.4149148464202881\n",
            "Batch: 2753    Epoch[3/3]    Training Loss: 0.6108055710792542\n",
            "Batch: 2754    Epoch[3/3]    Training Loss: 0.6788927316665649\n",
            "Batch: 2755    Epoch[3/3]    Training Loss: 0.8517236113548279\n",
            "Batch: 2756    Epoch[3/3]    Training Loss: 0.8850551843643188\n",
            "Batch: 2757    Epoch[3/3]    Training Loss: 0.7329539060592651\n",
            "Batch: 2758    Epoch[3/3]    Training Loss: 0.4865747094154358\n",
            "Batch: 2759    Epoch[3/3]    Training Loss: 0.7914860248565674\n",
            "Batch: 2760    Epoch[3/3]    Training Loss: 0.41147229075431824\n",
            "Batch: 2761    Epoch[3/3]    Training Loss: 0.5409331321716309\n",
            "Batch: 2762    Epoch[3/3]    Training Loss: 0.8171384930610657\n",
            "Batch: 2763    Epoch[3/3]    Training Loss: 0.3594086170196533\n",
            "Batch: 2764    Epoch[3/3]    Training Loss: 0.5832346081733704\n",
            "Batch: 2765    Epoch[3/3]    Training Loss: 0.9077587127685547\n",
            "Batch: 2766    Epoch[3/3]    Training Loss: 0.666106104850769\n",
            "Batch: 2767    Epoch[3/3]    Training Loss: 0.4712848663330078\n",
            "Batch: 2768    Epoch[3/3]    Training Loss: 0.6451499462127686\n",
            "Batch: 2769    Epoch[3/3]    Training Loss: 0.6967499852180481\n",
            "Batch: 2770    Epoch[3/3]    Training Loss: 0.6400821208953857\n",
            "Batch: 2771    Epoch[3/3]    Training Loss: 0.5726562738418579\n",
            "Batch: 2772    Epoch[3/3]    Training Loss: 0.28872498869895935\n",
            "Batch: 2773    Epoch[3/3]    Training Loss: 0.5592567324638367\n",
            "Batch: 2774    Epoch[3/3]    Training Loss: 0.6428921222686768\n",
            "Batch: 2775    Epoch[3/3]    Training Loss: 0.8222427368164062\n",
            "Batch: 2776    Epoch[3/3]    Training Loss: 0.6352037191390991\n",
            "Batch: 2777    Epoch[3/3]    Training Loss: 0.497840940952301\n",
            "Batch: 2778    Epoch[3/3]    Training Loss: 0.5783739686012268\n",
            "Batch: 2779    Epoch[3/3]    Training Loss: 0.5474821329116821\n",
            "Batch: 2780    Epoch[3/3]    Training Loss: 0.8875980377197266\n",
            "Batch: 2781    Epoch[3/3]    Training Loss: 0.6381807327270508\n",
            "Batch: 2782    Epoch[3/3]    Training Loss: 0.2880208492279053\n",
            "Batch: 2783    Epoch[3/3]    Training Loss: 0.2572733759880066\n",
            "Batch: 2784    Epoch[3/3]    Training Loss: 0.8356319069862366\n",
            "Batch: 2785    Epoch[3/3]    Training Loss: 0.48611724376678467\n",
            "Batch: 2786    Epoch[3/3]    Training Loss: 0.5646436810493469\n",
            "Batch: 2787    Epoch[3/3]    Training Loss: 0.2675127387046814\n",
            "Batch: 2788    Epoch[3/3]    Training Loss: 0.5249224305152893\n",
            "Batch: 2789    Epoch[3/3]    Training Loss: 0.8970462679862976\n",
            "Batch: 2790    Epoch[3/3]    Training Loss: 0.4774875342845917\n",
            "Batch: 2791    Epoch[3/3]    Training Loss: 0.9244840145111084\n",
            "Batch: 2792    Epoch[3/3]    Training Loss: 0.38228678703308105\n",
            "Batch: 2793    Epoch[3/3]    Training Loss: 0.9819902181625366\n",
            "Batch: 2794    Epoch[3/3]    Training Loss: 0.7809518575668335\n",
            "Batch: 2795    Epoch[3/3]    Training Loss: 0.4422987997531891\n",
            "Batch: 2796    Epoch[3/3]    Training Loss: 0.7658818960189819\n",
            "Batch: 2797    Epoch[3/3]    Training Loss: 0.38417595624923706\n",
            "Batch: 2798    Epoch[3/3]    Training Loss: 0.2627906799316406\n",
            "Batch: 2799    Epoch[3/3]    Training Loss: 0.27427375316619873\n",
            "Batch: 2800    Epoch[3/3]    Training Loss: 0.8600813150405884\n",
            "Batch: 2801    Epoch[3/3]    Training Loss: 0.15271076560020447\n",
            "Batch: 2802    Epoch[3/3]    Training Loss: 0.4436837434768677\n",
            "Batch: 2803    Epoch[3/3]    Training Loss: 0.5339895486831665\n",
            "Batch: 2804    Epoch[3/3]    Training Loss: 0.349035382270813\n",
            "Batch: 2805    Epoch[3/3]    Training Loss: 0.5495181083679199\n",
            "Batch: 2806    Epoch[3/3]    Training Loss: 0.5441404581069946\n",
            "Batch: 2807    Epoch[3/3]    Training Loss: 0.9258809089660645\n",
            "Batch: 2808    Epoch[3/3]    Training Loss: 0.2288331538438797\n",
            "Batch: 2809    Epoch[3/3]    Training Loss: 0.8707853555679321\n",
            "Batch: 2810    Epoch[3/3]    Training Loss: 0.22913171350955963\n",
            "Batch: 2811    Epoch[3/3]    Training Loss: 0.7092309594154358\n",
            "Batch: 2812    Epoch[3/3]    Training Loss: 0.6932618618011475\n",
            "Batch: 2813    Epoch[3/3]    Training Loss: 0.4247666299343109\n",
            "Batch: 2814    Epoch[3/3]    Training Loss: 0.6889597177505493\n",
            "Batch: 2815    Epoch[3/3]    Training Loss: 0.8826534152030945\n",
            "Batch: 2816    Epoch[3/3]    Training Loss: 0.5803649425506592\n",
            "Batch: 2817    Epoch[3/3]    Training Loss: 0.7394496202468872\n",
            "Batch: 2818    Epoch[3/3]    Training Loss: 0.3090018332004547\n",
            "Batch: 2819    Epoch[3/3]    Training Loss: 0.32045242190361023\n",
            "Batch: 2820    Epoch[3/3]    Training Loss: 0.42493563890457153\n",
            "Batch: 2821    Epoch[3/3]    Training Loss: 0.5389083623886108\n",
            "Batch: 2822    Epoch[3/3]    Training Loss: 0.5991173982620239\n",
            "Batch: 2823    Epoch[3/3]    Training Loss: 0.9377684593200684\n",
            "Batch: 2824    Epoch[3/3]    Training Loss: 0.21221597492694855\n",
            "Batch: 2825    Epoch[3/3]    Training Loss: 0.9454096555709839\n",
            "Batch: 2826    Epoch[3/3]    Training Loss: 0.3722894787788391\n",
            "Batch: 2827    Epoch[3/3]    Training Loss: 0.4615190029144287\n",
            "Batch: 2828    Epoch[3/3]    Training Loss: 0.6537752151489258\n",
            "Batch: 2829    Epoch[3/3]    Training Loss: 0.9922864437103271\n",
            "Batch: 2830    Epoch[3/3]    Training Loss: 0.6881400346755981\n",
            "Batch: 2831    Epoch[3/3]    Training Loss: 0.24491065740585327\n",
            "Batch: 2832    Epoch[3/3]    Training Loss: 1.0461435317993164\n",
            "Batch: 2833    Epoch[3/3]    Training Loss: 0.5352720022201538\n",
            "Batch: 2834    Epoch[3/3]    Training Loss: 0.6439012289047241\n",
            "Batch: 2835    Epoch[3/3]    Training Loss: 0.4598039984703064\n",
            "Batch: 2836    Epoch[3/3]    Training Loss: 0.7949279546737671\n",
            "Batch: 2837    Epoch[3/3]    Training Loss: 0.553682804107666\n",
            "Batch: 2838    Epoch[3/3]    Training Loss: 0.6417942643165588\n",
            "Batch: 2839    Epoch[3/3]    Training Loss: 0.3766450881958008\n",
            "Batch: 2840    Epoch[3/3]    Training Loss: 0.6726484298706055\n",
            "Batch: 2841    Epoch[3/3]    Training Loss: 0.6522223949432373\n",
            "Batch: 2842    Epoch[3/3]    Training Loss: 0.6962100267410278\n",
            "Batch: 2843    Epoch[3/3]    Training Loss: 0.3580784797668457\n",
            "Batch: 2844    Epoch[3/3]    Training Loss: 0.9451631307601929\n",
            "Batch: 2845    Epoch[3/3]    Training Loss: 0.7461717128753662\n",
            "Batch: 2846    Epoch[3/3]    Training Loss: 0.8651872873306274\n",
            "Batch: 2847    Epoch[3/3]    Training Loss: 0.730217456817627\n",
            "Batch: 2848    Epoch[3/3]    Training Loss: 0.41105884313583374\n",
            "Batch: 2849    Epoch[3/3]    Training Loss: 0.9064339995384216\n",
            "Batch: 2850    Epoch[3/3]    Training Loss: 0.6117182970046997\n",
            "Batch: 2851    Epoch[3/3]    Training Loss: 0.48272252082824707\n",
            "Batch: 2852    Epoch[3/3]    Training Loss: 0.7482119798660278\n",
            "Batch: 2853    Epoch[3/3]    Training Loss: 0.7784714102745056\n",
            "Batch: 2854    Epoch[3/3]    Training Loss: 0.6892192363739014\n",
            "Batch: 2855    Epoch[3/3]    Training Loss: 0.33891797065734863\n",
            "Batch: 2856    Epoch[3/3]    Training Loss: 0.9467018842697144\n",
            "Batch: 2857    Epoch[3/3]    Training Loss: 0.4910903573036194\n",
            "Batch: 2858    Epoch[3/3]    Training Loss: 0.5731405019760132\n",
            "Batch: 2859    Epoch[3/3]    Training Loss: 0.4341510534286499\n",
            "Batch: 2860    Epoch[3/3]    Training Loss: 1.0257058143615723\n",
            "Batch: 2861    Epoch[3/3]    Training Loss: 0.6852512955665588\n",
            "Batch: 2862    Epoch[3/3]    Training Loss: 0.5220173597335815\n",
            "Batch: 2863    Epoch[3/3]    Training Loss: 0.8660483956336975\n",
            "Batch: 2864    Epoch[3/3]    Training Loss: 0.4232807755470276\n",
            "Batch: 2865    Epoch[3/3]    Training Loss: 0.5814552307128906\n",
            "Batch: 2866    Epoch[3/3]    Training Loss: 0.8138476610183716\n",
            "Batch: 2867    Epoch[3/3]    Training Loss: 0.6376386880874634\n",
            "Batch: 2868    Epoch[3/3]    Training Loss: 0.7908399105072021\n",
            "Batch: 2869    Epoch[3/3]    Training Loss: 0.6519150137901306\n",
            "Batch: 2870    Epoch[3/3]    Training Loss: 0.4562329649925232\n",
            "Batch: 2871    Epoch[3/3]    Training Loss: 0.44643789529800415\n",
            "Batch: 2872    Epoch[3/3]    Training Loss: 0.9108646512031555\n",
            "Batch: 2873    Epoch[3/3]    Training Loss: 0.8734527826309204\n",
            "Batch: 2874    Epoch[3/3]    Training Loss: 0.7090033888816833\n",
            "Batch: 2875    Epoch[3/3]    Training Loss: 0.4679402709007263\n",
            "Batch: 2876    Epoch[3/3]    Training Loss: 0.6760843992233276\n",
            "Batch: 2877    Epoch[3/3]    Training Loss: 0.503544807434082\n",
            "Batch: 2878    Epoch[3/3]    Training Loss: 0.4077521562576294\n",
            "Batch: 2879    Epoch[3/3]    Training Loss: 0.3947165012359619\n",
            "Batch: 2880    Epoch[3/3]    Training Loss: 0.3410130441188812\n",
            "Batch: 2881    Epoch[3/3]    Training Loss: 0.456126868724823\n",
            "Batch: 2882    Epoch[3/3]    Training Loss: 0.6325018405914307\n",
            "Batch: 2883    Epoch[3/3]    Training Loss: 0.4837832450866699\n",
            "Batch: 2884    Epoch[3/3]    Training Loss: 0.5552269220352173\n",
            "Batch: 2885    Epoch[3/3]    Training Loss: 0.622167706489563\n",
            "Batch: 2886    Epoch[3/3]    Training Loss: 0.3552784323692322\n",
            "Batch: 2887    Epoch[3/3]    Training Loss: 1.485779047012329\n",
            "Batch: 2888    Epoch[3/3]    Training Loss: 1.283644437789917\n",
            "Batch: 2889    Epoch[3/3]    Training Loss: 0.25134897232055664\n",
            "Batch: 2890    Epoch[3/3]    Training Loss: 0.33789896965026855\n",
            "Batch: 2891    Epoch[3/3]    Training Loss: 0.5287535190582275\n",
            "Batch: 2892    Epoch[3/3]    Training Loss: 0.5829423666000366\n",
            "Batch: 2893    Epoch[3/3]    Training Loss: 0.5067691206932068\n",
            "Batch: 2894    Epoch[3/3]    Training Loss: 0.5143433809280396\n",
            "Batch: 2895    Epoch[3/3]    Training Loss: 0.9016320705413818\n",
            "Batch: 2896    Epoch[3/3]    Training Loss: 0.36767449975013733\n",
            "Batch: 2897    Epoch[3/3]    Training Loss: 0.4791850447654724\n",
            "Batch: 2898    Epoch[3/3]    Training Loss: 1.0916128158569336\n",
            "Batch: 2899    Epoch[3/3]    Training Loss: 0.734036922454834\n",
            "Batch: 2900    Epoch[3/3]    Training Loss: 0.5848126411437988\n",
            "Batch: 2901    Epoch[3/3]    Training Loss: 0.7267668843269348\n",
            "Batch: 2902    Epoch[3/3]    Training Loss: 0.408925861120224\n",
            "Batch: 2903    Epoch[3/3]    Training Loss: 0.5078063011169434\n",
            "Batch: 2904    Epoch[3/3]    Training Loss: 0.46137771010398865\n",
            "Batch: 2905    Epoch[3/3]    Training Loss: 0.6453942060470581\n",
            "Batch: 2906    Epoch[3/3]    Training Loss: 0.28912389278411865\n",
            "Batch: 2907    Epoch[3/3]    Training Loss: 0.8428807854652405\n",
            "Batch: 2908    Epoch[3/3]    Training Loss: 1.0519599914550781\n",
            "Batch: 2909    Epoch[3/3]    Training Loss: 0.849997878074646\n",
            "Batch: 2910    Epoch[3/3]    Training Loss: 0.6651787757873535\n",
            "Batch: 2911    Epoch[3/3]    Training Loss: 1.1864757537841797\n",
            "Batch: 2912    Epoch[3/3]    Training Loss: 0.4913581609725952\n",
            "Batch: 2913    Epoch[3/3]    Training Loss: 0.4643731415271759\n",
            "Batch: 2914    Epoch[3/3]    Training Loss: 0.6424503326416016\n",
            "Batch: 2915    Epoch[3/3]    Training Loss: 0.8320239782333374\n",
            "Batch: 2916    Epoch[3/3]    Training Loss: 0.7849925756454468\n",
            "Batch: 2917    Epoch[3/3]    Training Loss: 0.5921690464019775\n",
            "Batch: 2918    Epoch[3/3]    Training Loss: 0.625139057636261\n",
            "Batch: 2919    Epoch[3/3]    Training Loss: 0.8212432861328125\n",
            "Batch: 2920    Epoch[3/3]    Training Loss: 0.24953535199165344\n",
            "Batch: 2921    Epoch[3/3]    Training Loss: 0.5980900526046753\n",
            "Batch: 2922    Epoch[3/3]    Training Loss: 0.41779446601867676\n",
            "Batch: 2923    Epoch[3/3]    Training Loss: 0.7672204971313477\n",
            "Batch: 2924    Epoch[3/3]    Training Loss: 0.4424952268600464\n",
            "Batch: 2925    Epoch[3/3]    Training Loss: 0.43928760290145874\n",
            "Batch: 2926    Epoch[3/3]    Training Loss: 0.738771915435791\n",
            "Batch: 2927    Epoch[3/3]    Training Loss: 0.6610819101333618\n",
            "Batch: 2928    Epoch[3/3]    Training Loss: 0.9751535654067993\n",
            "Batch: 2929    Epoch[3/3]    Training Loss: 0.49548453092575073\n",
            "Batch: 2930    Epoch[3/3]    Training Loss: 0.30117490887641907\n",
            "Batch: 2931    Epoch[3/3]    Training Loss: 0.5899035930633545\n",
            "Batch: 2932    Epoch[3/3]    Training Loss: 0.6112793684005737\n",
            "Batch: 2933    Epoch[3/3]    Training Loss: 0.5747753381729126\n",
            "Batch: 2934    Epoch[3/3]    Training Loss: 0.38645413517951965\n",
            "Batch: 2935    Epoch[3/3]    Training Loss: 0.41611185669898987\n",
            "Batch: 2936    Epoch[3/3]    Training Loss: 0.6491997241973877\n",
            "Batch: 2937    Epoch[3/3]    Training Loss: 0.6146671772003174\n",
            "Batch: 2938    Epoch[3/3]    Training Loss: 0.9341453313827515\n",
            "Batch: 2939    Epoch[3/3]    Training Loss: 0.846624493598938\n",
            "Batch: 2940    Epoch[3/3]    Training Loss: 0.6290033459663391\n",
            "Batch: 2941    Epoch[3/3]    Training Loss: 0.8359322547912598\n",
            "Batch: 2942    Epoch[3/3]    Training Loss: 0.7279294729232788\n",
            "Batch: 2943    Epoch[3/3]    Training Loss: 0.7273938655853271\n",
            "Batch: 2944    Epoch[3/3]    Training Loss: 0.6356271505355835\n",
            "Batch: 2945    Epoch[3/3]    Training Loss: 0.7764617204666138\n",
            "Batch: 2946    Epoch[3/3]    Training Loss: 0.4498838186264038\n",
            "Batch: 2947    Epoch[3/3]    Training Loss: 0.22836758196353912\n",
            "Batch: 2948    Epoch[3/3]    Training Loss: 0.4681209921836853\n",
            "Batch: 2949    Epoch[3/3]    Training Loss: 0.8468928933143616\n",
            "Batch: 2950    Epoch[3/3]    Training Loss: 0.7862651944160461\n",
            "Batch: 2951    Epoch[3/3]    Training Loss: 0.4885355234146118\n",
            "Batch: 2952    Epoch[3/3]    Training Loss: 0.48598748445510864\n",
            "Batch: 2953    Epoch[3/3]    Training Loss: 0.4041193127632141\n",
            "Batch: 2954    Epoch[3/3]    Training Loss: 0.7987223267555237\n",
            "Batch: 2955    Epoch[3/3]    Training Loss: 0.7338031530380249\n",
            "Batch: 2956    Epoch[3/3]    Training Loss: 0.4508818984031677\n",
            "Batch: 2957    Epoch[3/3]    Training Loss: 0.7738190293312073\n",
            "Batch: 2958    Epoch[3/3]    Training Loss: 1.2413440942764282\n",
            "Batch: 2959    Epoch[3/3]    Training Loss: 0.1352405697107315\n",
            "Batch: 2960    Epoch[3/3]    Training Loss: 0.6359982490539551\n",
            "Batch: 2961    Epoch[3/3]    Training Loss: 0.6339288949966431\n",
            "Batch: 2962    Epoch[3/3]    Training Loss: 0.656151294708252\n",
            "Batch: 2963    Epoch[3/3]    Training Loss: 0.4953973889350891\n",
            "Batch: 2964    Epoch[3/3]    Training Loss: 0.6529443264007568\n",
            "Batch: 2965    Epoch[3/3]    Training Loss: 0.36565864086151123\n",
            "Batch: 2966    Epoch[3/3]    Training Loss: 0.8115435242652893\n",
            "Batch: 2967    Epoch[3/3]    Training Loss: 0.34900331497192383\n",
            "Batch: 2968    Epoch[3/3]    Training Loss: 0.2985478639602661\n",
            "Batch: 2969    Epoch[3/3]    Training Loss: 0.4430496096611023\n",
            "Batch: 2970    Epoch[3/3]    Training Loss: 0.6585177183151245\n",
            "Batch: 2971    Epoch[3/3]    Training Loss: 0.42245712876319885\n",
            "Batch: 2972    Epoch[3/3]    Training Loss: 0.840514063835144\n",
            "Batch: 2973    Epoch[3/3]    Training Loss: 0.5746537446975708\n",
            "Batch: 2974    Epoch[3/3]    Training Loss: 0.5334873199462891\n",
            "Batch: 2975    Epoch[3/3]    Training Loss: 0.36526691913604736\n",
            "Batch: 2976    Epoch[3/3]    Training Loss: 0.5482591986656189\n",
            "Batch: 2977    Epoch[3/3]    Training Loss: 0.7960236668586731\n",
            "Batch: 2978    Epoch[3/3]    Training Loss: 0.6222900748252869\n",
            "Batch: 2979    Epoch[3/3]    Training Loss: 0.8773360848426819\n",
            "Batch: 2980    Epoch[3/3]    Training Loss: 0.7964507341384888\n",
            "Batch: 2981    Epoch[3/3]    Training Loss: 0.9343154430389404\n",
            "Batch: 2982    Epoch[3/3]    Training Loss: 0.3256172835826874\n",
            "Batch: 2983    Epoch[3/3]    Training Loss: 0.600890040397644\n",
            "Batch: 2984    Epoch[3/3]    Training Loss: 0.953037440776825\n",
            "Batch: 2985    Epoch[3/3]    Training Loss: 1.522439956665039\n",
            "Batch: 2986    Epoch[3/3]    Training Loss: 0.4819885790348053\n",
            "Batch: 2987    Epoch[3/3]    Training Loss: 0.5071669816970825\n",
            "Batch: 2988    Epoch[3/3]    Training Loss: 0.7916228771209717\n",
            "Batch: 2989    Epoch[3/3]    Training Loss: 0.38608986139297485\n",
            "Batch: 2990    Epoch[3/3]    Training Loss: 0.3583185374736786\n",
            "Batch: 2991    Epoch[3/3]    Training Loss: 0.43078404664993286\n",
            "Batch: 2992    Epoch[3/3]    Training Loss: 0.8926679491996765\n",
            "Batch: 2993    Epoch[3/3]    Training Loss: 0.3743622899055481\n",
            "Batch: 2994    Epoch[3/3]    Training Loss: 0.6346632838249207\n",
            "Batch: 2995    Epoch[3/3]    Training Loss: 0.1846146583557129\n",
            "Batch: 2996    Epoch[3/3]    Training Loss: 0.30096036195755005\n",
            "Batch: 2997    Epoch[3/3]    Training Loss: 0.948343813419342\n",
            "Batch: 2998    Epoch[3/3]    Training Loss: 0.46326908469200134\n",
            "Batch: 2999    Epoch[3/3]    Training Loss: 0.6531085968017578\n",
            "Batch: 3000    Epoch[3/3]    Training Loss: 0.8239971399307251\n",
            "Batch: 3001    Epoch[3/3]    Training Loss: 0.6209902763366699\n",
            "Batch: 3002    Epoch[3/3]    Training Loss: 1.0192316770553589\n",
            "Batch: 3003    Epoch[3/3]    Training Loss: 0.4993264377117157\n",
            "Batch: 3004    Epoch[3/3]    Training Loss: 0.46897509694099426\n",
            "Batch: 3005    Epoch[3/3]    Training Loss: 0.35347869992256165\n",
            "Batch: 3006    Epoch[3/3]    Training Loss: 1.4439637660980225\n",
            "Batch: 3007    Epoch[3/3]    Training Loss: 0.6958889961242676\n",
            "Batch: 3008    Epoch[3/3]    Training Loss: 0.48395198583602905\n",
            "Batch: 3009    Epoch[3/3]    Training Loss: 0.5607714653015137\n",
            "Batch: 3010    Epoch[3/3]    Training Loss: 0.29739224910736084\n",
            "Batch: 3011    Epoch[3/3]    Training Loss: 0.8772678375244141\n",
            "Batch: 3012    Epoch[3/3]    Training Loss: 0.3960002064704895\n",
            "Batch: 3013    Epoch[3/3]    Training Loss: 0.7825424075126648\n",
            "Batch: 3014    Epoch[3/3]    Training Loss: 0.6195746660232544\n",
            "Batch: 3015    Epoch[3/3]    Training Loss: 0.46935197710990906\n",
            "Batch: 3016    Epoch[3/3]    Training Loss: 0.4180859327316284\n",
            "Batch: 3017    Epoch[3/3]    Training Loss: 1.0442430973052979\n",
            "Batch: 3018    Epoch[3/3]    Training Loss: 0.5407779216766357\n",
            "Batch: 3019    Epoch[3/3]    Training Loss: 0.4655202031135559\n",
            "Batch: 3020    Epoch[3/3]    Training Loss: 0.4459794759750366\n",
            "Batch: 3021    Epoch[3/3]    Training Loss: 1.2860277891159058\n",
            "Batch: 3022    Epoch[3/3]    Training Loss: 0.35087156295776367\n",
            "Batch: 3023    Epoch[3/3]    Training Loss: 0.4210149943828583\n",
            "Batch: 3024    Epoch[3/3]    Training Loss: 0.5037528872489929\n",
            "Batch: 3025    Epoch[3/3]    Training Loss: 0.6310169696807861\n",
            "Batch: 3026    Epoch[3/3]    Training Loss: 0.45726633071899414\n",
            "Batch: 3027    Epoch[3/3]    Training Loss: 0.8141535520553589\n",
            "Batch: 3028    Epoch[3/3]    Training Loss: 0.621709406375885\n",
            "Batch: 3029    Epoch[3/3]    Training Loss: 0.758826494216919\n",
            "Batch: 3030    Epoch[3/3]    Training Loss: 0.6972805857658386\n",
            "Batch: 3031    Epoch[3/3]    Training Loss: 0.4231266379356384\n",
            "Batch: 3032    Epoch[3/3]    Training Loss: 0.5173359513282776\n",
            "Batch: 3033    Epoch[3/3]    Training Loss: 0.6336629986763\n",
            "Batch: 3034    Epoch[3/3]    Training Loss: 0.3408913016319275\n",
            "Batch: 3035    Epoch[3/3]    Training Loss: 1.1433448791503906\n",
            "Batch: 3036    Epoch[3/3]    Training Loss: 0.34775346517562866\n",
            "Batch: 3037    Epoch[3/3]    Training Loss: 0.7587118148803711\n",
            "Batch: 3038    Epoch[3/3]    Training Loss: 0.9275124073028564\n",
            "Batch: 3039    Epoch[3/3]    Training Loss: 0.81510329246521\n",
            "Batch: 3040    Epoch[3/3]    Training Loss: 0.534105658531189\n",
            "Batch: 3041    Epoch[3/3]    Training Loss: 0.8934265375137329\n",
            "Batch: 3042    Epoch[3/3]    Training Loss: 0.5595033168792725\n",
            "Batch: 3043    Epoch[3/3]    Training Loss: 0.5852600336074829\n",
            "Batch: 3044    Epoch[3/3]    Training Loss: 0.8372446298599243\n",
            "Batch: 3045    Epoch[3/3]    Training Loss: 0.8563230037689209\n",
            "Batch: 3046    Epoch[3/3]    Training Loss: 0.6573655605316162\n",
            "Batch: 3047    Epoch[3/3]    Training Loss: 0.6573246717453003\n",
            "Batch: 3048    Epoch[3/3]    Training Loss: 0.347165584564209\n",
            "Batch: 3049    Epoch[3/3]    Training Loss: 0.6933896541595459\n",
            "Batch: 3050    Epoch[3/3]    Training Loss: 0.3703772723674774\n",
            "Batch: 3051    Epoch[3/3]    Training Loss: 0.6575459241867065\n",
            "Batch: 3052    Epoch[3/3]    Training Loss: 0.6864671111106873\n",
            "Batch: 3053    Epoch[3/3]    Training Loss: 0.37410658597946167\n",
            "Batch: 3054    Epoch[3/3]    Training Loss: 0.31543850898742676\n",
            "Batch: 3055    Epoch[3/3]    Training Loss: 1.2748100757598877\n",
            "Batch: 3056    Epoch[3/3]    Training Loss: 0.9102418422698975\n",
            "Batch: 3057    Epoch[3/3]    Training Loss: 0.3895196318626404\n",
            "Batch: 3058    Epoch[3/3]    Training Loss: 0.4130522608757019\n",
            "Batch: 3059    Epoch[3/3]    Training Loss: 1.0030313730239868\n",
            "Batch: 3060    Epoch[3/3]    Training Loss: 0.22686582803726196\n",
            "Batch: 3061    Epoch[3/3]    Training Loss: 1.0923631191253662\n",
            "Batch: 3062    Epoch[3/3]    Training Loss: 0.3029569387435913\n",
            "Batch: 3063    Epoch[3/3]    Training Loss: 0.5365092754364014\n",
            "Batch: 3064    Epoch[3/3]    Training Loss: 0.2875673472881317\n",
            "Batch: 3065    Epoch[3/3]    Training Loss: 0.24630539119243622\n",
            "Batch: 3066    Epoch[3/3]    Training Loss: 0.18780004978179932\n",
            "Batch: 3067    Epoch[3/3]    Training Loss: 0.7787050604820251\n",
            "Batch: 3068    Epoch[3/3]    Training Loss: 0.3506973087787628\n",
            "Batch: 3069    Epoch[3/3]    Training Loss: 0.34087681770324707\n",
            "Batch: 3070    Epoch[3/3]    Training Loss: 0.6209334135055542\n",
            "Batch: 3071    Epoch[3/3]    Training Loss: 0.5075243711471558\n",
            "Batch: 3072    Epoch[3/3]    Training Loss: 0.3794286251068115\n",
            "Batch: 3073    Epoch[3/3]    Training Loss: 1.0455865859985352\n",
            "Batch: 3074    Epoch[3/3]    Training Loss: 0.5274059772491455\n",
            "Batch: 3075    Epoch[3/3]    Training Loss: 0.6164261102676392\n",
            "Batch: 3076    Epoch[3/3]    Training Loss: 0.609937846660614\n",
            "Batch: 3077    Epoch[3/3]    Training Loss: 0.36921530961990356\n",
            "Batch: 3078    Epoch[3/3]    Training Loss: 1.225048542022705\n",
            "Batch: 3079    Epoch[3/3]    Training Loss: 0.3269800543785095\n",
            "Batch: 3080    Epoch[3/3]    Training Loss: 0.19926436245441437\n",
            "Batch: 3081    Epoch[3/3]    Training Loss: 0.694461464881897\n",
            "Batch: 3082    Epoch[3/3]    Training Loss: 0.6950743198394775\n",
            "Batch: 3083    Epoch[3/3]    Training Loss: 0.27598118782043457\n",
            "Batch: 3084    Epoch[3/3]    Training Loss: 0.7346097230911255\n",
            "Batch: 3085    Epoch[3/3]    Training Loss: 0.6015585064888\n",
            "Batch: 3086    Epoch[3/3]    Training Loss: 0.5161632299423218\n",
            "Batch: 3087    Epoch[3/3]    Training Loss: 0.33501267433166504\n",
            "Batch: 3088    Epoch[3/3]    Training Loss: 0.8886762261390686\n",
            "Batch: 3089    Epoch[3/3]    Training Loss: 0.40315812826156616\n",
            "Batch: 3090    Epoch[3/3]    Training Loss: 0.5691823363304138\n",
            "Batch: 3091    Epoch[3/3]    Training Loss: 0.9867047667503357\n",
            "Batch: 3092    Epoch[3/3]    Training Loss: 1.1143839359283447\n",
            "Batch: 3093    Epoch[3/3]    Training Loss: 0.1988813877105713\n",
            "Batch: 3094    Epoch[3/3]    Training Loss: 0.7520207166671753\n",
            "Batch: 3095    Epoch[3/3]    Training Loss: 0.7027784585952759\n",
            "Batch: 3096    Epoch[3/3]    Training Loss: 0.6360976696014404\n",
            "Batch: 3097    Epoch[3/3]    Training Loss: 0.8077967166900635\n",
            "Batch: 3098    Epoch[3/3]    Training Loss: 0.4657909870147705\n",
            "Batch: 3099    Epoch[3/3]    Training Loss: 0.588141918182373\n",
            "Batch: 3100    Epoch[3/3]    Training Loss: 0.5790309906005859\n",
            "Batch: 3101    Epoch[3/3]    Training Loss: 0.6832279562950134\n",
            "Batch: 3102    Epoch[3/3]    Training Loss: 0.6856083869934082\n",
            "Batch: 3103    Epoch[3/3]    Training Loss: 0.8182724714279175\n",
            "Batch: 3104    Epoch[3/3]    Training Loss: 0.5852174758911133\n",
            "Batch: 3105    Epoch[3/3]    Training Loss: 0.5167688727378845\n",
            "Batch: 3106    Epoch[3/3]    Training Loss: 0.5363813638687134\n",
            "Batch: 3107    Epoch[3/3]    Training Loss: 0.7573738098144531\n",
            "Batch: 3108    Epoch[3/3]    Training Loss: 0.8642652630805969\n",
            "Batch: 3109    Epoch[3/3]    Training Loss: 0.5456051826477051\n",
            "Batch: 3110    Epoch[3/3]    Training Loss: 0.5751352906227112\n",
            "Batch: 3111    Epoch[3/3]    Training Loss: 1.2074687480926514\n",
            "Batch: 3112    Epoch[3/3]    Training Loss: 0.5370820760726929\n",
            "Batch: 3113    Epoch[3/3]    Training Loss: 0.7553068995475769\n",
            "Batch: 3114    Epoch[3/3]    Training Loss: 1.3255704641342163\n",
            "Batch: 3115    Epoch[3/3]    Training Loss: 0.5382123589515686\n",
            "Batch: 3116    Epoch[3/3]    Training Loss: 0.4708879590034485\n",
            "Batch: 3117    Epoch[3/3]    Training Loss: 0.6378582119941711\n",
            "Batch: 3118    Epoch[3/3]    Training Loss: 0.7073555588722229\n",
            "Batch: 3119    Epoch[3/3]    Training Loss: 0.8583739995956421\n",
            "Batch: 3120    Epoch[3/3]    Training Loss: 0.6884686946868896\n",
            "Batch: 3121    Epoch[3/3]    Training Loss: 0.41821834444999695\n",
            "Batch: 3122    Epoch[3/3]    Training Loss: 0.5830133557319641\n",
            "Batch: 3123    Epoch[3/3]    Training Loss: 0.9652690887451172\n",
            "Batch: 3124    Epoch[3/3]    Training Loss: 0.2558339536190033\n",
            "Batch: 3125    Epoch[3/3]    Training Loss: 0.8304558992385864\n",
            "Batch: 3126    Epoch[3/3]    Training Loss: 0.6406996250152588\n",
            "Batch: 3127    Epoch[3/3]    Training Loss: 0.6605342626571655\n",
            "Batch: 3128    Epoch[3/3]    Training Loss: 0.37558460235595703\n",
            "Batch: 3129    Epoch[3/3]    Training Loss: 0.4442530572414398\n",
            "Batch: 3130    Epoch[3/3]    Training Loss: 0.9545036554336548\n",
            "Batch: 3131    Epoch[3/3]    Training Loss: 0.460279643535614\n",
            "Batch: 3132    Epoch[3/3]    Training Loss: 0.6058216094970703\n",
            "Batch: 3133    Epoch[3/3]    Training Loss: 0.3236716687679291\n",
            "Batch: 3134    Epoch[3/3]    Training Loss: 0.47313928604125977\n",
            "Batch: 3135    Epoch[3/3]    Training Loss: 0.6615180373191833\n",
            "Batch: 3136    Epoch[3/3]    Training Loss: 0.4097629189491272\n",
            "Batch: 3137    Epoch[3/3]    Training Loss: 0.8225904703140259\n",
            "Batch: 3138    Epoch[3/3]    Training Loss: 0.37404119968414307\n",
            "Batch: 3139    Epoch[3/3]    Training Loss: 0.6989281177520752\n",
            "Batch: 3140    Epoch[3/3]    Training Loss: 0.6523540019989014\n",
            "Batch: 3141    Epoch[3/3]    Training Loss: 0.5440746545791626\n",
            "Batch: 3142    Epoch[3/3]    Training Loss: 0.6539032459259033\n",
            "Batch: 3143    Epoch[3/3]    Training Loss: 0.5573692321777344\n",
            "Batch: 3144    Epoch[3/3]    Training Loss: 0.36449143290519714\n",
            "Batch: 3145    Epoch[3/3]    Training Loss: 0.44233494997024536\n",
            "Batch: 3146    Epoch[3/3]    Training Loss: 0.9189415574073792\n",
            "Batch: 3147    Epoch[3/3]    Training Loss: 0.583865761756897\n",
            "Batch: 3148    Epoch[3/3]    Training Loss: 0.7426584959030151\n",
            "Batch: 3149    Epoch[3/3]    Training Loss: 0.3745914101600647\n",
            "Batch: 3150    Epoch[3/3]    Training Loss: 0.7106860876083374\n",
            "Batch: 3151    Epoch[3/3]    Training Loss: 0.46240055561065674\n",
            "Batch: 3152    Epoch[3/3]    Training Loss: 0.7085776329040527\n",
            "Batch: 3153    Epoch[3/3]    Training Loss: 1.4865057468414307\n",
            "Batch: 3154    Epoch[3/3]    Training Loss: 0.667438268661499\n",
            "Batch: 3155    Epoch[3/3]    Training Loss: 0.5304384827613831\n",
            "Batch: 3156    Epoch[3/3]    Training Loss: 0.8807802200317383\n",
            "Batch: 3157    Epoch[3/3]    Training Loss: 0.7101758122444153\n",
            "Batch: 3158    Epoch[3/3]    Training Loss: 0.6551995873451233\n",
            "Batch: 3159    Epoch[3/3]    Training Loss: 1.1243751049041748\n",
            "Batch: 3160    Epoch[3/3]    Training Loss: 0.7138437032699585\n",
            "Batch: 3161    Epoch[3/3]    Training Loss: 0.61991286277771\n",
            "Batch: 3162    Epoch[3/3]    Training Loss: 0.6053457856178284\n",
            "Batch: 3163    Epoch[3/3]    Training Loss: 0.5976295471191406\n",
            "Batch: 3164    Epoch[3/3]    Training Loss: 0.4431580901145935\n",
            "Batch: 3165    Epoch[3/3]    Training Loss: 0.4019273519515991\n",
            "Batch: 3166    Epoch[3/3]    Training Loss: 0.582632303237915\n",
            "Batch: 3167    Epoch[3/3]    Training Loss: 1.0667543411254883\n",
            "Batch: 3168    Epoch[3/3]    Training Loss: 0.8787652254104614\n",
            "Batch: 3169    Epoch[3/3]    Training Loss: 0.48885419964790344\n",
            "Batch: 3170    Epoch[3/3]    Training Loss: 0.6747338175773621\n",
            "Batch: 3171    Epoch[3/3]    Training Loss: 0.6756213903427124\n",
            "Batch: 3172    Epoch[3/3]    Training Loss: 0.3640977144241333\n",
            "Batch: 3173    Epoch[3/3]    Training Loss: 0.7264257669448853\n",
            "Batch: 3174    Epoch[3/3]    Training Loss: 0.7179786562919617\n",
            "Batch: 3175    Epoch[3/3]    Training Loss: 0.4445239007472992\n",
            "Batch: 3176    Epoch[3/3]    Training Loss: 0.4406336545944214\n",
            "Batch: 3177    Epoch[3/3]    Training Loss: 0.979283332824707\n",
            "Batch: 3178    Epoch[3/3]    Training Loss: 0.42794594168663025\n",
            "Batch: 3179    Epoch[3/3]    Training Loss: 0.46360135078430176\n",
            "Batch: 3180    Epoch[3/3]    Training Loss: 0.3610415458679199\n",
            "Batch: 3181    Epoch[3/3]    Training Loss: 0.2906336784362793\n",
            "Batch: 3182    Epoch[3/3]    Training Loss: 0.39909806847572327\n",
            "Batch: 3183    Epoch[3/3]    Training Loss: 0.5350943803787231\n",
            "Batch: 3184    Epoch[3/3]    Training Loss: 0.23332355916500092\n",
            "Batch: 3185    Epoch[3/3]    Training Loss: 0.9851294159889221\n",
            "Batch: 3186    Epoch[3/3]    Training Loss: 1.0662859678268433\n",
            "Batch: 3187    Epoch[3/3]    Training Loss: 1.2718336582183838\n",
            "Batch: 3188    Epoch[3/3]    Training Loss: 0.5122661590576172\n",
            "Batch: 3189    Epoch[3/3]    Training Loss: 0.4409630298614502\n",
            "Batch: 3190    Epoch[3/3]    Training Loss: 0.7001583576202393\n",
            "Batch: 3191    Epoch[3/3]    Training Loss: 0.8966999053955078\n",
            "Batch: 3192    Epoch[3/3]    Training Loss: 1.3815733194351196\n",
            "Batch: 3193    Epoch[3/3]    Training Loss: 0.9211365580558777\n",
            "Batch: 3194    Epoch[3/3]    Training Loss: 0.8908456563949585\n",
            "Batch: 3195    Epoch[3/3]    Training Loss: 0.396040678024292\n",
            "Batch: 3196    Epoch[3/3]    Training Loss: 0.42706945538520813\n",
            "Batch: 3197    Epoch[3/3]    Training Loss: 0.6481258273124695\n",
            "Batch: 3198    Epoch[3/3]    Training Loss: 0.2615300416946411\n",
            "Batch: 3199    Epoch[3/3]    Training Loss: 0.9233328104019165\n",
            "Batch: 3200    Epoch[3/3]    Training Loss: 0.5978834629058838\n",
            "Batch: 3201    Epoch[3/3]    Training Loss: 0.3741902709007263\n",
            "Batch: 3202    Epoch[3/3]    Training Loss: 0.9264830946922302\n",
            "Batch: 3203    Epoch[3/3]    Training Loss: 0.873191237449646\n",
            "Batch: 3204    Epoch[3/3]    Training Loss: 1.066994071006775\n",
            "Batch: 3205    Epoch[3/3]    Training Loss: 0.8331233263015747\n",
            "Batch: 3206    Epoch[3/3]    Training Loss: 0.30625081062316895\n",
            "Batch: 3207    Epoch[3/3]    Training Loss: 0.506758451461792\n",
            "Batch: 3208    Epoch[3/3]    Training Loss: 0.7601662874221802\n",
            "Batch: 3209    Epoch[3/3]    Training Loss: 0.5572946071624756\n",
            "Batch: 3210    Epoch[3/3]    Training Loss: 0.8777564764022827\n",
            "Batch: 3211    Epoch[3/3]    Training Loss: 0.31812530755996704\n",
            "Batch: 3212    Epoch[3/3]    Training Loss: 0.6788840889930725\n",
            "Batch: 3213    Epoch[3/3]    Training Loss: 0.5403211116790771\n",
            "Batch: 3214    Epoch[3/3]    Training Loss: 0.8849151134490967\n",
            "Batch: 3215    Epoch[3/3]    Training Loss: 0.28844356536865234\n",
            "Batch: 3216    Epoch[3/3]    Training Loss: 0.6950159072875977\n",
            "Batch: 3217    Epoch[3/3]    Training Loss: 0.4037218987941742\n",
            "Batch: 3218    Epoch[3/3]    Training Loss: 0.6476394534111023\n",
            "Batch: 3219    Epoch[3/3]    Training Loss: 0.8044514060020447\n",
            "Batch: 3220    Epoch[3/3]    Training Loss: 0.7587274312973022\n",
            "Batch: 3221    Epoch[3/3]    Training Loss: 0.9549692869186401\n",
            "Batch: 3222    Epoch[3/3]    Training Loss: 0.6071160435676575\n",
            "Batch: 3223    Epoch[3/3]    Training Loss: 0.9464498162269592\n",
            "Batch: 3224    Epoch[3/3]    Training Loss: 0.45599570870399475\n",
            "Batch: 3225    Epoch[3/3]    Training Loss: 0.5954223275184631\n",
            "Batch: 3226    Epoch[3/3]    Training Loss: 0.7696229219436646\n",
            "Batch: 3227    Epoch[3/3]    Training Loss: 0.6960060596466064\n",
            "Batch: 3228    Epoch[3/3]    Training Loss: 0.2973857820034027\n",
            "Batch: 3229    Epoch[3/3]    Training Loss: 0.9368377923965454\n",
            "Batch: 3230    Epoch[3/3]    Training Loss: 0.5376081466674805\n",
            "Batch: 3231    Epoch[3/3]    Training Loss: 0.37418144941329956\n",
            "Batch: 3232    Epoch[3/3]    Training Loss: 1.3822917938232422\n",
            "Batch: 3233    Epoch[3/3]    Training Loss: 0.6217405200004578\n",
            "Batch: 3234    Epoch[3/3]    Training Loss: 1.0433491468429565\n",
            "Batch: 3235    Epoch[3/3]    Training Loss: 0.48134052753448486\n",
            "Batch: 3236    Epoch[3/3]    Training Loss: 0.7402355670928955\n",
            "Batch: 3237    Epoch[3/3]    Training Loss: 0.29352062940597534\n",
            "Batch: 3238    Epoch[3/3]    Training Loss: 0.5983610153198242\n",
            "Batch: 3239    Epoch[3/3]    Training Loss: 0.3517332077026367\n",
            "Batch: 3240    Epoch[3/3]    Training Loss: 0.3122817575931549\n",
            "Batch: 3241    Epoch[3/3]    Training Loss: 0.4141479730606079\n",
            "Batch: 3242    Epoch[3/3]    Training Loss: 0.46363455057144165\n",
            "Batch: 3243    Epoch[3/3]    Training Loss: 1.0017133951187134\n",
            "Batch: 3244    Epoch[3/3]    Training Loss: 0.4517463445663452\n",
            "Batch: 3245    Epoch[3/3]    Training Loss: 0.8140252828598022\n",
            "Batch: 3246    Epoch[3/3]    Training Loss: 0.3509507477283478\n",
            "Batch: 3247    Epoch[3/3]    Training Loss: 0.9331120252609253\n",
            "Batch: 3248    Epoch[3/3]    Training Loss: 0.6993213891983032\n",
            "Batch: 3249    Epoch[3/3]    Training Loss: 0.7203872203826904\n",
            "Batch: 3250    Epoch[3/3]    Training Loss: 0.7181068062782288\n",
            "Batch: 3251    Epoch[3/3]    Training Loss: 0.4328911304473877\n",
            "Batch: 3252    Epoch[3/3]    Training Loss: 1.2723655700683594\n",
            "Batch: 3253    Epoch[3/3]    Training Loss: 0.4859819710254669\n",
            "Batch: 3254    Epoch[3/3]    Training Loss: 0.707493782043457\n",
            "Batch: 3255    Epoch[3/3]    Training Loss: 1.000158667564392\n",
            "Batch: 3256    Epoch[3/3]    Training Loss: 0.4239150285720825\n",
            "Batch: 3257    Epoch[3/3]    Training Loss: 0.3589661717414856\n",
            "Batch: 3258    Epoch[3/3]    Training Loss: 0.5910307168960571\n",
            "Batch: 3259    Epoch[3/3]    Training Loss: 0.4680531620979309\n",
            "Batch: 3260    Epoch[3/3]    Training Loss: 0.3628106117248535\n",
            "Batch: 3261    Epoch[3/3]    Training Loss: 1.1989333629608154\n",
            "Batch: 3262    Epoch[3/3]    Training Loss: 0.8892836570739746\n",
            "Batch: 3263    Epoch[3/3]    Training Loss: 0.4133491516113281\n",
            "Batch: 3264    Epoch[3/3]    Training Loss: 0.9168462753295898\n",
            "Batch: 3265    Epoch[3/3]    Training Loss: 0.488547146320343\n",
            "Batch: 3266    Epoch[3/3]    Training Loss: 0.43775129318237305\n",
            "Batch: 3267    Epoch[3/3]    Training Loss: 0.5482277274131775\n",
            "Batch: 3268    Epoch[3/3]    Training Loss: 0.9023830890655518\n",
            "Batch: 3269    Epoch[3/3]    Training Loss: 0.3878510594367981\n",
            "Batch: 3270    Epoch[3/3]    Training Loss: 0.4593225419521332\n",
            "Batch: 3271    Epoch[3/3]    Training Loss: 0.31954067945480347\n",
            "Batch: 3272    Epoch[3/3]    Training Loss: 0.5731134414672852\n",
            "Batch: 3273    Epoch[3/3]    Training Loss: 0.7943952083587646\n",
            "Batch: 3274    Epoch[3/3]    Training Loss: 0.5185312628746033\n",
            "Batch: 3275    Epoch[3/3]    Training Loss: 0.4239649176597595\n",
            "Batch: 3276    Epoch[3/3]    Training Loss: 0.2685249447822571\n",
            "Batch: 3277    Epoch[3/3]    Training Loss: 0.629061222076416\n",
            "Batch: 3278    Epoch[3/3]    Training Loss: 0.5306316614151001\n",
            "Batch: 3279    Epoch[3/3]    Training Loss: 0.9465646743774414\n",
            "Batch: 3280    Epoch[3/3]    Training Loss: 0.8788199424743652\n",
            "Batch: 3281    Epoch[3/3]    Training Loss: 0.9702752828598022\n",
            "Batch: 3282    Epoch[3/3]    Training Loss: 0.564301609992981\n",
            "Batch: 3283    Epoch[3/3]    Training Loss: 0.4260193109512329\n",
            "Batch: 3284    Epoch[3/3]    Training Loss: 0.45674583315849304\n",
            "Batch: 3285    Epoch[3/3]    Training Loss: 0.49909618496894836\n",
            "Batch: 3286    Epoch[3/3]    Training Loss: 0.8487987518310547\n",
            "Batch: 3287    Epoch[3/3]    Training Loss: 0.6167108416557312\n",
            "Batch: 3288    Epoch[3/3]    Training Loss: 1.243580937385559\n",
            "Batch: 3289    Epoch[3/3]    Training Loss: 0.43052372336387634\n",
            "Batch: 3290    Epoch[3/3]    Training Loss: 0.4528624415397644\n",
            "Batch: 3291    Epoch[3/3]    Training Loss: 0.8452047109603882\n",
            "Batch: 3292    Epoch[3/3]    Training Loss: 0.6648572087287903\n",
            "Batch: 3293    Epoch[3/3]    Training Loss: 0.5021377801895142\n",
            "Batch: 3294    Epoch[3/3]    Training Loss: 0.6457748413085938\n",
            "Batch: 3295    Epoch[3/3]    Training Loss: 0.40666741132736206\n",
            "Batch: 3296    Epoch[3/3]    Training Loss: 0.9865112900733948\n",
            "Batch: 3297    Epoch[3/3]    Training Loss: 0.3406093716621399\n",
            "Batch: 3298    Epoch[3/3]    Training Loss: 0.4503820538520813\n",
            "Batch: 3299    Epoch[3/3]    Training Loss: 0.6633820533752441\n",
            "Batch: 3300    Epoch[3/3]    Training Loss: 1.0344469547271729\n",
            "Batch: 3301    Epoch[3/3]    Training Loss: 0.1685454547405243\n",
            "Batch: 3302    Epoch[3/3]    Training Loss: 0.319013774394989\n",
            "Batch: 3303    Epoch[3/3]    Training Loss: 0.42312684655189514\n",
            "Batch: 3304    Epoch[3/3]    Training Loss: 0.4092402756214142\n",
            "Batch: 3305    Epoch[3/3]    Training Loss: 0.4108192026615143\n",
            "Batch: 3306    Epoch[3/3]    Training Loss: 0.6821026802062988\n",
            "Batch: 3307    Epoch[3/3]    Training Loss: 1.2922438383102417\n",
            "Batch: 3308    Epoch[3/3]    Training Loss: 0.33087554574012756\n",
            "Batch: 3309    Epoch[3/3]    Training Loss: 0.47831231355667114\n",
            "Batch: 3310    Epoch[3/3]    Training Loss: 0.5379641056060791\n",
            "Batch: 3311    Epoch[3/3]    Training Loss: 0.8727685213088989\n",
            "Batch: 3312    Epoch[3/3]    Training Loss: 1.3878458738327026\n",
            "Batch: 3313    Epoch[3/3]    Training Loss: 0.5652788877487183\n",
            "Batch: 3314    Epoch[3/3]    Training Loss: 0.49270856380462646\n",
            "Batch: 3315    Epoch[3/3]    Training Loss: 0.5499674081802368\n",
            "Batch: 3316    Epoch[3/3]    Training Loss: 0.7313265800476074\n",
            "Batch: 3317    Epoch[3/3]    Training Loss: 0.7448021173477173\n",
            "Batch: 3318    Epoch[3/3]    Training Loss: 0.6426553726196289\n",
            "Batch: 3319    Epoch[3/3]    Training Loss: 1.2480640411376953\n",
            "Batch: 3320    Epoch[3/3]    Training Loss: 0.8755500316619873\n",
            "Batch: 3321    Epoch[3/3]    Training Loss: 0.6712270975112915\n",
            "Batch: 3322    Epoch[3/3]    Training Loss: 0.6673810482025146\n",
            "Batch: 3323    Epoch[3/3]    Training Loss: 0.5989826917648315\n",
            "Batch: 3324    Epoch[3/3]    Training Loss: 0.42290398478507996\n",
            "Batch: 3325    Epoch[3/3]    Training Loss: 0.4520632028579712\n",
            "Batch: 3326    Epoch[3/3]    Training Loss: 0.9266033172607422\n",
            "Batch: 3327    Epoch[3/3]    Training Loss: 0.6792186498641968\n",
            "Batch: 3328    Epoch[3/3]    Training Loss: 0.4776190519332886\n",
            "Batch: 3329    Epoch[3/3]    Training Loss: 0.922797679901123\n",
            "Batch: 3330    Epoch[3/3]    Training Loss: 0.5344602465629578\n",
            "Batch: 3331    Epoch[3/3]    Training Loss: 0.5880740880966187\n",
            "Batch: 3332    Epoch[3/3]    Training Loss: 0.7945965528488159\n",
            "Batch: 3333    Epoch[3/3]    Training Loss: 0.7073646187782288\n",
            "Batch: 3334    Epoch[3/3]    Training Loss: 0.41522976756095886\n",
            "Batch: 3335    Epoch[3/3]    Training Loss: 0.7773820757865906\n",
            "Batch: 3336    Epoch[3/3]    Training Loss: 0.5798954367637634\n",
            "Batch: 3337    Epoch[3/3]    Training Loss: 1.0534603595733643\n",
            "Batch: 3338    Epoch[3/3]    Training Loss: 0.6592257022857666\n",
            "Batch: 3339    Epoch[3/3]    Training Loss: 0.5330287218093872\n",
            "Batch: 3340    Epoch[3/3]    Training Loss: 0.5659858584403992\n",
            "Batch: 3341    Epoch[3/3]    Training Loss: 0.6688288450241089\n",
            "Batch: 3342    Epoch[3/3]    Training Loss: 0.6600369811058044\n",
            "Batch: 3343    Epoch[3/3]    Training Loss: 0.4608623683452606\n",
            "Batch: 3344    Epoch[3/3]    Training Loss: 0.989372730255127\n",
            "Batch: 3345    Epoch[3/3]    Training Loss: 1.0345280170440674\n",
            "Batch: 3346    Epoch[3/3]    Training Loss: 0.676140546798706\n",
            "Batch: 3347    Epoch[3/3]    Training Loss: 0.7069799304008484\n",
            "Batch: 3348    Epoch[3/3]    Training Loss: 0.31927794218063354\n",
            "Batch: 3349    Epoch[3/3]    Training Loss: 0.34721866250038147\n",
            "Batch: 3350    Epoch[3/3]    Training Loss: 0.5246553421020508\n",
            "Batch: 3351    Epoch[3/3]    Training Loss: 0.3593190610408783\n",
            "Batch: 3352    Epoch[3/3]    Training Loss: 0.601382851600647\n",
            "Batch: 3353    Epoch[3/3]    Training Loss: 0.5231096744537354\n",
            "Batch: 3354    Epoch[3/3]    Training Loss: 0.6669228076934814\n",
            "Batch: 3355    Epoch[3/3]    Training Loss: 0.569420337677002\n",
            "Batch: 3356    Epoch[3/3]    Training Loss: 0.5587527751922607\n",
            "Batch: 3357    Epoch[3/3]    Training Loss: 0.6263266801834106\n",
            "Batch: 3358    Epoch[3/3]    Training Loss: 0.6570320129394531\n",
            "Batch: 3359    Epoch[3/3]    Training Loss: 0.5486748814582825\n",
            "Batch: 3360    Epoch[3/3]    Training Loss: 0.4713124632835388\n",
            "Batch: 3361    Epoch[3/3]    Training Loss: 0.5510306358337402\n",
            "Batch: 3362    Epoch[3/3]    Training Loss: 0.5921064019203186\n",
            "Batch: 3363    Epoch[3/3]    Training Loss: 0.41470372676849365\n",
            "Batch: 3364    Epoch[3/3]    Training Loss: 0.41657698154449463\n",
            "Batch: 3365    Epoch[3/3]    Training Loss: 1.0919638872146606\n",
            "Batch: 3366    Epoch[3/3]    Training Loss: 0.6809344291687012\n",
            "Batch: 3367    Epoch[3/3]    Training Loss: 0.7271442413330078\n",
            "Batch: 3368    Epoch[3/3]    Training Loss: 0.48256734013557434\n",
            "Batch: 3369    Epoch[3/3]    Training Loss: 0.5091108083724976\n",
            "Batch: 3370    Epoch[3/3]    Training Loss: 0.6934031248092651\n",
            "Batch: 3371    Epoch[3/3]    Training Loss: 0.13535985350608826\n",
            "Batch: 3372    Epoch[3/3]    Training Loss: 1.7129385471343994\n",
            "Batch: 3373    Epoch[3/3]    Training Loss: 0.9302318096160889\n",
            "Batch: 3374    Epoch[3/3]    Training Loss: 0.754820704460144\n",
            "Batch: 3375    Epoch[3/3]    Training Loss: 0.5449572801589966\n",
            "Batch: 3376    Epoch[3/3]    Training Loss: 0.4131922125816345\n",
            "Batch: 3377    Epoch[3/3]    Training Loss: 0.9878026247024536\n",
            "Batch: 3378    Epoch[3/3]    Training Loss: 0.8332842588424683\n",
            "Batch: 3379    Epoch[3/3]    Training Loss: 0.23495274782180786\n",
            "Batch: 3380    Epoch[3/3]    Training Loss: 0.590842068195343\n",
            "Batch: 3381    Epoch[3/3]    Training Loss: 0.6514899730682373\n",
            "Batch: 3382    Epoch[3/3]    Training Loss: 0.7419211268424988\n",
            "Batch: 3383    Epoch[3/3]    Training Loss: 0.9884658455848694\n",
            "Batch: 3384    Epoch[3/3]    Training Loss: 0.2567591369152069\n",
            "Batch: 3385    Epoch[3/3]    Training Loss: 0.633681058883667\n",
            "Batch: 3386    Epoch[3/3]    Training Loss: 0.5613275170326233\n",
            "Batch: 3387    Epoch[3/3]    Training Loss: 0.8565067052841187\n",
            "Batch: 3388    Epoch[3/3]    Training Loss: 0.6056796312332153\n",
            "Batch: 3389    Epoch[3/3]    Training Loss: 0.41594016551971436\n",
            "Batch: 3390    Epoch[3/3]    Training Loss: 1.033214807510376\n",
            "Batch: 3391    Epoch[3/3]    Training Loss: 0.8022111058235168\n",
            "Batch: 3392    Epoch[3/3]    Training Loss: 0.711387038230896\n",
            "Batch: 3393    Epoch[3/3]    Training Loss: 0.4454370141029358\n",
            "Batch: 3394    Epoch[3/3]    Training Loss: 0.4554976224899292\n",
            "Batch: 3395    Epoch[3/3]    Training Loss: 0.6243692636489868\n",
            "Batch: 3396    Epoch[3/3]    Training Loss: 0.6150033473968506\n",
            "Batch: 3397    Epoch[3/3]    Training Loss: 0.8559496402740479\n",
            "Batch: 3398    Epoch[3/3]    Training Loss: 0.43925371766090393\n",
            "Batch: 3399    Epoch[3/3]    Training Loss: 0.3994700312614441\n",
            "Batch: 3400    Epoch[3/3]    Training Loss: 0.5183072686195374\n",
            "Batch: 3401    Epoch[3/3]    Training Loss: 0.5763566493988037\n",
            "Batch: 3402    Epoch[3/3]    Training Loss: 0.46532142162323\n",
            "Batch: 3403    Epoch[3/3]    Training Loss: 0.77994304895401\n",
            "Batch: 3404    Epoch[3/3]    Training Loss: 0.5612318515777588\n",
            "Batch: 3405    Epoch[3/3]    Training Loss: 0.49355679750442505\n",
            "Batch: 3406    Epoch[3/3]    Training Loss: 0.41970568895339966\n",
            "Batch: 3407    Epoch[3/3]    Training Loss: 0.5347752571105957\n",
            "Batch: 3408    Epoch[3/3]    Training Loss: 0.8346757292747498\n",
            "Batch: 3409    Epoch[3/3]    Training Loss: 0.5285150408744812\n",
            "Batch: 3410    Epoch[3/3]    Training Loss: 0.91720050573349\n",
            "Batch: 3411    Epoch[3/3]    Training Loss: 0.46393102407455444\n",
            "Batch: 3412    Epoch[3/3]    Training Loss: 0.8675330877304077\n",
            "Batch: 3413    Epoch[3/3]    Training Loss: 0.4156050384044647\n",
            "Batch: 3414    Epoch[3/3]    Training Loss: 0.9285578727722168\n",
            "Batch: 3415    Epoch[3/3]    Training Loss: 0.6280654668807983\n",
            "Batch: 3416    Epoch[3/3]    Training Loss: 0.33841580152511597\n",
            "Batch: 3417    Epoch[3/3]    Training Loss: 0.42061787843704224\n",
            "Batch: 3418    Epoch[3/3]    Training Loss: 0.538949728012085\n",
            "Batch: 3419    Epoch[3/3]    Training Loss: 0.4923909902572632\n",
            "Batch: 3420    Epoch[3/3]    Training Loss: 0.8469253778457642\n",
            "Batch: 3421    Epoch[3/3]    Training Loss: 0.7801313996315002\n",
            "Batch: 3422    Epoch[3/3]    Training Loss: 0.5719910264015198\n",
            "Batch: 3423    Epoch[3/3]    Training Loss: 0.8256831169128418\n",
            "Batch: 3424    Epoch[3/3]    Training Loss: 0.6393610835075378\n",
            "Batch: 3425    Epoch[3/3]    Training Loss: 0.3114714026451111\n",
            "Batch: 3426    Epoch[3/3]    Training Loss: 0.36708134412765503\n",
            "Batch: 3427    Epoch[3/3]    Training Loss: 1.2110300064086914\n",
            "Batch: 3428    Epoch[3/3]    Training Loss: 0.4276459515094757\n",
            "Batch: 3429    Epoch[3/3]    Training Loss: 0.54808509349823\n",
            "Batch: 3430    Epoch[3/3]    Training Loss: 0.5638728141784668\n",
            "Batch: 3431    Epoch[3/3]    Training Loss: 0.3999684453010559\n",
            "Batch: 3432    Epoch[3/3]    Training Loss: 1.0831140279769897\n",
            "Batch: 3433    Epoch[3/3]    Training Loss: 0.6028363704681396\n",
            "Batch: 3434    Epoch[3/3]    Training Loss: 0.8219839334487915\n",
            "Batch: 3435    Epoch[3/3]    Training Loss: 0.5041548609733582\n",
            "Batch: 3436    Epoch[3/3]    Training Loss: 0.5213918685913086\n",
            "Batch: 3437    Epoch[3/3]    Training Loss: 0.39403659105300903\n",
            "Batch: 3438    Epoch[3/3]    Training Loss: 0.8791484832763672\n",
            "Batch: 3439    Epoch[3/3]    Training Loss: 0.5176726579666138\n",
            "Batch: 3440    Epoch[3/3]    Training Loss: 0.5571802854537964\n",
            "Batch: 3441    Epoch[3/3]    Training Loss: 0.635179877281189\n",
            "Batch: 3442    Epoch[3/3]    Training Loss: 0.7549487352371216\n",
            "Batch: 3443    Epoch[3/3]    Training Loss: 0.30431467294692993\n",
            "Batch: 3444    Epoch[3/3]    Training Loss: 0.6637390851974487\n",
            "Batch: 3445    Epoch[3/3]    Training Loss: 0.8482323884963989\n",
            "Batch: 3446    Epoch[3/3]    Training Loss: 0.3655880093574524\n",
            "Batch: 3447    Epoch[3/3]    Training Loss: 0.3160647749900818\n",
            "Batch: 3448    Epoch[3/3]    Training Loss: 0.6760229468345642\n",
            "Batch: 3449    Epoch[3/3]    Training Loss: 0.20958387851715088\n",
            "Batch: 3450    Epoch[3/3]    Training Loss: 0.4419035315513611\n",
            "Batch: 3451    Epoch[3/3]    Training Loss: 0.40192079544067383\n",
            "Batch: 3452    Epoch[3/3]    Training Loss: 0.51839280128479\n",
            "Batch: 3453    Epoch[3/3]    Training Loss: 0.5168042182922363\n",
            "Batch: 3454    Epoch[3/3]    Training Loss: 1.5936565399169922\n",
            "Batch: 3455    Epoch[3/3]    Training Loss: 0.4645535349845886\n",
            "Batch: 3456    Epoch[3/3]    Training Loss: 0.6692177653312683\n",
            "Batch: 3457    Epoch[3/3]    Training Loss: 0.5630953311920166\n",
            "Batch: 3458    Epoch[3/3]    Training Loss: 0.8780710697174072\n",
            "Batch: 3459    Epoch[3/3]    Training Loss: 0.9977760910987854\n",
            "Batch: 3460    Epoch[3/3]    Training Loss: 0.5589112639427185\n",
            "Batch: 3461    Epoch[3/3]    Training Loss: 0.44919687509536743\n",
            "Batch: 3462    Epoch[3/3]    Training Loss: 0.7264416217803955\n",
            "Batch: 3463    Epoch[3/3]    Training Loss: 0.5092363953590393\n",
            "Batch: 3464    Epoch[3/3]    Training Loss: 0.5679616332054138\n",
            "Batch: 3465    Epoch[3/3]    Training Loss: 0.3646244406700134\n",
            "Batch: 3466    Epoch[3/3]    Training Loss: 0.5834497213363647\n",
            "Batch: 3467    Epoch[3/3]    Training Loss: 0.39906033873558044\n",
            "Batch: 3468    Epoch[3/3]    Training Loss: 0.43441706895828247\n",
            "Batch: 3469    Epoch[3/3]    Training Loss: 0.3499065041542053\n",
            "Batch: 3470    Epoch[3/3]    Training Loss: 0.6232285499572754\n",
            "Batch: 3471    Epoch[3/3]    Training Loss: 0.7170791029930115\n",
            "Batch: 3472    Epoch[3/3]    Training Loss: 0.4815841615200043\n",
            "Batch: 3473    Epoch[3/3]    Training Loss: 0.1511370986700058\n",
            "Batch: 3474    Epoch[3/3]    Training Loss: 0.6478540897369385\n",
            "Batch: 3475    Epoch[3/3]    Training Loss: 0.3248114585876465\n",
            "Batch: 3476    Epoch[3/3]    Training Loss: 0.27466267347335815\n",
            "Batch: 3477    Epoch[3/3]    Training Loss: 0.42560428380966187\n",
            "Batch: 3478    Epoch[3/3]    Training Loss: 0.4706244170665741\n",
            "Batch: 3479    Epoch[3/3]    Training Loss: 0.6545069217681885\n",
            "Batch: 3480    Epoch[3/3]    Training Loss: 0.9851281642913818\n",
            "Batch: 3481    Epoch[3/3]    Training Loss: 0.3693159222602844\n",
            "Batch: 3482    Epoch[3/3]    Training Loss: 0.2802594304084778\n",
            "Batch: 3483    Epoch[3/3]    Training Loss: 0.31749868392944336\n",
            "Batch: 3484    Epoch[3/3]    Training Loss: 0.9052450656890869\n",
            "Batch: 3485    Epoch[3/3]    Training Loss: 0.9074150323867798\n",
            "Batch: 3486    Epoch[3/3]    Training Loss: 0.6009507775306702\n",
            "Batch: 3487    Epoch[3/3]    Training Loss: 0.5775676369667053\n",
            "Batch: 3488    Epoch[3/3]    Training Loss: 0.7048690319061279\n",
            "Batch: 3489    Epoch[3/3]    Training Loss: 0.6042556762695312\n",
            "Batch: 3490    Epoch[3/3]    Training Loss: 0.6327972412109375\n",
            "Batch: 3491    Epoch[3/3]    Training Loss: 0.678276538848877\n",
            "Batch: 3492    Epoch[3/3]    Training Loss: 0.44238707423210144\n",
            "Batch: 3493    Epoch[3/3]    Training Loss: 0.6780298948287964\n",
            "Batch: 3494    Epoch[3/3]    Training Loss: 1.2536938190460205\n",
            "Batch: 3495    Epoch[3/3]    Training Loss: 0.973513126373291\n",
            "Batch: 3496    Epoch[3/3]    Training Loss: 0.7089622020721436\n",
            "Batch: 3497    Epoch[3/3]    Training Loss: 0.8731638193130493\n",
            "Batch: 3498    Epoch[3/3]    Training Loss: 0.07733878493309021\n",
            "Batch: 3499    Epoch[3/3]    Training Loss: 0.42538994550704956\n",
            "Batch: 3500    Epoch[3/3]    Training Loss: 0.6991783380508423\n",
            "Batch: 3501    Epoch[3/3]    Training Loss: 0.32281845808029175\n",
            "Batch: 3502    Epoch[3/3]    Training Loss: 0.46396055817604065\n",
            "Batch: 3503    Epoch[3/3]    Training Loss: 0.6787297129631042\n",
            "Batch: 3504    Epoch[3/3]    Training Loss: 0.39806467294692993\n",
            "Batch: 3505    Epoch[3/3]    Training Loss: 0.793113112449646\n",
            "Batch: 3506    Epoch[3/3]    Training Loss: 0.4260355830192566\n",
            "Batch: 3507    Epoch[3/3]    Training Loss: 0.7440073490142822\n",
            "Batch: 3508    Epoch[3/3]    Training Loss: 0.49257582426071167\n",
            "Batch: 3509    Epoch[3/3]    Training Loss: 0.5450034141540527\n",
            "Batch: 3510    Epoch[3/3]    Training Loss: 0.35147082805633545\n",
            "Batch: 3511    Epoch[3/3]    Training Loss: 0.37228620052337646\n",
            "Batch: 3512    Epoch[3/3]    Training Loss: 1.3015906810760498\n",
            "Batch: 3513    Epoch[3/3]    Training Loss: 0.3266829252243042\n",
            "Batch: 3514    Epoch[3/3]    Training Loss: 0.6211607456207275\n",
            "Batch: 3515    Epoch[3/3]    Training Loss: 0.5862855911254883\n",
            "Batch: 3516    Epoch[3/3]    Training Loss: 0.6496317386627197\n",
            "Batch: 3517    Epoch[3/3]    Training Loss: 1.1062496900558472\n",
            "Batch: 3518    Epoch[3/3]    Training Loss: 1.159987211227417\n",
            "Batch: 3519    Epoch[3/3]    Training Loss: 1.0083463191986084\n",
            "Batch: 3520    Epoch[3/3]    Training Loss: 0.927251398563385\n",
            "Batch: 3521    Epoch[3/3]    Training Loss: 0.571515679359436\n",
            "Batch: 3522    Epoch[3/3]    Training Loss: 0.672738254070282\n",
            "Batch: 3523    Epoch[3/3]    Training Loss: 0.45188021659851074\n",
            "Batch: 3524    Epoch[3/3]    Training Loss: 0.6228511333465576\n",
            "Batch: 3525    Epoch[3/3]    Training Loss: 0.6293269395828247\n",
            "Batch: 3526    Epoch[3/3]    Training Loss: 0.21813949942588806\n",
            "Batch: 3527    Epoch[3/3]    Training Loss: 0.5333648324012756\n",
            "Batch: 3528    Epoch[3/3]    Training Loss: 0.4849899411201477\n",
            "Batch: 3529    Epoch[3/3]    Training Loss: 0.8923988938331604\n",
            "Batch: 3530    Epoch[3/3]    Training Loss: 0.29945269227027893\n",
            "Batch: 3531    Epoch[3/3]    Training Loss: 0.9197545051574707\n",
            "Batch: 3532    Epoch[3/3]    Training Loss: 1.0602262020111084\n",
            "Batch: 3533    Epoch[3/3]    Training Loss: 0.4759765863418579\n",
            "Batch: 3534    Epoch[3/3]    Training Loss: 0.4477134943008423\n",
            "Batch: 3535    Epoch[3/3]    Training Loss: 0.6893714070320129\n",
            "Batch: 3536    Epoch[3/3]    Training Loss: 0.41147005558013916\n",
            "Batch: 3537    Epoch[3/3]    Training Loss: 0.806923508644104\n",
            "Batch: 3538    Epoch[3/3]    Training Loss: 0.5125173330307007\n",
            "Batch: 3539    Epoch[3/3]    Training Loss: 0.3156645894050598\n",
            "Batch: 3540    Epoch[3/3]    Training Loss: 0.49412524700164795\n",
            "Batch: 3541    Epoch[3/3]    Training Loss: 0.9887970685958862\n",
            "Batch: 3542    Epoch[3/3]    Training Loss: 0.7559587955474854\n",
            "Batch: 3543    Epoch[3/3]    Training Loss: 0.3728494644165039\n",
            "Batch: 3544    Epoch[3/3]    Training Loss: 0.30630117654800415\n",
            "Batch: 3545    Epoch[3/3]    Training Loss: 0.4312262535095215\n",
            "Batch: 3546    Epoch[3/3]    Training Loss: 0.6768616437911987\n",
            "Batch: 3547    Epoch[3/3]    Training Loss: 1.0556645393371582\n",
            "Batch: 3548    Epoch[3/3]    Training Loss: 0.619634747505188\n",
            "Batch: 3549    Epoch[3/3]    Training Loss: 0.3357807397842407\n",
            "Batch: 3550    Epoch[3/3]    Training Loss: 0.4616633355617523\n",
            "Batch: 3551    Epoch[3/3]    Training Loss: 0.4711216986179352\n",
            "Batch: 3552    Epoch[3/3]    Training Loss: 0.7841259241104126\n",
            "Batch: 3553    Epoch[3/3]    Training Loss: 0.8654939532279968\n",
            "Batch: 3554    Epoch[3/3]    Training Loss: 0.6039989590644836\n",
            "Batch: 3555    Epoch[3/3]    Training Loss: 0.37187659740448\n",
            "Batch: 3556    Epoch[3/3]    Training Loss: 0.8011085987091064\n",
            "Batch: 3557    Epoch[3/3]    Training Loss: 0.8435214161872864\n",
            "Batch: 3558    Epoch[3/3]    Training Loss: 0.778836727142334\n",
            "Batch: 3559    Epoch[3/3]    Training Loss: 0.8621469140052795\n",
            "Batch: 3560    Epoch[3/3]    Training Loss: 0.3611699938774109\n",
            "Batch: 3561    Epoch[3/3]    Training Loss: 0.4477437734603882\n",
            "Batch: 3562    Epoch[3/3]    Training Loss: 0.38109731674194336\n",
            "Batch: 3563    Epoch[3/3]    Training Loss: 0.29753684997558594\n",
            "Batch: 3564    Epoch[3/3]    Training Loss: 0.4820373058319092\n",
            "Batch: 3565    Epoch[3/3]    Training Loss: 1.553558588027954\n",
            "Batch: 3566    Epoch[3/3]    Training Loss: 0.47565656900405884\n",
            "Batch: 3567    Epoch[3/3]    Training Loss: 0.582088828086853\n",
            "Batch: 3568    Epoch[3/3]    Training Loss: 0.37780147790908813\n",
            "Batch: 3569    Epoch[3/3]    Training Loss: 0.2227281779050827\n",
            "Batch: 3570    Epoch[3/3]    Training Loss: 1.6304526329040527\n",
            "Batch: 3571    Epoch[3/3]    Training Loss: 0.33891013264656067\n",
            "Batch: 3572    Epoch[3/3]    Training Loss: 0.5111850500106812\n",
            "Batch: 3573    Epoch[3/3]    Training Loss: 0.9278020262718201\n",
            "Batch: 3574    Epoch[3/3]    Training Loss: 0.42599400877952576\n",
            "Batch: 3575    Epoch[3/3]    Training Loss: 0.23355527222156525\n",
            "Batch: 3576    Epoch[3/3]    Training Loss: 0.3684637248516083\n",
            "Batch: 3577    Epoch[3/3]    Training Loss: 0.250861793756485\n",
            "Batch: 3578    Epoch[3/3]    Training Loss: 0.951134443283081\n",
            "Batch: 3579    Epoch[3/3]    Training Loss: 0.517329216003418\n",
            "Batch: 3580    Epoch[3/3]    Training Loss: 0.4630129933357239\n",
            "Batch: 3581    Epoch[3/3]    Training Loss: 0.5496008396148682\n",
            "Batch: 3582    Epoch[3/3]    Training Loss: 0.508175253868103\n",
            "Batch: 3583    Epoch[3/3]    Training Loss: 1.2865118980407715\n",
            "Batch: 3584    Epoch[3/3]    Training Loss: 0.4601345658302307\n",
            "Batch: 3585    Epoch[3/3]    Training Loss: 0.1947002410888672\n",
            "Batch: 3586    Epoch[3/3]    Training Loss: 0.7625868320465088\n",
            "Batch: 3587    Epoch[3/3]    Training Loss: 0.8969667553901672\n",
            "Batch: 3588    Epoch[3/3]    Training Loss: 0.42134714126586914\n",
            "Batch: 3589    Epoch[3/3]    Training Loss: 1.3477373123168945\n",
            "Batch: 3590    Epoch[3/3]    Training Loss: 0.8345913290977478\n",
            "Batch: 3591    Epoch[3/3]    Training Loss: 0.8251063823699951\n",
            "Batch: 3592    Epoch[3/3]    Training Loss: 0.5939003229141235\n",
            "Batch: 3593    Epoch[3/3]    Training Loss: 0.33866581320762634\n",
            "Batch: 3594    Epoch[3/3]    Training Loss: 0.48571962118148804\n",
            "Batch: 3595    Epoch[3/3]    Training Loss: 0.5381753444671631\n",
            "Batch: 3596    Epoch[3/3]    Training Loss: 0.7414734363555908\n",
            "Batch: 3597    Epoch[3/3]    Training Loss: 0.3223128616809845\n",
            "Batch: 3598    Epoch[3/3]    Training Loss: 0.26235586404800415\n",
            "Batch: 3599    Epoch[3/3]    Training Loss: 1.2648983001708984\n",
            "Batch: 3600    Epoch[3/3]    Training Loss: 0.464289128780365\n",
            "Batch: 3601    Epoch[3/3]    Training Loss: 0.6006754040718079\n",
            "Batch: 3602    Epoch[3/3]    Training Loss: 0.689087986946106\n",
            "Batch: 3603    Epoch[3/3]    Training Loss: 0.25130775570869446\n",
            "Batch: 3604    Epoch[3/3]    Training Loss: 0.904299259185791\n",
            "Batch: 3605    Epoch[3/3]    Training Loss: 0.9363480806350708\n",
            "Batch: 3606    Epoch[3/3]    Training Loss: 0.7147557735443115\n",
            "Batch: 3607    Epoch[3/3]    Training Loss: 0.7157555818557739\n",
            "Batch: 3608    Epoch[3/3]    Training Loss: 0.5950012803077698\n",
            "Batch: 3609    Epoch[3/3]    Training Loss: 0.933375358581543\n",
            "Batch: 3610    Epoch[3/3]    Training Loss: 0.3540792763233185\n",
            "Batch: 3611    Epoch[3/3]    Training Loss: 0.4147876501083374\n",
            "Batch: 3612    Epoch[3/3]    Training Loss: 0.6720045804977417\n",
            "Batch: 3613    Epoch[3/3]    Training Loss: 0.5091772079467773\n",
            "Batch: 3614    Epoch[3/3]    Training Loss: 0.6094205975532532\n",
            "Batch: 3615    Epoch[3/3]    Training Loss: 0.8515193462371826\n",
            "Batch: 3616    Epoch[3/3]    Training Loss: 0.378714919090271\n",
            "Batch: 3617    Epoch[3/3]    Training Loss: 0.3104040324687958\n",
            "Batch: 3618    Epoch[3/3]    Training Loss: 0.3528643846511841\n",
            "Batch: 3619    Epoch[3/3]    Training Loss: 0.7906219959259033\n",
            "Batch: 3620    Epoch[3/3]    Training Loss: 0.35433319211006165\n",
            "Batch: 3621    Epoch[3/3]    Training Loss: 0.2731701135635376\n",
            "Batch: 3622    Epoch[3/3]    Training Loss: 0.4308331310749054\n",
            "Batch: 3623    Epoch[3/3]    Training Loss: 0.9962491393089294\n",
            "Batch: 3624    Epoch[3/3]    Training Loss: 0.3939342200756073\n",
            "Batch: 3625    Epoch[3/3]    Training Loss: 0.623837411403656\n",
            "Batch: 3626    Epoch[3/3]    Training Loss: 0.9753633737564087\n",
            "Batch: 3627    Epoch[3/3]    Training Loss: 0.2781498432159424\n",
            "Batch: 3628    Epoch[3/3]    Training Loss: 1.2779219150543213\n",
            "Batch: 3629    Epoch[3/3]    Training Loss: 0.6043378710746765\n",
            "Batch: 3630    Epoch[3/3]    Training Loss: 0.9269269704818726\n",
            "Batch: 3631    Epoch[3/3]    Training Loss: 0.6949594616889954\n",
            "Batch: 3632    Epoch[3/3]    Training Loss: 0.7348557710647583\n",
            "Batch: 3633    Epoch[3/3]    Training Loss: 0.46475136280059814\n",
            "Batch: 3634    Epoch[3/3]    Training Loss: 0.5277284383773804\n",
            "Batch: 3635    Epoch[3/3]    Training Loss: 0.8057162165641785\n",
            "Batch: 3636    Epoch[3/3]    Training Loss: 0.8103723526000977\n",
            "Batch: 3637    Epoch[3/3]    Training Loss: 1.562690019607544\n",
            "Batch: 3638    Epoch[3/3]    Training Loss: 0.15092450380325317\n",
            "Batch: 3639    Epoch[3/3]    Training Loss: 0.40197721123695374\n",
            "Batch: 3640    Epoch[3/3]    Training Loss: 0.6735334992408752\n",
            "Batch: 3641    Epoch[3/3]    Training Loss: 0.42132413387298584\n",
            "Batch: 3642    Epoch[3/3]    Training Loss: 0.38866961002349854\n",
            "Batch: 3643    Epoch[3/3]    Training Loss: 0.6649562120437622\n",
            "Batch: 3644    Epoch[3/3]    Training Loss: 0.614533543586731\n",
            "Batch: 3645    Epoch[3/3]    Training Loss: 0.7605944871902466\n",
            "Batch: 3646    Epoch[3/3]    Training Loss: 0.9507437944412231\n",
            "Batch: 3647    Epoch[3/3]    Training Loss: 0.5035820007324219\n",
            "Batch: 3648    Epoch[3/3]    Training Loss: 0.6583330631256104\n",
            "Batch: 3649    Epoch[3/3]    Training Loss: 0.6960879564285278\n",
            "Batch: 3650    Epoch[3/3]    Training Loss: 1.113478660583496\n",
            "Batch: 3651    Epoch[3/3]    Training Loss: 0.4722245931625366\n",
            "Batch: 3652    Epoch[3/3]    Training Loss: 0.5888317823410034\n",
            "Batch: 3653    Epoch[3/3]    Training Loss: 0.7383614182472229\n",
            "Batch: 3654    Epoch[3/3]    Training Loss: 0.4693599343299866\n",
            "Batch: 3655    Epoch[3/3]    Training Loss: 0.5590824484825134\n",
            "Batch: 3656    Epoch[3/3]    Training Loss: 0.7229760885238647\n",
            "Batch: 3657    Epoch[3/3]    Training Loss: 0.6423218250274658\n",
            "Batch: 3658    Epoch[3/3]    Training Loss: 0.4329725205898285\n",
            "Batch: 3659    Epoch[3/3]    Training Loss: 0.37838804721832275\n",
            "Batch: 3660    Epoch[3/3]    Training Loss: 1.0994594097137451\n",
            "Batch: 3661    Epoch[3/3]    Training Loss: 0.6754366159439087\n",
            "Batch: 3662    Epoch[3/3]    Training Loss: 0.7712681293487549\n",
            "Batch: 3663    Epoch[3/3]    Training Loss: 0.8415321707725525\n",
            "Batch: 3664    Epoch[3/3]    Training Loss: 0.34841811656951904\n",
            "Batch: 3665    Epoch[3/3]    Training Loss: 0.5180233716964722\n",
            "Batch: 3666    Epoch[3/3]    Training Loss: 0.4961051940917969\n",
            "Batch: 3667    Epoch[3/3]    Training Loss: 0.46505656838417053\n",
            "Batch: 3668    Epoch[3/3]    Training Loss: 0.8776455521583557\n",
            "Batch: 3669    Epoch[3/3]    Training Loss: 0.637305498123169\n",
            "Batch: 3670    Epoch[3/3]    Training Loss: 0.5236625075340271\n",
            "Batch: 3671    Epoch[3/3]    Training Loss: 0.8026509284973145\n",
            "Batch: 3672    Epoch[3/3]    Training Loss: 0.7613978981971741\n",
            "Batch: 3673    Epoch[3/3]    Training Loss: 0.5511445999145508\n",
            "Batch: 3674    Epoch[3/3]    Training Loss: 1.0546725988388062\n",
            "Batch: 3675    Epoch[3/3]    Training Loss: 0.6448771357536316\n",
            "Batch: 3676    Epoch[3/3]    Training Loss: 0.6880673766136169\n",
            "Batch: 3677    Epoch[3/3]    Training Loss: 0.2944401502609253\n",
            "Batch: 3678    Epoch[3/3]    Training Loss: 0.9157509803771973\n",
            "Batch: 3679    Epoch[3/3]    Training Loss: 0.49870938062667847\n",
            "Batch: 3680    Epoch[3/3]    Training Loss: 0.7115545868873596\n",
            "Batch: 3681    Epoch[3/3]    Training Loss: 0.6701880693435669\n",
            "Batch: 3682    Epoch[3/3]    Training Loss: 0.2770894169807434\n",
            "Batch: 3683    Epoch[3/3]    Training Loss: 0.4150882959365845\n",
            "Batch: 3684    Epoch[3/3]    Training Loss: 0.5787049531936646\n",
            "Batch: 3685    Epoch[3/3]    Training Loss: 0.3182668387889862\n",
            "Batch: 3686    Epoch[3/3]    Training Loss: 0.8669077157974243\n",
            "Batch: 3687    Epoch[3/3]    Training Loss: 0.7097182273864746\n",
            "Batch: 3688    Epoch[3/3]    Training Loss: 0.7714505195617676\n",
            "Batch: 3689    Epoch[3/3]    Training Loss: 1.181146264076233\n",
            "Batch: 3690    Epoch[3/3]    Training Loss: 0.46983200311660767\n",
            "Batch: 3691    Epoch[3/3]    Training Loss: 0.46905872225761414\n",
            "Batch: 3692    Epoch[3/3]    Training Loss: 0.48596036434173584\n",
            "Batch: 3693    Epoch[3/3]    Training Loss: 0.8427577018737793\n",
            "Batch: 3694    Epoch[3/3]    Training Loss: 0.20950904488563538\n",
            "Batch: 3695    Epoch[3/3]    Training Loss: 0.8090026378631592\n",
            "Batch: 3696    Epoch[3/3]    Training Loss: 0.40770381689071655\n",
            "Batch: 3697    Epoch[3/3]    Training Loss: 0.4708781838417053\n",
            "Batch: 3698    Epoch[3/3]    Training Loss: 0.7687474489212036\n",
            "Batch: 3699    Epoch[3/3]    Training Loss: 1.0169428586959839\n",
            "Batch: 3700    Epoch[3/3]    Training Loss: 0.41613155603408813\n",
            "Batch: 3701    Epoch[3/3]    Training Loss: 0.6739993095397949\n",
            "Batch: 3702    Epoch[3/3]    Training Loss: 0.9568763971328735\n",
            "Batch: 3703    Epoch[3/3]    Training Loss: 0.6122310161590576\n",
            "Batch: 3704    Epoch[3/3]    Training Loss: 0.704337477684021\n",
            "Batch: 3705    Epoch[3/3]    Training Loss: 0.39131802320480347\n",
            "Batch: 3706    Epoch[3/3]    Training Loss: 0.8583149909973145\n",
            "Batch: 3707    Epoch[3/3]    Training Loss: 0.7876253724098206\n",
            "Batch: 3708    Epoch[3/3]    Training Loss: 0.8991497755050659\n",
            "Batch: 3709    Epoch[3/3]    Training Loss: 0.2798905372619629\n",
            "Batch: 3710    Epoch[3/3]    Training Loss: 0.45447856187820435\n",
            "Batch: 3711    Epoch[3/3]    Training Loss: 0.6825183629989624\n",
            "Batch: 3712    Epoch[3/3]    Training Loss: 0.5931432247161865\n",
            "Batch: 3713    Epoch[3/3]    Training Loss: 0.3625844717025757\n",
            "Batch: 3714    Epoch[3/3]    Training Loss: 0.34764665365219116\n",
            "Batch: 3715    Epoch[3/3]    Training Loss: 0.6436681747436523\n",
            "Batch: 3716    Epoch[3/3]    Training Loss: 0.610069990158081\n",
            "Batch: 3717    Epoch[3/3]    Training Loss: 0.8710658550262451\n",
            "Batch: 3718    Epoch[3/3]    Training Loss: 0.4038010239601135\n",
            "Batch: 3719    Epoch[3/3]    Training Loss: 0.6309680938720703\n",
            "Batch: 3720    Epoch[3/3]    Training Loss: 1.0830154418945312\n",
            "Batch: 3721    Epoch[3/3]    Training Loss: 0.8918652534484863\n",
            "Batch: 3722    Epoch[3/3]    Training Loss: 0.4267174005508423\n",
            "Batch: 3723    Epoch[3/3]    Training Loss: 0.6172420978546143\n",
            "Batch: 3724    Epoch[3/3]    Training Loss: 0.4267098009586334\n",
            "Batch: 3725    Epoch[3/3]    Training Loss: 0.536512017250061\n",
            "Batch: 3726    Epoch[3/3]    Training Loss: 0.8139798641204834\n",
            "Batch: 3727    Epoch[3/3]    Training Loss: 0.5980541706085205\n",
            "Batch: 3728    Epoch[3/3]    Training Loss: 0.4564640522003174\n",
            "Batch: 3729    Epoch[3/3]    Training Loss: 0.4953702688217163\n",
            "Batch: 3730    Epoch[3/3]    Training Loss: 0.8771067261695862\n",
            "Batch: 3731    Epoch[3/3]    Training Loss: 0.6671682596206665\n",
            "Batch: 3732    Epoch[3/3]    Training Loss: 0.16193673014640808\n",
            "Batch: 3733    Epoch[3/3]    Training Loss: 0.7346707582473755\n",
            "Batch: 3734    Epoch[3/3]    Training Loss: 0.7832130193710327\n",
            "Batch: 3735    Epoch[3/3]    Training Loss: 0.7125880122184753\n",
            "Batch: 3736    Epoch[3/3]    Training Loss: 0.39792096614837646\n",
            "Batch: 3737    Epoch[3/3]    Training Loss: 0.8537908792495728\n",
            "Batch: 3738    Epoch[3/3]    Training Loss: 0.2684541344642639\n",
            "Batch: 3739    Epoch[3/3]    Training Loss: 0.6563485860824585\n",
            "Batch: 3740    Epoch[3/3]    Training Loss: 0.4809902310371399\n",
            "Batch: 3741    Epoch[3/3]    Training Loss: 0.27326709032058716\n",
            "Batch: 3742    Epoch[3/3]    Training Loss: 0.6775934100151062\n",
            "Batch: 3743    Epoch[3/3]    Training Loss: 0.8191924095153809\n",
            "Batch: 3744    Epoch[3/3]    Training Loss: 0.19915153086185455\n",
            "Batch: 3745    Epoch[3/3]    Training Loss: 1.0694873332977295\n",
            "Batch: 3746    Epoch[3/3]    Training Loss: 0.7149920463562012\n",
            "Batch: 3747    Epoch[3/3]    Training Loss: 0.7220081090927124\n",
            "Batch: 3748    Epoch[3/3]    Training Loss: 0.6278015375137329\n",
            "Batch: 3749    Epoch[3/3]    Training Loss: 0.6586226224899292\n",
            "Batch: 3750    Epoch[3/3]    Training Loss: 0.6365034580230713\n",
            "Batch: 3751    Epoch[3/3]    Training Loss: 0.5233742594718933\n",
            "Batch: 3752    Epoch[3/3]    Training Loss: 0.551611065864563\n",
            "Batch: 3753    Epoch[3/3]    Training Loss: 0.6052636504173279\n",
            "Batch: 3754    Epoch[3/3]    Training Loss: 0.5695960521697998\n",
            "Batch: 3755    Epoch[3/3]    Training Loss: 0.8168932795524597\n",
            "Batch: 3756    Epoch[3/3]    Training Loss: 0.6091748476028442\n",
            "Batch: 3757    Epoch[3/3]    Training Loss: 0.5548889636993408\n",
            "Batch: 3758    Epoch[3/3]    Training Loss: 0.6258375644683838\n",
            "Batch: 3759    Epoch[3/3]    Training Loss: 0.4503500163555145\n",
            "Batch: 3760    Epoch[3/3]    Training Loss: 0.3453706204891205\n",
            "Batch: 3761    Epoch[3/3]    Training Loss: 0.7451369762420654\n",
            "Batch: 3762    Epoch[3/3]    Training Loss: 1.0952742099761963\n",
            "Batch: 3763    Epoch[3/3]    Training Loss: 0.8738375902175903\n",
            "Batch: 3764    Epoch[3/3]    Training Loss: 0.44967469573020935\n",
            "Batch: 3765    Epoch[3/3]    Training Loss: 0.6248646974563599\n",
            "Batch: 3766    Epoch[3/3]    Training Loss: 0.6467786431312561\n",
            "Batch: 3767    Epoch[3/3]    Training Loss: 0.3735416531562805\n",
            "Batch: 3768    Epoch[3/3]    Training Loss: 0.568843424320221\n",
            "Batch: 3769    Epoch[3/3]    Training Loss: 0.40103113651275635\n",
            "Batch: 3770    Epoch[3/3]    Training Loss: 0.8481063842773438\n",
            "Batch: 3771    Epoch[3/3]    Training Loss: 1.0281147956848145\n",
            "Batch: 3772    Epoch[3/3]    Training Loss: 0.25601547956466675\n",
            "Batch: 3773    Epoch[3/3]    Training Loss: 0.25127285718917847\n",
            "Batch: 3774    Epoch[3/3]    Training Loss: 0.8568728566169739\n",
            "Batch: 3775    Epoch[3/3]    Training Loss: 0.565317690372467\n",
            "Batch: 3776    Epoch[3/3]    Training Loss: 0.8126999139785767\n",
            "Batch: 3777    Epoch[3/3]    Training Loss: 0.25474482774734497\n",
            "Batch: 3778    Epoch[3/3]    Training Loss: 0.5096432566642761\n",
            "Batch: 3779    Epoch[3/3]    Training Loss: 0.26807111501693726\n",
            "Batch: 3780    Epoch[3/3]    Training Loss: 0.707724928855896\n",
            "Batch: 3781    Epoch[3/3]    Training Loss: 0.5580959320068359\n",
            "Batch: 3782    Epoch[3/3]    Training Loss: 0.19964610040187836\n",
            "Batch: 3783    Epoch[3/3]    Training Loss: 0.8050223588943481\n",
            "Batch: 3784    Epoch[3/3]    Training Loss: 0.9833744168281555\n",
            "Batch: 3785    Epoch[3/3]    Training Loss: 0.7164968848228455\n",
            "Batch: 3786    Epoch[3/3]    Training Loss: 1.4237802028656006\n",
            "Batch: 3787    Epoch[3/3]    Training Loss: 0.7022708058357239\n",
            "Batch: 3788    Epoch[3/3]    Training Loss: 0.7253411412239075\n",
            "Batch: 3789    Epoch[3/3]    Training Loss: 1.0324057340621948\n",
            "Batch: 3790    Epoch[3/3]    Training Loss: 0.6959659457206726\n",
            "Batch: 3791    Epoch[3/3]    Training Loss: 0.41390830278396606\n",
            "Batch: 3792    Epoch[3/3]    Training Loss: 0.8260492086410522\n",
            "Batch: 3793    Epoch[3/3]    Training Loss: 0.5886478424072266\n",
            "Batch: 3794    Epoch[3/3]    Training Loss: 0.5221045017242432\n",
            "Batch: 3795    Epoch[3/3]    Training Loss: 0.9697810411453247\n",
            "Batch: 3796    Epoch[3/3]    Training Loss: 0.5320249795913696\n",
            "Batch: 3797    Epoch[3/3]    Training Loss: 0.5894129872322083\n",
            "Batch: 3798    Epoch[3/3]    Training Loss: 0.9121288061141968\n",
            "Batch: 3799    Epoch[3/3]    Training Loss: 0.8299592733383179\n",
            "Batch: 3800    Epoch[3/3]    Training Loss: 0.7075294852256775\n",
            "Batch: 3801    Epoch[3/3]    Training Loss: 1.1347140073776245\n",
            "Batch: 3802    Epoch[3/3]    Training Loss: 1.4916318655014038\n",
            "Batch: 3803    Epoch[3/3]    Training Loss: 0.5563018918037415\n",
            "Batch: 3804    Epoch[3/3]    Training Loss: 0.44605547189712524\n",
            "Batch: 3805    Epoch[3/3]    Training Loss: 0.38322922587394714\n",
            "Batch: 3806    Epoch[3/3]    Training Loss: 0.3609939217567444\n",
            "Batch: 3807    Epoch[3/3]    Training Loss: 0.8268951773643494\n",
            "Batch: 3808    Epoch[3/3]    Training Loss: 0.7343334555625916\n",
            "Batch: 3809    Epoch[3/3]    Training Loss: 0.7752454876899719\n",
            "Batch: 3810    Epoch[3/3]    Training Loss: 0.5825576186180115\n",
            "Batch: 3811    Epoch[3/3]    Training Loss: 1.025637149810791\n",
            "Batch: 3812    Epoch[3/3]    Training Loss: 0.6270309090614319\n",
            "Batch: 3813    Epoch[3/3]    Training Loss: 0.4932006001472473\n",
            "Batch: 3814    Epoch[3/3]    Training Loss: 0.6570245027542114\n",
            "Batch: 3815    Epoch[3/3]    Training Loss: 0.6475381851196289\n",
            "Batch: 3816    Epoch[3/3]    Training Loss: 0.5333854556083679\n",
            "Batch: 3817    Epoch[3/3]    Training Loss: 0.4293050765991211\n",
            "Batch: 3818    Epoch[3/3]    Training Loss: 0.5060321688652039\n",
            "Batch: 3819    Epoch[3/3]    Training Loss: 0.40056824684143066\n",
            "Batch: 3820    Epoch[3/3]    Training Loss: 0.6524417996406555\n",
            "Batch: 3821    Epoch[3/3]    Training Loss: 0.4932190775871277\n",
            "Batch: 3822    Epoch[3/3]    Training Loss: 1.0465023517608643\n",
            "Batch: 3823    Epoch[3/3]    Training Loss: 0.7031291723251343\n",
            "Batch: 3824    Epoch[3/3]    Training Loss: 0.30253875255584717\n",
            "Batch: 3825    Epoch[3/3]    Training Loss: 0.5969017148017883\n",
            "Batch: 3826    Epoch[3/3]    Training Loss: 0.6764615774154663\n",
            "Batch: 3827    Epoch[3/3]    Training Loss: 0.6107037663459778\n",
            "Batch: 3828    Epoch[3/3]    Training Loss: 0.8009147644042969\n",
            "Batch: 3829    Epoch[3/3]    Training Loss: 0.8422300815582275\n",
            "Batch: 3830    Epoch[3/3]    Training Loss: 0.36799779534339905\n",
            "Batch: 3831    Epoch[3/3]    Training Loss: 0.3729691207408905\n",
            "Batch: 3832    Epoch[3/3]    Training Loss: 1.0091850757598877\n",
            "Batch: 3833    Epoch[3/3]    Training Loss: 0.6375691890716553\n",
            "Batch: 3834    Epoch[3/3]    Training Loss: 0.403523325920105\n",
            "Batch: 3835    Epoch[3/3]    Training Loss: 0.594772219657898\n",
            "Batch: 3836    Epoch[3/3]    Training Loss: 0.776573896408081\n",
            "Batch: 3837    Epoch[3/3]    Training Loss: 0.33145007491111755\n",
            "Batch: 3838    Epoch[3/3]    Training Loss: 0.3809851408004761\n",
            "Batch: 3839    Epoch[3/3]    Training Loss: 0.766298770904541\n",
            "Batch: 3840    Epoch[3/3]    Training Loss: 0.5102527141571045\n",
            "Batch: 3841    Epoch[3/3]    Training Loss: 0.9550296068191528\n",
            "Batch: 3842    Epoch[3/3]    Training Loss: 0.6367566585540771\n",
            "Batch: 3843    Epoch[3/3]    Training Loss: 0.9327864050865173\n",
            "Batch: 3844    Epoch[3/3]    Training Loss: 0.8359658718109131\n",
            "Batch: 3845    Epoch[3/3]    Training Loss: 0.23947042226791382\n",
            "Batch: 3846    Epoch[3/3]    Training Loss: 0.3991256356239319\n",
            "Batch: 3847    Epoch[3/3]    Training Loss: 1.0575215816497803\n",
            "Batch: 3848    Epoch[3/3]    Training Loss: 0.38273078203201294\n",
            "Batch: 3849    Epoch[3/3]    Training Loss: 0.4860200881958008\n",
            "Batch: 3850    Epoch[3/3]    Training Loss: 0.740888237953186\n",
            "Batch: 3851    Epoch[3/3]    Training Loss: 0.6527859568595886\n",
            "Batch: 3852    Epoch[3/3]    Training Loss: 0.5769103169441223\n",
            "Batch: 3853    Epoch[3/3]    Training Loss: 0.7019168138504028\n",
            "Batch: 3854    Epoch[3/3]    Training Loss: 0.7127611041069031\n",
            "Batch: 3855    Epoch[3/3]    Training Loss: 0.7227376103401184\n",
            "Batch: 3856    Epoch[3/3]    Training Loss: 0.6528734564781189\n",
            "Batch: 3857    Epoch[3/3]    Training Loss: 0.29504111409187317\n",
            "Batch: 3858    Epoch[3/3]    Training Loss: 0.7524067163467407\n",
            "Batch: 3859    Epoch[3/3]    Training Loss: 0.41275203227996826\n",
            "Batch: 3860    Epoch[3/3]    Training Loss: 0.568501889705658\n",
            "Batch: 3861    Epoch[3/3]    Training Loss: 0.34696483612060547\n",
            "Batch: 3862    Epoch[3/3]    Training Loss: 0.2997742295265198\n",
            "Batch: 3863    Epoch[3/3]    Training Loss: 0.549750804901123\n",
            "Batch: 3864    Epoch[3/3]    Training Loss: 0.6983402967453003\n",
            "Batch: 3865    Epoch[3/3]    Training Loss: 0.44180619716644287\n",
            "Batch: 3866    Epoch[3/3]    Training Loss: 0.43440863490104675\n",
            "Batch: 3867    Epoch[3/3]    Training Loss: 0.6474319100379944\n",
            "Batch: 3868    Epoch[3/3]    Training Loss: 0.8025718331336975\n",
            "Batch: 3869    Epoch[3/3]    Training Loss: 0.26556113362312317\n",
            "Batch: 3870    Epoch[3/3]    Training Loss: 0.8812386393547058\n",
            "Batch: 3871    Epoch[3/3]    Training Loss: 0.6667589545249939\n",
            "Batch: 3872    Epoch[3/3]    Training Loss: 1.0914994478225708\n",
            "Batch: 3873    Epoch[3/3]    Training Loss: 0.26759031414985657\n",
            "Batch: 3874    Epoch[3/3]    Training Loss: 0.4801417887210846\n",
            "Batch: 3875    Epoch[3/3]    Training Loss: 0.6731473207473755\n",
            "Batch: 3876    Epoch[3/3]    Training Loss: 0.5645381212234497\n",
            "Batch: 3877    Epoch[3/3]    Training Loss: 1.1880402565002441\n",
            "Batch: 3878    Epoch[3/3]    Training Loss: 0.6841535568237305\n",
            "Batch: 3879    Epoch[3/3]    Training Loss: 0.42395541071891785\n",
            "Batch: 3880    Epoch[3/3]    Training Loss: 0.46581918001174927\n",
            "Batch: 3881    Epoch[3/3]    Training Loss: 0.6276226043701172\n",
            "Batch: 3882    Epoch[3/3]    Training Loss: 0.5255216360092163\n",
            "Batch: 3883    Epoch[3/3]    Training Loss: 0.2912619709968567\n",
            "Batch: 3884    Epoch[3/3]    Training Loss: 0.8227958083152771\n",
            "Batch: 3885    Epoch[3/3]    Training Loss: 0.4357636570930481\n",
            "Batch: 3886    Epoch[3/3]    Training Loss: 0.3131304681301117\n",
            "Batch: 3887    Epoch[3/3]    Training Loss: 0.515853762626648\n",
            "Batch: 3888    Epoch[3/3]    Training Loss: 0.4234069883823395\n",
            "Batch: 3889    Epoch[3/3]    Training Loss: 0.9476984739303589\n",
            "Batch: 3890    Epoch[3/3]    Training Loss: 0.707986056804657\n",
            "Batch: 3891    Epoch[3/3]    Training Loss: 0.9927628040313721\n",
            "Batch: 3892    Epoch[3/3]    Training Loss: 0.5294703841209412\n",
            "Batch: 3893    Epoch[3/3]    Training Loss: 0.7514533400535583\n",
            "Batch: 3894    Epoch[3/3]    Training Loss: 0.4548400044441223\n",
            "Batch: 3895    Epoch[3/3]    Training Loss: 0.37223535776138306\n",
            "Batch: 3896    Epoch[3/3]    Training Loss: 0.7784386873245239\n",
            "Batch: 3897    Epoch[3/3]    Training Loss: 0.6741704940795898\n",
            "Batch: 3898    Epoch[3/3]    Training Loss: 0.6375437378883362\n",
            "Batch: 3899    Epoch[3/3]    Training Loss: 0.5187394618988037\n",
            "Batch: 3900    Epoch[3/3]    Training Loss: 0.6113365888595581\n",
            "Batch: 3901    Epoch[3/3]    Training Loss: 0.40634387731552124\n",
            "Batch: 3902    Epoch[3/3]    Training Loss: 0.520028829574585\n",
            "Batch: 3903    Epoch[3/3]    Training Loss: 0.7618584632873535\n",
            "Batch: 3904    Epoch[3/3]    Training Loss: 0.66405189037323\n",
            "Batch: 3905    Epoch[3/3]    Training Loss: 0.38076138496398926\n",
            "Batch: 3906    Epoch[3/3]    Training Loss: 0.746206521987915\n",
            "Batch: 3907    Epoch[3/3]    Training Loss: 0.5212656259536743\n",
            "Batch: 3908    Epoch[3/3]    Training Loss: 0.25558310747146606\n",
            "Batch: 3909    Epoch[3/3]    Training Loss: 0.4372982382774353\n",
            "Batch: 3910    Epoch[3/3]    Training Loss: 0.5528560876846313\n",
            "Batch: 3911    Epoch[3/3]    Training Loss: 0.40603387355804443\n",
            "Batch: 3912    Epoch[3/3]    Training Loss: 0.5357659459114075\n",
            "Batch: 3913    Epoch[3/3]    Training Loss: 0.44523781538009644\n",
            "Batch: 3914    Epoch[3/3]    Training Loss: 0.8462249040603638\n",
            "Batch: 3915    Epoch[3/3]    Training Loss: 0.550641655921936\n",
            "Batch: 3916    Epoch[3/3]    Training Loss: 0.621160626411438\n",
            "Batch: 3917    Epoch[3/3]    Training Loss: 0.7382227778434753\n",
            "Batch: 3918    Epoch[3/3]    Training Loss: 0.9505505561828613\n",
            "Batch: 3919    Epoch[3/3]    Training Loss: 0.32377299666404724\n",
            "Batch: 3920    Epoch[3/3]    Training Loss: 0.7474133968353271\n",
            "Batch: 3921    Epoch[3/3]    Training Loss: 0.6583738327026367\n",
            "Batch: 3922    Epoch[3/3]    Training Loss: 0.29759788513183594\n",
            "Batch: 3923    Epoch[3/3]    Training Loss: 0.8488636016845703\n",
            "Batch: 3924    Epoch[3/3]    Training Loss: 0.1735668033361435\n",
            "Batch: 3925    Epoch[3/3]    Training Loss: 0.24552465975284576\n",
            "Batch: 3926    Epoch[3/3]    Training Loss: 0.5289969444274902\n",
            "Batch: 3927    Epoch[3/3]    Training Loss: 0.6618496775627136\n",
            "Batch: 3928    Epoch[3/3]    Training Loss: 0.8029609322547913\n",
            "Batch: 3929    Epoch[3/3]    Training Loss: 0.8774456977844238\n",
            "Batch: 3930    Epoch[3/3]    Training Loss: 0.42516666650772095\n",
            "Batch: 3931    Epoch[3/3]    Training Loss: 0.5591427683830261\n",
            "Batch: 3932    Epoch[3/3]    Training Loss: 0.8338680267333984\n",
            "Batch: 3933    Epoch[3/3]    Training Loss: 0.37981656193733215\n",
            "Batch: 3934    Epoch[3/3]    Training Loss: 0.4716779887676239\n",
            "Batch: 3935    Epoch[3/3]    Training Loss: 0.5912601947784424\n",
            "Batch: 3936    Epoch[3/3]    Training Loss: 0.8599960803985596\n",
            "Batch: 3937    Epoch[3/3]    Training Loss: 0.5830919742584229\n",
            "Batch: 3938    Epoch[3/3]    Training Loss: 0.2986437678337097\n",
            "Batch: 3939    Epoch[3/3]    Training Loss: 1.4849896430969238\n",
            "Batch: 3940    Epoch[3/3]    Training Loss: 0.7798336744308472\n",
            "Batch: 3941    Epoch[3/3]    Training Loss: 0.4052983522415161\n",
            "Batch: 3942    Epoch[3/3]    Training Loss: 0.6194376349449158\n",
            "Batch: 3943    Epoch[3/3]    Training Loss: 0.43257927894592285\n",
            "Batch: 3944    Epoch[3/3]    Training Loss: 0.5834766626358032\n",
            "Batch: 3945    Epoch[3/3]    Training Loss: 0.7201688289642334\n",
            "Batch: 3946    Epoch[3/3]    Training Loss: 0.5028549432754517\n",
            "Batch: 3947    Epoch[3/3]    Training Loss: 0.5093511343002319\n",
            "Batch: 3948    Epoch[3/3]    Training Loss: 0.6237607598304749\n",
            "Batch: 3949    Epoch[3/3]    Training Loss: 0.8854921460151672\n",
            "Batch: 3950    Epoch[3/3]    Training Loss: 0.767876386642456\n",
            "Batch: 3951    Epoch[3/3]    Training Loss: 0.7903232574462891\n",
            "Batch: 3952    Epoch[3/3]    Training Loss: 0.9284415245056152\n",
            "Batch: 3953    Epoch[3/3]    Training Loss: 0.7128611207008362\n",
            "Batch: 3954    Epoch[3/3]    Training Loss: 0.45842522382736206\n",
            "Batch: 3955    Epoch[3/3]    Training Loss: 0.4832612872123718\n",
            "Batch: 3956    Epoch[3/3]    Training Loss: 0.476956307888031\n",
            "Batch: 3957    Epoch[3/3]    Training Loss: 0.23401273787021637\n",
            "Batch: 3958    Epoch[3/3]    Training Loss: 0.6176421642303467\n",
            "Batch: 3959    Epoch[3/3]    Training Loss: 0.33535003662109375\n",
            "Batch: 3960    Epoch[3/3]    Training Loss: 0.8107043504714966\n",
            "Batch: 3961    Epoch[3/3]    Training Loss: 0.25882619619369507\n",
            "Batch: 3962    Epoch[3/3]    Training Loss: 0.7427701950073242\n",
            "Batch: 3963    Epoch[3/3]    Training Loss: 0.8178572058677673\n",
            "Batch: 3964    Epoch[3/3]    Training Loss: 0.4482843279838562\n",
            "Batch: 3965    Epoch[3/3]    Training Loss: 0.21010826528072357\n",
            "Batch: 3966    Epoch[3/3]    Training Loss: 0.9688388109207153\n",
            "Batch: 3967    Epoch[3/3]    Training Loss: 0.4320588707923889\n",
            "Batch: 3968    Epoch[3/3]    Training Loss: 0.6505639553070068\n",
            "Batch: 3969    Epoch[3/3]    Training Loss: 0.7091916799545288\n",
            "Batch: 3970    Epoch[3/3]    Training Loss: 1.1155974864959717\n",
            "Batch: 3971    Epoch[3/3]    Training Loss: 0.8117716312408447\n",
            "Batch: 3972    Epoch[3/3]    Training Loss: 0.27561283111572266\n",
            "Batch: 3973    Epoch[3/3]    Training Loss: 0.43082141876220703\n",
            "Batch: 3974    Epoch[3/3]    Training Loss: 0.38284385204315186\n",
            "Batch: 3975    Epoch[3/3]    Training Loss: 0.5178838968276978\n",
            "Batch: 3976    Epoch[3/3]    Training Loss: 0.8271063566207886\n",
            "Batch: 3977    Epoch[3/3]    Training Loss: 0.5553188920021057\n",
            "Batch: 3978    Epoch[3/3]    Training Loss: 0.22761356830596924\n",
            "Batch: 3979    Epoch[3/3]    Training Loss: 0.6510010957717896\n",
            "Batch: 3980    Epoch[3/3]    Training Loss: 0.17961663007736206\n",
            "Batch: 3981    Epoch[3/3]    Training Loss: 0.8714406490325928\n",
            "Batch: 3982    Epoch[3/3]    Training Loss: 0.3193233013153076\n",
            "Batch: 3983    Epoch[3/3]    Training Loss: 0.44716930389404297\n",
            "Batch: 3984    Epoch[3/3]    Training Loss: 0.5968599915504456\n",
            "Batch: 3985    Epoch[3/3]    Training Loss: 0.429152250289917\n",
            "Batch: 3986    Epoch[3/3]    Training Loss: 0.8331024646759033\n",
            "Batch: 3987    Epoch[3/3]    Training Loss: 0.39141106605529785\n",
            "Batch: 3988    Epoch[3/3]    Training Loss: 0.6260184645652771\n",
            "Batch: 3989    Epoch[3/3]    Training Loss: 0.6118999123573303\n",
            "Batch: 3990    Epoch[3/3]    Training Loss: 0.5479740500450134\n",
            "Batch: 3991    Epoch[3/3]    Training Loss: 0.742397665977478\n",
            "Batch: 3992    Epoch[3/3]    Training Loss: 0.856002926826477\n",
            "Batch: 3993    Epoch[3/3]    Training Loss: 0.5103411674499512\n",
            "Batch: 3994    Epoch[3/3]    Training Loss: 0.67873615026474\n",
            "Batch: 3995    Epoch[3/3]    Training Loss: 0.3405569791793823\n",
            "Batch: 3996    Epoch[3/3]    Training Loss: 0.6835243105888367\n",
            "Batch: 3997    Epoch[3/3]    Training Loss: 0.5896091461181641\n",
            "Batch: 3998    Epoch[3/3]    Training Loss: 0.4426511526107788\n",
            "Batch: 3999    Epoch[3/3]    Training Loss: 0.7450292110443115\n",
            "Batch: 4000    Epoch[3/3]    Training Loss: 0.38015657663345337\n",
            "Batch: 4001    Epoch[3/3]    Training Loss: 0.7644972801208496\n",
            "Batch: 4002    Epoch[3/3]    Training Loss: 0.6953749656677246\n",
            "Batch: 4003    Epoch[3/3]    Training Loss: 0.9605600833892822\n",
            "Batch: 4004    Epoch[3/3]    Training Loss: 0.5712828040122986\n",
            "Batch: 4005    Epoch[3/3]    Training Loss: 0.38828444480895996\n",
            "Batch: 4006    Epoch[3/3]    Training Loss: 0.39325758814811707\n",
            "Batch: 4007    Epoch[3/3]    Training Loss: 0.32412099838256836\n",
            "Batch: 4008    Epoch[3/3]    Training Loss: 1.0133442878723145\n",
            "Batch: 4009    Epoch[3/3]    Training Loss: 0.39527466893196106\n",
            "Batch: 4010    Epoch[3/3]    Training Loss: 0.8413411974906921\n",
            "Batch: 4011    Epoch[3/3]    Training Loss: 1.0610556602478027\n",
            "Batch: 4012    Epoch[3/3]    Training Loss: 0.4510546624660492\n",
            "Batch: 4013    Epoch[3/3]    Training Loss: 0.9792863130569458\n",
            "Batch: 4014    Epoch[3/3]    Training Loss: 0.3168533444404602\n",
            "Batch: 4015    Epoch[3/3]    Training Loss: 1.3809858560562134\n",
            "Batch: 4016    Epoch[3/3]    Training Loss: 0.5760834813117981\n",
            "Batch: 4017    Epoch[3/3]    Training Loss: 0.5851072072982788\n",
            "Batch: 4018    Epoch[3/3]    Training Loss: 1.2727744579315186\n",
            "Batch: 4019    Epoch[3/3]    Training Loss: 0.4492126703262329\n",
            "Batch: 4020    Epoch[3/3]    Training Loss: 0.8836270570755005\n",
            "Batch: 4021    Epoch[3/3]    Training Loss: 0.5634598135948181\n",
            "Batch: 4022    Epoch[3/3]    Training Loss: 0.4025317430496216\n",
            "Batch: 4023    Epoch[3/3]    Training Loss: 0.29951271414756775\n",
            "Batch: 4024    Epoch[3/3]    Training Loss: 0.39393001794815063\n",
            "Batch: 4025    Epoch[3/3]    Training Loss: 0.2857540249824524\n",
            "Batch: 4026    Epoch[3/3]    Training Loss: 0.5826823711395264\n",
            "Batch: 4027    Epoch[3/3]    Training Loss: 0.3525170683860779\n",
            "Batch: 4028    Epoch[3/3]    Training Loss: 0.8366638422012329\n",
            "Batch: 4029    Epoch[3/3]    Training Loss: 0.6001241207122803\n",
            "Batch: 4030    Epoch[3/3]    Training Loss: 0.3784763514995575\n",
            "Batch: 4031    Epoch[3/3]    Training Loss: 0.857875645160675\n",
            "Batch: 4032    Epoch[3/3]    Training Loss: 0.702052116394043\n",
            "Batch: 4033    Epoch[3/3]    Training Loss: 1.0176217555999756\n",
            "Batch: 4034    Epoch[3/3]    Training Loss: 0.23072294890880585\n",
            "Batch: 4035    Epoch[3/3]    Training Loss: 0.4752787947654724\n",
            "Batch: 4036    Epoch[3/3]    Training Loss: 0.49286264181137085\n",
            "Batch: 4037    Epoch[3/3]    Training Loss: 0.6318384408950806\n",
            "Batch: 4038    Epoch[3/3]    Training Loss: 0.5456500053405762\n",
            "Batch: 4039    Epoch[3/3]    Training Loss: 0.6210991144180298\n",
            "Batch: 4040    Epoch[3/3]    Training Loss: 0.3389427661895752\n",
            "Batch: 4041    Epoch[3/3]    Training Loss: 0.293759822845459\n",
            "Batch: 4042    Epoch[3/3]    Training Loss: 0.4793355464935303\n",
            "Batch: 4043    Epoch[3/3]    Training Loss: 0.4643059968948364\n",
            "Batch: 4044    Epoch[3/3]    Training Loss: 0.4413638710975647\n",
            "Batch: 4045    Epoch[3/3]    Training Loss: 0.3519892692565918\n",
            "Batch: 4046    Epoch[3/3]    Training Loss: 0.4988933205604553\n",
            "Batch: 4047    Epoch[3/3]    Training Loss: 0.5094518065452576\n",
            "Batch: 4048    Epoch[3/3]    Training Loss: 0.9752989411354065\n",
            "Batch: 4049    Epoch[3/3]    Training Loss: 0.5599614977836609\n",
            "Batch: 4050    Epoch[3/3]    Training Loss: 0.12948253750801086\n",
            "Batch: 4051    Epoch[3/3]    Training Loss: 0.7333177328109741\n",
            "Batch: 4052    Epoch[3/3]    Training Loss: 0.5636457204818726\n",
            "Batch: 4053    Epoch[3/3]    Training Loss: 0.6508888006210327\n",
            "Batch: 4054    Epoch[3/3]    Training Loss: 0.4550500512123108\n",
            "Batch: 4055    Epoch[3/3]    Training Loss: 0.4540148973464966\n",
            "Batch: 4056    Epoch[3/3]    Training Loss: 0.844794511795044\n",
            "Batch: 4057    Epoch[3/3]    Training Loss: 0.7637943029403687\n",
            "Batch: 4058    Epoch[3/3]    Training Loss: 0.7063390016555786\n",
            "Batch: 4059    Epoch[3/3]    Training Loss: 1.4313035011291504\n",
            "Batch: 4060    Epoch[3/3]    Training Loss: 1.1504839658737183\n",
            "Batch: 4061    Epoch[3/3]    Training Loss: 1.05076265335083\n",
            "Batch: 4062    Epoch[3/3]    Training Loss: 0.12276588380336761\n",
            "Batch: 4063    Epoch[3/3]    Training Loss: 0.7030764818191528\n",
            "Batch: 4064    Epoch[3/3]    Training Loss: 0.5025782585144043\n",
            "Batch: 4065    Epoch[3/3]    Training Loss: 0.85447096824646\n",
            "Batch: 4066    Epoch[3/3]    Training Loss: 0.3918463885784149\n",
            "Batch: 4067    Epoch[3/3]    Training Loss: 0.756105899810791\n",
            "Batch: 4068    Epoch[3/3]    Training Loss: 0.9305712580680847\n",
            "Batch: 4069    Epoch[3/3]    Training Loss: 0.47508203983306885\n",
            "Batch: 4070    Epoch[3/3]    Training Loss: 0.4184790849685669\n",
            "Batch: 4071    Epoch[3/3]    Training Loss: 0.4593215882778168\n",
            "Batch: 4072    Epoch[3/3]    Training Loss: 0.5568432807922363\n",
            "Batch: 4073    Epoch[3/3]    Training Loss: 0.7790027856826782\n",
            "Batch: 4074    Epoch[3/3]    Training Loss: 0.4773121476173401\n",
            "Batch: 4075    Epoch[3/3]    Training Loss: 0.5817356705665588\n",
            "Batch: 4076    Epoch[3/3]    Training Loss: 0.6874837875366211\n",
            "Batch: 4077    Epoch[3/3]    Training Loss: 0.484144926071167\n",
            "Batch: 4078    Epoch[3/3]    Training Loss: 0.970137894153595\n",
            "Batch: 4079    Epoch[3/3]    Training Loss: 0.4199947118759155\n",
            "Batch: 4080    Epoch[3/3]    Training Loss: 0.6721692681312561\n",
            "Batch: 4081    Epoch[3/3]    Training Loss: 0.6358306407928467\n",
            "Batch: 4082    Epoch[3/3]    Training Loss: 0.8928470611572266\n",
            "Batch: 4083    Epoch[3/3]    Training Loss: 0.657733678817749\n",
            "Batch: 4084    Epoch[3/3]    Training Loss: 0.5387028455734253\n",
            "Batch: 4085    Epoch[3/3]    Training Loss: 0.9708764553070068\n",
            "Batch: 4086    Epoch[3/3]    Training Loss: 0.7059845924377441\n",
            "Batch: 4087    Epoch[3/3]    Training Loss: 0.5327390432357788\n",
            "Batch: 4088    Epoch[3/3]    Training Loss: 0.46915504336357117\n",
            "Batch: 4089    Epoch[3/3]    Training Loss: 0.7971287369728088\n",
            "Batch: 4090    Epoch[3/3]    Training Loss: 0.6328389048576355\n",
            "Batch: 4091    Epoch[3/3]    Training Loss: 0.7255260944366455\n",
            "Batch: 4092    Epoch[3/3]    Training Loss: 0.7273262143135071\n",
            "Batch: 4093    Epoch[3/3]    Training Loss: 0.3806055188179016\n",
            "Batch: 4094    Epoch[3/3]    Training Loss: 0.8200722932815552\n",
            "Batch: 4095    Epoch[3/3]    Training Loss: 0.2782633304595947\n",
            "Batch: 4096    Epoch[3/3]    Training Loss: 0.8247585296630859\n",
            "Batch: 4097    Epoch[3/3]    Training Loss: 0.5171554088592529\n",
            "Batch: 4098    Epoch[3/3]    Training Loss: 0.7530083656311035\n",
            "Batch: 4099    Epoch[3/3]    Training Loss: 0.5699378252029419\n",
            "Batch: 4100    Epoch[3/3]    Training Loss: 0.8901907205581665\n",
            "Batch: 4101    Epoch[3/3]    Training Loss: 0.6165952682495117\n",
            "Batch: 4102    Epoch[3/3]    Training Loss: 0.31836655735969543\n",
            "Batch: 4103    Epoch[3/3]    Training Loss: 0.44214797019958496\n",
            "Batch: 4104    Epoch[3/3]    Training Loss: 0.936410665512085\n",
            "Batch: 4105    Epoch[3/3]    Training Loss: 0.4442170560359955\n",
            "Batch: 4106    Epoch[3/3]    Training Loss: 0.7579900622367859\n",
            "Batch: 4107    Epoch[3/3]    Training Loss: 0.29836052656173706\n",
            "Batch: 4108    Epoch[3/3]    Training Loss: 0.5311477184295654\n",
            "Batch: 4109    Epoch[3/3]    Training Loss: 0.31131088733673096\n",
            "Batch: 4110    Epoch[3/3]    Training Loss: 0.6353249549865723\n",
            "Batch: 4111    Epoch[3/3]    Training Loss: 1.4899667501449585\n",
            "Batch: 4112    Epoch[3/3]    Training Loss: 0.5288705825805664\n",
            "Batch: 4113    Epoch[3/3]    Training Loss: 0.573670506477356\n",
            "Batch: 4114    Epoch[3/3]    Training Loss: 0.4475212097167969\n",
            "Batch: 4115    Epoch[3/3]    Training Loss: 0.6130655407905579\n",
            "Batch: 4116    Epoch[3/3]    Training Loss: 0.4722825288772583\n",
            "Batch: 4117    Epoch[3/3]    Training Loss: 1.3139749765396118\n",
            "Batch: 4118    Epoch[3/3]    Training Loss: 0.6444510817527771\n",
            "Batch: 4119    Epoch[3/3]    Training Loss: 0.4989866018295288\n",
            "Batch: 4120    Epoch[3/3]    Training Loss: 0.30808958411216736\n",
            "Batch: 4121    Epoch[3/3]    Training Loss: 1.0923340320587158\n",
            "Batch: 4122    Epoch[3/3]    Training Loss: 0.4484044313430786\n",
            "Batch: 4123    Epoch[3/3]    Training Loss: 0.8577656745910645\n",
            "Batch: 4124    Epoch[3/3]    Training Loss: 0.2521056830883026\n",
            "Batch: 4125    Epoch[3/3]    Training Loss: 0.6200689077377319\n",
            "Batch: 4126    Epoch[3/3]    Training Loss: 1.113135814666748\n",
            "Batch: 4127    Epoch[3/3]    Training Loss: 0.42668280005455017\n",
            "Batch: 4128    Epoch[3/3]    Training Loss: 0.6033830642700195\n",
            "Batch: 4129    Epoch[3/3]    Training Loss: 0.8883880972862244\n",
            "Batch: 4130    Epoch[3/3]    Training Loss: 0.3628830909729004\n",
            "Batch: 4131    Epoch[3/3]    Training Loss: 0.6407536268234253\n",
            "Batch: 4132    Epoch[3/3]    Training Loss: 0.2222984880208969\n",
            "Batch: 4133    Epoch[3/3]    Training Loss: 0.5914288759231567\n",
            "Batch: 4134    Epoch[3/3]    Training Loss: 0.4786562919616699\n",
            "Batch: 4135    Epoch[3/3]    Training Loss: 0.48933371901512146\n",
            "Batch: 4136    Epoch[3/3]    Training Loss: 0.4331669211387634\n",
            "Batch: 4137    Epoch[3/3]    Training Loss: 0.5710982084274292\n",
            "Batch: 4138    Epoch[3/3]    Training Loss: 0.8457427024841309\n",
            "Batch: 4139    Epoch[3/3]    Training Loss: 0.42618846893310547\n",
            "Batch: 4140    Epoch[3/3]    Training Loss: 0.4169800281524658\n",
            "Batch: 4141    Epoch[3/3]    Training Loss: 0.6932673454284668\n",
            "Batch: 4142    Epoch[3/3]    Training Loss: 0.42187708616256714\n",
            "Batch: 4143    Epoch[3/3]    Training Loss: 0.8616905808448792\n",
            "Batch: 4144    Epoch[3/3]    Training Loss: 0.9751524925231934\n",
            "Batch: 4145    Epoch[3/3]    Training Loss: 0.29113197326660156\n",
            "Batch: 4146    Epoch[3/3]    Training Loss: 0.22823098301887512\n",
            "Batch: 4147    Epoch[3/3]    Training Loss: 0.6492412686347961\n",
            "Batch: 4148    Epoch[3/3]    Training Loss: 0.46352601051330566\n",
            "Batch: 4149    Epoch[3/3]    Training Loss: 0.23965442180633545\n",
            "Batch: 4150    Epoch[3/3]    Training Loss: 0.2925637364387512\n",
            "Batch: 4151    Epoch[3/3]    Training Loss: 0.37000083923339844\n",
            "Batch: 4152    Epoch[3/3]    Training Loss: 0.2162158042192459\n",
            "Batch: 4153    Epoch[3/3]    Training Loss: 0.4594101905822754\n",
            "Batch: 4154    Epoch[3/3]    Training Loss: 0.49627524614334106\n",
            "Batch: 4155    Epoch[3/3]    Training Loss: 0.8390573263168335\n",
            "Batch: 4156    Epoch[3/3]    Training Loss: 0.8501413464546204\n",
            "Batch: 4157    Epoch[3/3]    Training Loss: 0.7323977947235107\n",
            "Batch: 4158    Epoch[3/3]    Training Loss: 1.0453031063079834\n",
            "Batch: 4159    Epoch[3/3]    Training Loss: 0.6988097429275513\n",
            "Batch: 4160    Epoch[3/3]    Training Loss: 0.2800900936126709\n",
            "Batch: 4161    Epoch[3/3]    Training Loss: 0.5926254987716675\n",
            "Batch: 4162    Epoch[3/3]    Training Loss: 0.7105141878128052\n",
            "Batch: 4163    Epoch[3/3]    Training Loss: 0.6990414261817932\n",
            "Batch: 4164    Epoch[3/3]    Training Loss: 0.375324547290802\n",
            "Batch: 4165    Epoch[3/3]    Training Loss: 0.34863007068634033\n",
            "Batch: 4166    Epoch[3/3]    Training Loss: 0.11148159950971603\n",
            "Batch: 4167    Epoch[3/3]    Training Loss: 0.6248534917831421\n",
            "Batch: 4168    Epoch[3/3]    Training Loss: 0.6718396544456482\n",
            "Batch: 4169    Epoch[3/3]    Training Loss: 0.8441720008850098\n",
            "Batch: 4170    Epoch[3/3]    Training Loss: 0.9853854775428772\n",
            "Batch: 4171    Epoch[3/3]    Training Loss: 0.8495811820030212\n",
            "Batch: 4172    Epoch[3/3]    Training Loss: 0.5860657691955566\n",
            "Batch: 4173    Epoch[3/3]    Training Loss: 0.4443715810775757\n",
            "Batch: 4174    Epoch[3/3]    Training Loss: 0.21847431361675262\n",
            "Batch: 4175    Epoch[3/3]    Training Loss: 0.6718703508377075\n",
            "Batch: 4176    Epoch[3/3]    Training Loss: 0.6298940181732178\n",
            "Batch: 4177    Epoch[3/3]    Training Loss: 1.029669165611267\n",
            "Batch: 4178    Epoch[3/3]    Training Loss: 0.6018373370170593\n",
            "Batch: 4179    Epoch[3/3]    Training Loss: 0.5956832766532898\n",
            "Batch: 4180    Epoch[3/3]    Training Loss: 0.5381966829299927\n",
            "Batch: 4181    Epoch[3/3]    Training Loss: 0.7726566791534424\n",
            "Batch: 4182    Epoch[3/3]    Training Loss: 0.48417341709136963\n",
            "Batch: 4183    Epoch[3/3]    Training Loss: 0.8712406158447266\n",
            "Batch: 4184    Epoch[3/3]    Training Loss: 0.38266265392303467\n",
            "Batch: 4185    Epoch[3/3]    Training Loss: 0.35336053371429443\n",
            "Batch: 4186    Epoch[3/3]    Training Loss: 0.933552086353302\n",
            "Batch: 4187    Epoch[3/3]    Training Loss: 0.7412561178207397\n",
            "Batch: 4188    Epoch[3/3]    Training Loss: 0.36409127712249756\n",
            "Batch: 4189    Epoch[3/3]    Training Loss: 0.43087831139564514\n",
            "Batch: 4190    Epoch[3/3]    Training Loss: 1.3449723720550537\n",
            "Batch: 4191    Epoch[3/3]    Training Loss: 0.42718496918678284\n",
            "Batch: 4192    Epoch[3/3]    Training Loss: 0.8714020848274231\n",
            "Batch: 4193    Epoch[3/3]    Training Loss: 0.33679187297821045\n",
            "Batch: 4194    Epoch[3/3]    Training Loss: 1.0327515602111816\n",
            "Batch: 4195    Epoch[3/3]    Training Loss: 0.47076791524887085\n",
            "Batch: 4196    Epoch[3/3]    Training Loss: 0.9008532762527466\n",
            "Batch: 4197    Epoch[3/3]    Training Loss: 0.8345122933387756\n",
            "Batch: 4198    Epoch[3/3]    Training Loss: 0.3599835932254791\n",
            "Batch: 4199    Epoch[3/3]    Training Loss: 0.40580031275749207\n",
            "Batch: 4200    Epoch[3/3]    Training Loss: 0.41573280096054077\n",
            "Batch: 4201    Epoch[3/3]    Training Loss: 0.7394163012504578\n",
            "Batch: 4202    Epoch[3/3]    Training Loss: 0.5982652306556702\n",
            "Batch: 4203    Epoch[3/3]    Training Loss: 0.388212114572525\n",
            "Batch: 4204    Epoch[3/3]    Training Loss: 1.3498553037643433\n",
            "Batch: 4205    Epoch[3/3]    Training Loss: 0.31552785634994507\n",
            "Batch: 4206    Epoch[3/3]    Training Loss: 0.511229395866394\n",
            "Batch: 4207    Epoch[3/3]    Training Loss: 0.25724244117736816\n",
            "Batch: 4208    Epoch[3/3]    Training Loss: 0.37218135595321655\n",
            "Batch: 4209    Epoch[3/3]    Training Loss: 0.5330460071563721\n",
            "Batch: 4210    Epoch[3/3]    Training Loss: 0.7075543403625488\n",
            "Batch: 4211    Epoch[3/3]    Training Loss: 0.4595547318458557\n",
            "Batch: 4212    Epoch[3/3]    Training Loss: 0.44615116715431213\n",
            "Batch: 4213    Epoch[3/3]    Training Loss: 0.5328986644744873\n",
            "Batch: 4214    Epoch[3/3]    Training Loss: 0.7351408004760742\n",
            "Batch: 4215    Epoch[3/3]    Training Loss: 0.9877278804779053\n",
            "Batch: 4216    Epoch[3/3]    Training Loss: 0.5641023516654968\n",
            "Batch: 4217    Epoch[3/3]    Training Loss: 0.40779733657836914\n",
            "Batch: 4218    Epoch[3/3]    Training Loss: 0.4455028176307678\n",
            "Batch: 4219    Epoch[3/3]    Training Loss: 0.29696333408355713\n",
            "Batch: 4220    Epoch[3/3]    Training Loss: 0.6099154949188232\n",
            "Batch: 4221    Epoch[3/3]    Training Loss: 0.6499605178833008\n",
            "Batch: 4222    Epoch[3/3]    Training Loss: 0.20711630582809448\n",
            "Batch: 4223    Epoch[3/3]    Training Loss: 0.294261634349823\n",
            "Batch: 4224    Epoch[3/3]    Training Loss: 0.5793554782867432\n",
            "Batch: 4225    Epoch[3/3]    Training Loss: 1.1564970016479492\n",
            "Batch: 4226    Epoch[3/3]    Training Loss: 0.5889031887054443\n",
            "Batch: 4227    Epoch[3/3]    Training Loss: 0.9224561452865601\n",
            "Batch: 4228    Epoch[3/3]    Training Loss: 0.5558328628540039\n",
            "Batch: 4229    Epoch[3/3]    Training Loss: 0.7674636840820312\n",
            "Batch: 4230    Epoch[3/3]    Training Loss: 0.47155505418777466\n",
            "Batch: 4231    Epoch[3/3]    Training Loss: 0.2860628366470337\n",
            "Batch: 4232    Epoch[3/3]    Training Loss: 0.30129510164260864\n",
            "Batch: 4233    Epoch[3/3]    Training Loss: 0.6923506259918213\n",
            "Batch: 4234    Epoch[3/3]    Training Loss: 0.47524893283843994\n",
            "Batch: 4235    Epoch[3/3]    Training Loss: 0.5162650346755981\n",
            "Batch: 4236    Epoch[3/3]    Training Loss: 0.6320549249649048\n",
            "Batch: 4237    Epoch[3/3]    Training Loss: 0.4135144352912903\n",
            "Batch: 4238    Epoch[3/3]    Training Loss: 0.5981330871582031\n",
            "Batch: 4239    Epoch[3/3]    Training Loss: 0.9810634851455688\n",
            "Batch: 4240    Epoch[3/3]    Training Loss: 0.5889979600906372\n",
            "Batch: 4241    Epoch[3/3]    Training Loss: 0.6287648677825928\n",
            "Batch: 4242    Epoch[3/3]    Training Loss: 0.5711792707443237\n",
            "Batch: 4243    Epoch[3/3]    Training Loss: 0.8843367695808411\n",
            "Batch: 4244    Epoch[3/3]    Training Loss: 0.459421306848526\n",
            "Batch: 4245    Epoch[3/3]    Training Loss: 0.8359474539756775\n",
            "Batch: 4246    Epoch[3/3]    Training Loss: 0.40294337272644043\n",
            "Batch: 4247    Epoch[3/3]    Training Loss: 0.7476494908332825\n",
            "Batch: 4248    Epoch[3/3]    Training Loss: 0.667161226272583\n",
            "Batch: 4249    Epoch[3/3]    Training Loss: 0.632597029209137\n",
            "Batch: 4250    Epoch[3/3]    Training Loss: 0.5159819722175598\n",
            "Batch: 4251    Epoch[3/3]    Training Loss: 0.6194829344749451\n",
            "Batch: 4252    Epoch[3/3]    Training Loss: 0.7631253004074097\n",
            "Batch: 4253    Epoch[3/3]    Training Loss: 0.7883126735687256\n",
            "Batch: 4254    Epoch[3/3]    Training Loss: 1.0148125886917114\n",
            "Batch: 4255    Epoch[3/3]    Training Loss: 0.678320050239563\n",
            "Batch: 4256    Epoch[3/3]    Training Loss: 0.8859648704528809\n",
            "Batch: 4257    Epoch[3/3]    Training Loss: 0.4555894732475281\n",
            "Batch: 4258    Epoch[3/3]    Training Loss: 0.5610190629959106\n",
            "Batch: 4259    Epoch[3/3]    Training Loss: 0.4350421130657196\n",
            "Batch: 4260    Epoch[3/3]    Training Loss: 1.1323590278625488\n",
            "Batch: 4261    Epoch[3/3]    Training Loss: 0.6544687747955322\n",
            "Batch: 4262    Epoch[3/3]    Training Loss: 0.6585766673088074\n",
            "Batch: 4263    Epoch[3/3]    Training Loss: 0.8570445775985718\n",
            "Batch: 4264    Epoch[3/3]    Training Loss: 0.3701481223106384\n",
            "Batch: 4265    Epoch[3/3]    Training Loss: 0.37173891067504883\n",
            "Batch: 4266    Epoch[3/3]    Training Loss: 0.36800408363342285\n",
            "Batch: 4267    Epoch[3/3]    Training Loss: 0.8304027318954468\n",
            "Batch: 4268    Epoch[3/3]    Training Loss: 0.5957397818565369\n",
            "Batch: 4269    Epoch[3/3]    Training Loss: 0.7952553033828735\n",
            "Batch: 4270    Epoch[3/3]    Training Loss: 1.1550744771957397\n",
            "Batch: 4271    Epoch[3/3]    Training Loss: 0.6322248578071594\n",
            "Batch: 4272    Epoch[3/3]    Training Loss: 0.6625690460205078\n",
            "Batch: 4273    Epoch[3/3]    Training Loss: 0.23445388674736023\n",
            "Batch: 4274    Epoch[3/3]    Training Loss: 0.5213164687156677\n",
            "Batch: 4275    Epoch[3/3]    Training Loss: 0.6173428297042847\n",
            "Batch: 4276    Epoch[3/3]    Training Loss: 0.41093695163726807\n",
            "Batch: 4277    Epoch[3/3]    Training Loss: 0.5700459480285645\n",
            "Batch: 4278    Epoch[3/3]    Training Loss: 0.6968657970428467\n",
            "Batch: 4279    Epoch[3/3]    Training Loss: 0.23572802543640137\n",
            "Batch: 4280    Epoch[3/3]    Training Loss: 0.663080096244812\n",
            "Batch: 4281    Epoch[3/3]    Training Loss: 0.638524055480957\n",
            "Batch: 4282    Epoch[3/3]    Training Loss: 0.9916557669639587\n",
            "Batch: 4283    Epoch[3/3]    Training Loss: 0.22889775037765503\n",
            "Batch: 4284    Epoch[3/3]    Training Loss: 0.5176980495452881\n",
            "Batch: 4285    Epoch[3/3]    Training Loss: 1.0840612649917603\n",
            "Batch: 4286    Epoch[3/3]    Training Loss: 0.6748414635658264\n",
            "Batch: 4287    Epoch[3/3]    Training Loss: 0.8319178223609924\n",
            "Batch: 4288    Epoch[3/3]    Training Loss: 0.5983811616897583\n",
            "Batch: 4289    Epoch[3/3]    Training Loss: 0.6326479911804199\n",
            "Batch: 4290    Epoch[3/3]    Training Loss: 0.4771696925163269\n",
            "Batch: 4291    Epoch[3/3]    Training Loss: 0.2782309651374817\n",
            "Batch: 4292    Epoch[3/3]    Training Loss: 1.1675176620483398\n",
            "Batch: 4293    Epoch[3/3]    Training Loss: 0.6562990546226501\n",
            "Batch: 4294    Epoch[3/3]    Training Loss: 1.068892002105713\n",
            "Batch: 4295    Epoch[3/3]    Training Loss: 0.8293956518173218\n",
            "Batch: 4296    Epoch[3/3]    Training Loss: 0.736415445804596\n",
            "Batch: 4297    Epoch[3/3]    Training Loss: 0.4382440447807312\n",
            "Batch: 4298    Epoch[3/3]    Training Loss: 0.8636175394058228\n",
            "Batch: 4299    Epoch[3/3]    Training Loss: 0.5925812721252441\n",
            "Batch: 4300    Epoch[3/3]    Training Loss: 0.8821276426315308\n",
            "Batch: 4301    Epoch[3/3]    Training Loss: 0.7729127407073975\n",
            "Batch: 4302    Epoch[3/3]    Training Loss: 0.5216770172119141\n",
            "Batch: 4303    Epoch[3/3]    Training Loss: 0.5728572010993958\n",
            "Batch: 4304    Epoch[3/3]    Training Loss: 0.5023281574249268\n",
            "Batch: 4305    Epoch[3/3]    Training Loss: 0.6913002729415894\n",
            "Batch: 4306    Epoch[3/3]    Training Loss: 0.5548521876335144\n",
            "Batch: 4307    Epoch[3/3]    Training Loss: 0.5137250423431396\n",
            "Batch: 4308    Epoch[3/3]    Training Loss: 0.5833866596221924\n",
            "Batch: 4309    Epoch[3/3]    Training Loss: 0.6993718147277832\n",
            "Batch: 4310    Epoch[3/3]    Training Loss: 0.2707056403160095\n",
            "Batch: 4311    Epoch[3/3]    Training Loss: 0.724940299987793\n",
            "Batch: 4312    Epoch[3/3]    Training Loss: 0.3781314492225647\n",
            "Batch: 4313    Epoch[3/3]    Training Loss: 0.6389886140823364\n",
            "Batch: 4314    Epoch[3/3]    Training Loss: 0.6921582818031311\n",
            "Batch: 4315    Epoch[3/3]    Training Loss: 0.6151614189147949\n",
            "Batch: 4316    Epoch[3/3]    Training Loss: 0.5268405675888062\n",
            "Batch: 4317    Epoch[3/3]    Training Loss: 0.47314348816871643\n",
            "Batch: 4318    Epoch[3/3]    Training Loss: 0.31029266119003296\n",
            "Batch: 4319    Epoch[3/3]    Training Loss: 0.44909799098968506\n",
            "Batch: 4320    Epoch[3/3]    Training Loss: 0.6605572700500488\n",
            "Batch: 4321    Epoch[3/3]    Training Loss: 0.3349432051181793\n",
            "Batch: 4322    Epoch[3/3]    Training Loss: 0.44950249791145325\n",
            "Batch: 4323    Epoch[3/3]    Training Loss: 0.7759270668029785\n",
            "Batch: 4324    Epoch[3/3]    Training Loss: 1.1011018753051758\n",
            "Batch: 4325    Epoch[3/3]    Training Loss: 0.7257145643234253\n",
            "Batch: 4326    Epoch[3/3]    Training Loss: 0.9181891679763794\n",
            "Batch: 4327    Epoch[3/3]    Training Loss: 1.0247735977172852\n",
            "Batch: 4328    Epoch[3/3]    Training Loss: 0.5690441131591797\n",
            "Batch: 4329    Epoch[3/3]    Training Loss: 0.36482977867126465\n",
            "Batch: 4330    Epoch[3/3]    Training Loss: 0.9593085050582886\n",
            "Batch: 4331    Epoch[3/3]    Training Loss: 0.45589837431907654\n",
            "Batch: 4332    Epoch[3/3]    Training Loss: 0.36978358030319214\n",
            "Batch: 4333    Epoch[3/3]    Training Loss: 1.0231906175613403\n",
            "Batch: 4334    Epoch[3/3]    Training Loss: 1.2451874017715454\n",
            "Batch: 4335    Epoch[3/3]    Training Loss: 0.7970852851867676\n",
            "Batch: 4336    Epoch[3/3]    Training Loss: 0.2975112497806549\n",
            "Batch: 4337    Epoch[3/3]    Training Loss: 1.009413719177246\n",
            "Batch: 4338    Epoch[3/3]    Training Loss: 0.9280827641487122\n",
            "Batch: 4339    Epoch[3/3]    Training Loss: 0.43693670630455017\n",
            "Batch: 4340    Epoch[3/3]    Training Loss: 0.8650751113891602\n",
            "Batch: 4341    Epoch[3/3]    Training Loss: 0.8759875297546387\n",
            "Batch: 4342    Epoch[3/3]    Training Loss: 0.6352967023849487\n",
            "Batch: 4343    Epoch[3/3]    Training Loss: 0.37500178813934326\n",
            "Batch: 4344    Epoch[3/3]    Training Loss: 0.7018479108810425\n",
            "Batch: 4345    Epoch[3/3]    Training Loss: 0.26054662466049194\n",
            "Batch: 4346    Epoch[3/3]    Training Loss: 0.3058212399482727\n",
            "Batch: 4347    Epoch[3/3]    Training Loss: 1.0685936212539673\n",
            "Batch: 4348    Epoch[3/3]    Training Loss: 1.0085407495498657\n",
            "Batch: 4349    Epoch[3/3]    Training Loss: 0.7454462647438049\n",
            "Batch: 4350    Epoch[3/3]    Training Loss: 0.9423470497131348\n",
            "Batch: 4351    Epoch[3/3]    Training Loss: 0.7235949039459229\n",
            "Batch: 4352    Epoch[3/3]    Training Loss: 0.5437495708465576\n",
            "Batch: 4353    Epoch[3/3]    Training Loss: 0.986665666103363\n",
            "Batch: 4354    Epoch[3/3]    Training Loss: 1.0159558057785034\n",
            "Batch: 4355    Epoch[3/3]    Training Loss: 0.40707382559776306\n",
            "Batch: 4356    Epoch[3/3]    Training Loss: 0.8949902057647705\n",
            "Batch: 4357    Epoch[3/3]    Training Loss: 0.4906514883041382\n",
            "Batch: 4358    Epoch[3/3]    Training Loss: 0.5645143389701843\n",
            "Batch: 4359    Epoch[3/3]    Training Loss: 0.3998737931251526\n",
            "Batch: 4360    Epoch[3/3]    Training Loss: 0.8472158908843994\n",
            "Batch: 4361    Epoch[3/3]    Training Loss: 0.394356369972229\n",
            "Batch: 4362    Epoch[3/3]    Training Loss: 0.5928230285644531\n",
            "Batch: 4363    Epoch[3/3]    Training Loss: 0.13639187812805176\n",
            "Batch: 4364    Epoch[3/3]    Training Loss: 0.8021727800369263\n",
            "Batch: 4365    Epoch[3/3]    Training Loss: 0.5850600004196167\n",
            "Batch: 4366    Epoch[3/3]    Training Loss: 0.277648389339447\n",
            "Batch: 4367    Epoch[3/3]    Training Loss: 0.33594608306884766\n",
            "Batch: 4368    Epoch[3/3]    Training Loss: 0.6270202994346619\n",
            "Batch: 4369    Epoch[3/3]    Training Loss: 0.8912158012390137\n",
            "Batch: 4370    Epoch[3/3]    Training Loss: 0.45631152391433716\n",
            "Batch: 4371    Epoch[3/3]    Training Loss: 0.9382973313331604\n",
            "Batch: 4372    Epoch[3/3]    Training Loss: 0.5990493297576904\n",
            "Batch: 4373    Epoch[3/3]    Training Loss: 0.5606154203414917\n",
            "Batch: 4374    Epoch[3/3]    Training Loss: 0.4530404210090637\n",
            "Batch: 4375    Epoch[3/3]    Training Loss: 0.5585580468177795\n",
            "Batch: 4376    Epoch[3/3]    Training Loss: 0.6439933180809021\n",
            "Batch: 4377    Epoch[3/3]    Training Loss: 0.33767199516296387\n",
            "Batch: 4378    Epoch[3/3]    Training Loss: 0.3076736629009247\n",
            "Batch: 4379    Epoch[3/3]    Training Loss: 0.1842387169599533\n",
            "Batch: 4380    Epoch[3/3]    Training Loss: 0.34987643361091614\n",
            "Batch: 4381    Epoch[3/3]    Training Loss: 0.7519900798797607\n",
            "Batch: 4382    Epoch[3/3]    Training Loss: 0.5409488677978516\n",
            "Batch: 4383    Epoch[3/3]    Training Loss: 1.031731367111206\n",
            "Batch: 4384    Epoch[3/3]    Training Loss: 0.6633073091506958\n",
            "Batch: 4385    Epoch[3/3]    Training Loss: 0.7363724708557129\n",
            "Batch: 4386    Epoch[3/3]    Training Loss: 0.7893207669258118\n",
            "Batch: 4387    Epoch[3/3]    Training Loss: 0.7045669555664062\n",
            "Batch: 4388    Epoch[3/3]    Training Loss: 0.905768632888794\n",
            "Batch: 4389    Epoch[3/3]    Training Loss: 0.6788387894630432\n",
            "Batch: 4390    Epoch[3/3]    Training Loss: 0.5715964436531067\n",
            "Batch: 4391    Epoch[3/3]    Training Loss: 0.610568642616272\n",
            "Batch: 4392    Epoch[3/3]    Training Loss: 0.194502055644989\n",
            "Batch: 4393    Epoch[3/3]    Training Loss: 0.23974421620368958\n",
            "Batch: 4394    Epoch[3/3]    Training Loss: 0.5728738307952881\n",
            "Batch: 4395    Epoch[3/3]    Training Loss: 0.5180749893188477\n",
            "Batch: 4396    Epoch[3/3]    Training Loss: 0.19281791150569916\n",
            "Batch: 4397    Epoch[3/3]    Training Loss: 0.8789825439453125\n",
            "Batch: 4398    Epoch[3/3]    Training Loss: 1.1631355285644531\n",
            "Batch: 4399    Epoch[3/3]    Training Loss: 0.5844658613204956\n",
            "Batch: 4400    Epoch[3/3]    Training Loss: 0.4823629558086395\n",
            "Batch: 4401    Epoch[3/3]    Training Loss: 1.3827834129333496\n",
            "Batch: 4402    Epoch[3/3]    Training Loss: 0.6752680540084839\n",
            "Batch: 4403    Epoch[3/3]    Training Loss: 0.8191840052604675\n",
            "Batch: 4404    Epoch[3/3]    Training Loss: 0.6297038793563843\n",
            "Batch: 4405    Epoch[3/3]    Training Loss: 0.6801724433898926\n",
            "Batch: 4406    Epoch[3/3]    Training Loss: 1.107208490371704\n",
            "Batch: 4407    Epoch[3/3]    Training Loss: 0.6167594194412231\n",
            "Batch: 4408    Epoch[3/3]    Training Loss: 1.0000524520874023\n",
            "Batch: 4409    Epoch[3/3]    Training Loss: 0.6172599196434021\n",
            "Batch: 4410    Epoch[3/3]    Training Loss: 0.699650764465332\n",
            "Batch: 4411    Epoch[3/3]    Training Loss: 0.6496766805648804\n",
            "Batch: 4412    Epoch[3/3]    Training Loss: 0.8214899301528931\n",
            "Batch: 4413    Epoch[3/3]    Training Loss: 0.469227135181427\n",
            "Batch: 4414    Epoch[3/3]    Training Loss: 0.4832461476325989\n",
            "Batch: 4415    Epoch[3/3]    Training Loss: 0.6261376142501831\n",
            "Batch: 4416    Epoch[3/3]    Training Loss: 1.077512502670288\n",
            "Batch: 4417    Epoch[3/3]    Training Loss: 0.27855223417282104\n",
            "Batch: 4418    Epoch[3/3]    Training Loss: 0.8663376569747925\n",
            "Batch: 4419    Epoch[3/3]    Training Loss: 0.705487847328186\n",
            "Batch: 4420    Epoch[3/3]    Training Loss: 0.3382634222507477\n",
            "Batch: 4421    Epoch[3/3]    Training Loss: 0.44623303413391113\n",
            "Batch: 4422    Epoch[3/3]    Training Loss: 0.8350130915641785\n",
            "Batch: 4423    Epoch[3/3]    Training Loss: 0.1843269169330597\n",
            "Batch: 4424    Epoch[3/3]    Training Loss: 0.14084021747112274\n",
            "Batch: 4425    Epoch[3/3]    Training Loss: 0.6288630962371826\n",
            "Batch: 4426    Epoch[3/3]    Training Loss: 0.9226027727127075\n",
            "Batch: 4427    Epoch[3/3]    Training Loss: 0.7772767543792725\n",
            "Batch: 4428    Epoch[3/3]    Training Loss: 0.8092741370201111\n",
            "Batch: 4429    Epoch[3/3]    Training Loss: 0.7299535274505615\n",
            "Batch: 4430    Epoch[3/3]    Training Loss: 1.057685375213623\n",
            "Batch: 4431    Epoch[3/3]    Training Loss: 0.49012279510498047\n",
            "Batch: 4432    Epoch[3/3]    Training Loss: 0.40154731273651123\n",
            "Batch: 4433    Epoch[3/3]    Training Loss: 0.49019286036491394\n",
            "Batch: 4434    Epoch[3/3]    Training Loss: 1.1411702632904053\n",
            "Batch: 4435    Epoch[3/3]    Training Loss: 0.36101189255714417\n",
            "Batch: 4436    Epoch[3/3]    Training Loss: 0.8124358654022217\n",
            "Batch: 4437    Epoch[3/3]    Training Loss: 0.8951913118362427\n",
            "Batch: 4438    Epoch[3/3]    Training Loss: 0.41331660747528076\n",
            "Batch: 4439    Epoch[3/3]    Training Loss: 0.6273852586746216\n",
            "Batch: 4440    Epoch[3/3]    Training Loss: 0.1967865228652954\n",
            "Batch: 4441    Epoch[3/3]    Training Loss: 0.6674786806106567\n",
            "Batch: 4442    Epoch[3/3]    Training Loss: 0.5766975283622742\n",
            "Batch: 4443    Epoch[3/3]    Training Loss: 1.1859493255615234\n",
            "Batch: 4444    Epoch[3/3]    Training Loss: 0.324606329202652\n",
            "Batch: 4445    Epoch[3/3]    Training Loss: 0.6718156337738037\n",
            "Batch: 4446    Epoch[3/3]    Training Loss: 1.0033397674560547\n",
            "Batch: 4447    Epoch[3/3]    Training Loss: 0.4530119001865387\n",
            "Batch: 4448    Epoch[3/3]    Training Loss: 0.5937216877937317\n",
            "Batch: 4449    Epoch[3/3]    Training Loss: 0.6106467843055725\n",
            "Batch: 4450    Epoch[3/3]    Training Loss: 0.4135395884513855\n",
            "Batch: 4451    Epoch[3/3]    Training Loss: 1.0004124641418457\n",
            "Batch: 4452    Epoch[3/3]    Training Loss: 0.553651750087738\n",
            "Batch: 4453    Epoch[3/3]    Training Loss: 0.7801424264907837\n",
            "Batch: 4454    Epoch[3/3]    Training Loss: 0.6680178642272949\n",
            "Batch: 4455    Epoch[3/3]    Training Loss: 0.2762831449508667\n",
            "Batch: 4456    Epoch[3/3]    Training Loss: 1.000493049621582\n",
            "Batch: 4457    Epoch[3/3]    Training Loss: 0.7872021794319153\n",
            "Batch: 4458    Epoch[3/3]    Training Loss: 0.8241242170333862\n",
            "Batch: 4459    Epoch[3/3]    Training Loss: 0.3703504800796509\n",
            "Batch: 4460    Epoch[3/3]    Training Loss: 0.6094268560409546\n",
            "Batch: 4461    Epoch[3/3]    Training Loss: 0.3186458945274353\n",
            "Batch: 4462    Epoch[3/3]    Training Loss: 0.800788402557373\n",
            "Batch: 4463    Epoch[3/3]    Training Loss: 0.6821210980415344\n",
            "Batch: 4464    Epoch[3/3]    Training Loss: 0.6920117735862732\n",
            "Batch: 4465    Epoch[3/3]    Training Loss: 0.3767682909965515\n",
            "Batch: 4466    Epoch[3/3]    Training Loss: 0.8793560266494751\n",
            "Batch: 4467    Epoch[3/3]    Training Loss: 0.6773561239242554\n",
            "Batch: 4468    Epoch[3/3]    Training Loss: 0.3048403263092041\n",
            "Batch: 4469    Epoch[3/3]    Training Loss: 0.9878287315368652\n",
            "Batch: 4470    Epoch[3/3]    Training Loss: 1.048950433731079\n",
            "Batch: 4471    Epoch[3/3]    Training Loss: 0.3639020025730133\n",
            "Batch: 4472    Epoch[3/3]    Training Loss: 0.5919291973114014\n",
            "Batch: 4473    Epoch[3/3]    Training Loss: 0.4409274458885193\n",
            "Batch: 4474    Epoch[3/3]    Training Loss: 0.6769571900367737\n",
            "Batch: 4475    Epoch[3/3]    Training Loss: 0.6859533190727234\n",
            "Batch: 4476    Epoch[3/3]    Training Loss: 0.48189109563827515\n",
            "Batch: 4477    Epoch[3/3]    Training Loss: 0.6844639778137207\n",
            "Batch: 4478    Epoch[3/3]    Training Loss: 0.37517136335372925\n",
            "Batch: 4479    Epoch[3/3]    Training Loss: 0.6802551746368408\n",
            "Batch: 4480    Epoch[3/3]    Training Loss: 0.5315320491790771\n",
            "Batch: 4481    Epoch[3/3]    Training Loss: 0.2837011218070984\n",
            "Batch: 4482    Epoch[3/3]    Training Loss: 0.8379839062690735\n",
            "Batch: 4483    Epoch[3/3]    Training Loss: 0.35952144861221313\n",
            "Batch: 4484    Epoch[3/3]    Training Loss: 0.5784563422203064\n",
            "Batch: 4485    Epoch[3/3]    Training Loss: 0.529125452041626\n",
            "Batch: 4486    Epoch[3/3]    Training Loss: 0.4222971498966217\n",
            "Batch: 4487    Epoch[3/3]    Training Loss: 1.3821980953216553\n",
            "Batch: 4488    Epoch[3/3]    Training Loss: 0.5537411570549011\n",
            "Batch: 4489    Epoch[3/3]    Training Loss: 0.31338828802108765\n",
            "Batch: 4490    Epoch[3/3]    Training Loss: 0.6010532379150391\n",
            "Batch: 4491    Epoch[3/3]    Training Loss: 0.38372689485549927\n",
            "Batch: 4492    Epoch[3/3]    Training Loss: 0.8577686548233032\n",
            "Batch: 4493    Epoch[3/3]    Training Loss: 0.6500607132911682\n",
            "Batch: 4494    Epoch[3/3]    Training Loss: 0.27159085869789124\n",
            "Batch: 4495    Epoch[3/3]    Training Loss: 0.2740887999534607\n",
            "Batch: 4496    Epoch[3/3]    Training Loss: 0.25285953283309937\n",
            "Batch: 4497    Epoch[3/3]    Training Loss: 0.3067466616630554\n",
            "Batch: 4498    Epoch[3/3]    Training Loss: 0.5214086771011353\n",
            "Batch: 4499    Epoch[3/3]    Training Loss: 0.448111891746521\n",
            "Batch: 4500    Epoch[3/3]    Training Loss: 0.37191975116729736\n",
            "Batch: 4501    Epoch[3/3]    Training Loss: 1.2940642833709717\n",
            "Batch: 4502    Epoch[3/3]    Training Loss: 0.28517574071884155\n",
            "Batch: 4503    Epoch[3/3]    Training Loss: 0.31093621253967285\n",
            "Batch: 4504    Epoch[3/3]    Training Loss: 0.3875940442085266\n",
            "Batch: 4505    Epoch[3/3]    Training Loss: 0.30653518438339233\n",
            "Batch: 4506    Epoch[3/3]    Training Loss: 0.5937261581420898\n",
            "Batch: 4507    Epoch[3/3]    Training Loss: 0.3356492221355438\n",
            "Batch: 4508    Epoch[3/3]    Training Loss: 0.4193989336490631\n",
            "Batch: 4509    Epoch[3/3]    Training Loss: 0.9031694531440735\n",
            "Batch: 4510    Epoch[3/3]    Training Loss: 0.831824541091919\n",
            "Batch: 4511    Epoch[3/3]    Training Loss: 0.7365776300430298\n",
            "Batch: 4512    Epoch[3/3]    Training Loss: 0.6384261846542358\n",
            "Batch: 4513    Epoch[3/3]    Training Loss: 0.3703427314758301\n",
            "Batch: 4514    Epoch[3/3]    Training Loss: 0.453046977519989\n",
            "Batch: 4515    Epoch[3/3]    Training Loss: 0.5787853002548218\n",
            "Batch: 4516    Epoch[3/3]    Training Loss: 0.31830424070358276\n",
            "Batch: 4517    Epoch[3/3]    Training Loss: 0.8134936690330505\n",
            "Batch: 4518    Epoch[3/3]    Training Loss: 1.324918508529663\n",
            "Batch: 4519    Epoch[3/3]    Training Loss: 0.751953661441803\n",
            "Batch: 4520    Epoch[3/3]    Training Loss: 0.4864787459373474\n",
            "Batch: 4521    Epoch[3/3]    Training Loss: 0.577799379825592\n",
            "Batch: 4522    Epoch[3/3]    Training Loss: 0.39255547523498535\n",
            "Batch: 4523    Epoch[3/3]    Training Loss: 0.24884098768234253\n",
            "Batch: 4524    Epoch[3/3]    Training Loss: 0.39998868107795715\n",
            "Batch: 4525    Epoch[3/3]    Training Loss: 0.5769473314285278\n",
            "Batch: 4526    Epoch[3/3]    Training Loss: 0.9249110221862793\n",
            "Batch: 4527    Epoch[3/3]    Training Loss: 0.8082611560821533\n",
            "Batch: 4528    Epoch[3/3]    Training Loss: 0.3425515294075012\n",
            "Batch: 4529    Epoch[3/3]    Training Loss: 0.39627695083618164\n",
            "Batch: 4530    Epoch[3/3]    Training Loss: 0.7651218175888062\n",
            "Batch: 4531    Epoch[3/3]    Training Loss: 0.5487135052680969\n",
            "Batch: 4532    Epoch[3/3]    Training Loss: 1.484762191772461\n",
            "Batch: 4533    Epoch[3/3]    Training Loss: 0.4367590844631195\n",
            "Batch: 4534    Epoch[3/3]    Training Loss: 0.6984306573867798\n",
            "Batch: 4535    Epoch[3/3]    Training Loss: 0.8856346607208252\n",
            "Batch: 4536    Epoch[3/3]    Training Loss: 0.9442988038063049\n",
            "Batch: 4537    Epoch[3/3]    Training Loss: 0.6626302599906921\n",
            "Batch: 4538    Epoch[3/3]    Training Loss: 0.24464933574199677\n",
            "Batch: 4539    Epoch[3/3]    Training Loss: 0.793408989906311\n",
            "Batch: 4540    Epoch[3/3]    Training Loss: 0.6604114770889282\n",
            "Batch: 4541    Epoch[3/3]    Training Loss: 0.3560972809791565\n",
            "Batch: 4542    Epoch[3/3]    Training Loss: 0.6081362962722778\n",
            "Batch: 4543    Epoch[3/3]    Training Loss: 0.5034782290458679\n",
            "Batch: 4544    Epoch[3/3]    Training Loss: 0.457866370677948\n",
            "Batch: 4545    Epoch[3/3]    Training Loss: 0.6460381746292114\n",
            "Batch: 4546    Epoch[3/3]    Training Loss: 0.6841093301773071\n",
            "Batch: 4547    Epoch[3/3]    Training Loss: 0.17353162169456482\n",
            "Batch: 4548    Epoch[3/3]    Training Loss: 0.3217464089393616\n",
            "Batch: 4549    Epoch[3/3]    Training Loss: 0.4775692820549011\n",
            "Batch: 4550    Epoch[3/3]    Training Loss: 0.6432515382766724\n",
            "Batch: 4551    Epoch[3/3]    Training Loss: 0.7094067335128784\n",
            "Batch: 4552    Epoch[3/3]    Training Loss: 0.44562214612960815\n",
            "Batch: 4553    Epoch[3/3]    Training Loss: 0.3436622619628906\n",
            "Batch: 4554    Epoch[3/3]    Training Loss: 0.5351923704147339\n",
            "Batch: 4555    Epoch[3/3]    Training Loss: 1.0385017395019531\n",
            "Batch: 4556    Epoch[3/3]    Training Loss: 0.567568302154541\n",
            "Batch: 4557    Epoch[3/3]    Training Loss: 0.4490782618522644\n",
            "Batch: 4558    Epoch[3/3]    Training Loss: 0.5860781073570251\n",
            "Batch: 4559    Epoch[3/3]    Training Loss: 0.7015954852104187\n",
            "Batch: 4560    Epoch[3/3]    Training Loss: 0.5302532911300659\n",
            "Batch: 4561    Epoch[3/3]    Training Loss: 0.8709248304367065\n",
            "Batch: 4562    Epoch[3/3]    Training Loss: 0.6366708278656006\n",
            "Batch: 4563    Epoch[3/3]    Training Loss: 0.6186659336090088\n",
            "Batch: 4564    Epoch[3/3]    Training Loss: 0.31888818740844727\n",
            "Batch: 4565    Epoch[3/3]    Training Loss: 1.2803592681884766\n",
            "Batch: 4566    Epoch[3/3]    Training Loss: 0.8361901044845581\n",
            "Batch: 4567    Epoch[3/3]    Training Loss: 0.8680508732795715\n",
            "Batch: 4568    Epoch[3/3]    Training Loss: 0.8479759693145752\n",
            "Batch: 4569    Epoch[3/3]    Training Loss: 0.31084659695625305\n",
            "Batch: 4570    Epoch[3/3]    Training Loss: 0.4474000334739685\n",
            "Batch: 4571    Epoch[3/3]    Training Loss: 0.41739287972450256\n",
            "Batch: 4572    Epoch[3/3]    Training Loss: 0.7050153613090515\n",
            "Batch: 4573    Epoch[3/3]    Training Loss: 0.5215169787406921\n",
            "Batch: 4574    Epoch[3/3]    Training Loss: 0.5259487628936768\n",
            "Batch: 4575    Epoch[3/3]    Training Loss: 0.7066956758499146\n",
            "Batch: 4576    Epoch[3/3]    Training Loss: 0.5110896825790405\n",
            "Batch: 4577    Epoch[3/3]    Training Loss: 0.6325131058692932\n",
            "Batch: 4578    Epoch[3/3]    Training Loss: 1.0129425525665283\n",
            "Batch: 4579    Epoch[3/3]    Training Loss: 1.0212576389312744\n",
            "Batch: 4580    Epoch[3/3]    Training Loss: 0.7243590354919434\n",
            "Batch: 4581    Epoch[3/3]    Training Loss: 0.778933048248291\n",
            "Batch: 4582    Epoch[3/3]    Training Loss: 0.5023913383483887\n",
            "Batch: 4583    Epoch[3/3]    Training Loss: 0.7196285724639893\n",
            "Batch: 4584    Epoch[3/3]    Training Loss: 0.6697021722793579\n",
            "Batch: 4585    Epoch[3/3]    Training Loss: 0.5466454029083252\n",
            "Batch: 4586    Epoch[3/3]    Training Loss: 0.3706575036048889\n",
            "Batch: 4587    Epoch[3/3]    Training Loss: 0.27128148078918457\n",
            "Batch: 4588    Epoch[3/3]    Training Loss: 0.5232902765274048\n",
            "Batch: 4589    Epoch[3/3]    Training Loss: 0.6518110632896423\n",
            "Batch: 4590    Epoch[3/3]    Training Loss: 0.4354984164237976\n",
            "Batch: 4591    Epoch[3/3]    Training Loss: 0.25786659121513367\n",
            "Batch: 4592    Epoch[3/3]    Training Loss: 0.46274930238723755\n",
            "Batch: 4593    Epoch[3/3]    Training Loss: 0.27597886323928833\n",
            "Batch: 4594    Epoch[3/3]    Training Loss: 0.6556717753410339\n",
            "Batch: 4595    Epoch[3/3]    Training Loss: 0.5428338050842285\n",
            "Batch: 4596    Epoch[3/3]    Training Loss: 0.4612031579017639\n",
            "Batch: 4597    Epoch[3/3]    Training Loss: 0.6114407777786255\n",
            "Batch: 4598    Epoch[3/3]    Training Loss: 0.6992963552474976\n",
            "Batch: 4599    Epoch[3/3]    Training Loss: 1.0110745429992676\n",
            "Batch: 4600    Epoch[3/3]    Training Loss: 0.4909331202507019\n",
            "Batch: 4601    Epoch[3/3]    Training Loss: 0.4242454767227173\n",
            "Batch: 4602    Epoch[3/3]    Training Loss: 0.6565248966217041\n",
            "Batch: 4603    Epoch[3/3]    Training Loss: 1.0173043012619019\n",
            "Batch: 4604    Epoch[3/3]    Training Loss: 0.5902180671691895\n",
            "Batch: 4605    Epoch[3/3]    Training Loss: 0.36862123012542725\n",
            "Batch: 4606    Epoch[3/3]    Training Loss: 1.0504196882247925\n",
            "Batch: 4607    Epoch[3/3]    Training Loss: 0.420093834400177\n",
            "Batch: 4608    Epoch[3/3]    Training Loss: 0.85495924949646\n",
            "Batch: 4609    Epoch[3/3]    Training Loss: 0.6418580412864685\n",
            "Batch: 4610    Epoch[3/3]    Training Loss: 0.9434330463409424\n",
            "Batch: 4611    Epoch[3/3]    Training Loss: 0.9442594051361084\n",
            "Batch: 4612    Epoch[3/3]    Training Loss: 0.3111908733844757\n",
            "Batch: 4613    Epoch[3/3]    Training Loss: 0.35942286252975464\n",
            "Batch: 4614    Epoch[3/3]    Training Loss: 0.3982729911804199\n",
            "Batch: 4615    Epoch[3/3]    Training Loss: 0.5428845286369324\n",
            "Batch: 4616    Epoch[3/3]    Training Loss: 0.3572266101837158\n",
            "Batch: 4617    Epoch[3/3]    Training Loss: 0.7660567760467529\n",
            "Batch: 4618    Epoch[3/3]    Training Loss: 0.7733765244483948\n",
            "Batch: 4619    Epoch[3/3]    Training Loss: 0.45190301537513733\n",
            "Batch: 4620    Epoch[3/3]    Training Loss: 0.36287254095077515\n",
            "Batch: 4621    Epoch[3/3]    Training Loss: 1.2086496353149414\n",
            "Batch: 4622    Epoch[3/3]    Training Loss: 0.7715479731559753\n",
            "Batch: 4623    Epoch[3/3]    Training Loss: 0.6580634713172913\n",
            "Batch: 4624    Epoch[3/3]    Training Loss: 0.822411060333252\n",
            "Batch: 4625    Epoch[3/3]    Training Loss: 0.5347945690155029\n",
            "Batch: 4626    Epoch[3/3]    Training Loss: 0.662519633769989\n",
            "Batch: 4627    Epoch[3/3]    Training Loss: 0.5236024856567383\n",
            "Batch: 4628    Epoch[3/3]    Training Loss: 0.9129238724708557\n",
            "Batch: 4629    Epoch[3/3]    Training Loss: 0.7500559091567993\n",
            "Batch: 4630    Epoch[3/3]    Training Loss: 0.6574774980545044\n",
            "Batch: 4631    Epoch[3/3]    Training Loss: 0.623958945274353\n",
            "Batch: 4632    Epoch[3/3]    Training Loss: 0.3669748306274414\n",
            "Batch: 4633    Epoch[3/3]    Training Loss: 0.5447171330451965\n",
            "Batch: 4634    Epoch[3/3]    Training Loss: 0.7082253098487854\n",
            "Batch: 4635    Epoch[3/3]    Training Loss: 0.7124218940734863\n",
            "Batch: 4636    Epoch[3/3]    Training Loss: 1.4829661846160889\n",
            "Batch: 4637    Epoch[3/3]    Training Loss: 0.8326606750488281\n",
            "Batch: 4638    Epoch[3/3]    Training Loss: 0.7537146806716919\n",
            "Batch: 4639    Epoch[3/3]    Training Loss: 0.8254349827766418\n",
            "Batch: 4640    Epoch[3/3]    Training Loss: 0.7581183910369873\n",
            "Batch: 4641    Epoch[3/3]    Training Loss: 0.9371812343597412\n",
            "Batch: 4642    Epoch[3/3]    Training Loss: 0.6834201216697693\n",
            "Batch: 4643    Epoch[3/3]    Training Loss: 1.048747181892395\n",
            "Batch: 4644    Epoch[3/3]    Training Loss: 0.3182078003883362\n",
            "Batch: 4645    Epoch[3/3]    Training Loss: 1.0471429824829102\n",
            "Batch: 4646    Epoch[3/3]    Training Loss: 0.5077118873596191\n",
            "Batch: 4647    Epoch[3/3]    Training Loss: 0.8598140478134155\n",
            "Batch: 4648    Epoch[3/3]    Training Loss: 0.3836725354194641\n",
            "Batch: 4649    Epoch[3/3]    Training Loss: 0.5116851925849915\n",
            "Batch: 4650    Epoch[3/3]    Training Loss: 0.6120083332061768\n",
            "Batch: 4651    Epoch[3/3]    Training Loss: 0.9139466285705566\n",
            "Batch: 4652    Epoch[3/3]    Training Loss: 0.7651962041854858\n",
            "Batch: 4653    Epoch[3/3]    Training Loss: 0.4470382034778595\n",
            "Batch: 4654    Epoch[3/3]    Training Loss: 0.48256897926330566\n",
            "Batch: 4655    Epoch[3/3]    Training Loss: 0.3046723008155823\n",
            "Batch: 4656    Epoch[3/3]    Training Loss: 0.6205725073814392\n",
            "Batch: 4657    Epoch[3/3]    Training Loss: 0.5153319239616394\n",
            "Batch: 4658    Epoch[3/3]    Training Loss: 0.5942156314849854\n",
            "Batch: 4659    Epoch[3/3]    Training Loss: 0.33811303973197937\n",
            "Batch: 4660    Epoch[3/3]    Training Loss: 0.37557095289230347\n",
            "Batch: 4661    Epoch[3/3]    Training Loss: 0.7857702374458313\n",
            "Batch: 4662    Epoch[3/3]    Training Loss: 1.0251307487487793\n",
            "Batch: 4663    Epoch[3/3]    Training Loss: 0.2884138822555542\n",
            "Batch: 4664    Epoch[3/3]    Training Loss: 0.673722505569458\n",
            "Batch: 4665    Epoch[3/3]    Training Loss: 0.9202209711074829\n",
            "Batch: 4666    Epoch[3/3]    Training Loss: 0.8564910292625427\n",
            "Batch: 4667    Epoch[3/3]    Training Loss: 0.8077472448348999\n",
            "Batch: 4668    Epoch[3/3]    Training Loss: 0.6263256669044495\n",
            "Batch: 4669    Epoch[3/3]    Training Loss: 0.5404372215270996\n",
            "Batch: 4670    Epoch[3/3]    Training Loss: 0.30530208349227905\n",
            "Batch: 4671    Epoch[3/3]    Training Loss: 0.6651391983032227\n",
            "Batch: 4672    Epoch[3/3]    Training Loss: 1.126030445098877\n",
            "Batch: 4673    Epoch[3/3]    Training Loss: 0.876600444316864\n",
            "Batch: 4674    Epoch[3/3]    Training Loss: 1.0909205675125122\n",
            "Batch: 4675    Epoch[3/3]    Training Loss: 0.9802980422973633\n",
            "Batch: 4676    Epoch[3/3]    Training Loss: 0.6932001113891602\n",
            "Batch: 4677    Epoch[3/3]    Training Loss: 0.5608282089233398\n",
            "Batch: 4678    Epoch[3/3]    Training Loss: 0.5586527585983276\n",
            "Batch: 4679    Epoch[3/3]    Training Loss: 0.3984145224094391\n",
            "Batch: 4680    Epoch[3/3]    Training Loss: 0.619772732257843\n",
            "Batch: 4681    Epoch[3/3]    Training Loss: 0.31166601181030273\n",
            "Batch: 4682    Epoch[3/3]    Training Loss: 0.37940916419029236\n",
            "Batch: 4683    Epoch[3/3]    Training Loss: 0.6125543117523193\n",
            "Batch: 4684    Epoch[3/3]    Training Loss: 1.3179746866226196\n",
            "Batch: 4685    Epoch[3/3]    Training Loss: 0.4704083502292633\n",
            "Batch: 4686    Epoch[3/3]    Training Loss: 0.3423556685447693\n",
            "Batch: 4687    Epoch[3/3]    Training Loss: 0.5663026571273804\n",
            "Batch: 4688    Epoch[3/3]    Training Loss: 0.26345282793045044\n",
            "Batch: 4689    Epoch[3/3]    Training Loss: 0.64313805103302\n",
            "Batch: 4690    Epoch[3/3]    Training Loss: 0.8666909337043762\n",
            "Batch: 4691    Epoch[3/3]    Training Loss: 0.8964616060256958\n",
            "Batch: 4692    Epoch[3/3]    Training Loss: 0.6875413656234741\n",
            "Batch: 4693    Epoch[3/3]    Training Loss: 0.3893725574016571\n",
            "Batch: 4694    Epoch[3/3]    Training Loss: 0.3052428066730499\n",
            "Batch: 4695    Epoch[3/3]    Training Loss: 1.0234498977661133\n",
            "Batch: 4696    Epoch[3/3]    Training Loss: 0.3905125558376312\n",
            "Batch: 4697    Epoch[3/3]    Training Loss: 1.1830952167510986\n",
            "Batch: 4698    Epoch[3/3]    Training Loss: 0.6876010894775391\n",
            "Batch: 4699    Epoch[3/3]    Training Loss: 0.8496171236038208\n",
            "Batch: 4700    Epoch[3/3]    Training Loss: 0.4577963650226593\n",
            "Batch: 4701    Epoch[3/3]    Training Loss: 0.4789436161518097\n",
            "Batch: 4702    Epoch[3/3]    Training Loss: 0.29171591997146606\n",
            "Batch: 4703    Epoch[3/3]    Training Loss: 0.4795699715614319\n",
            "Batch: 4704    Epoch[3/3]    Training Loss: 0.614404559135437\n",
            "Batch: 4705    Epoch[3/3]    Training Loss: 0.6485584378242493\n",
            "Batch: 4706    Epoch[3/3]    Training Loss: 0.6101429462432861\n",
            "Batch: 4707    Epoch[3/3]    Training Loss: 0.8024505376815796\n",
            "Batch: 4708    Epoch[3/3]    Training Loss: 0.8643550872802734\n",
            "Batch: 4709    Epoch[3/3]    Training Loss: 0.5666645765304565\n",
            "Batch: 4710    Epoch[3/3]    Training Loss: 0.7189081907272339\n",
            "Batch: 4711    Epoch[3/3]    Training Loss: 0.6974871158599854\n",
            "Batch: 4712    Epoch[3/3]    Training Loss: 0.29157403111457825\n",
            "Batch: 4713    Epoch[3/3]    Training Loss: 0.5299361944198608\n",
            "Batch: 4714    Epoch[3/3]    Training Loss: 0.6762540340423584\n",
            "Batch: 4715    Epoch[3/3]    Training Loss: 1.286255955696106\n",
            "Batch: 4716    Epoch[3/3]    Training Loss: 0.5238926410675049\n",
            "Batch: 4717    Epoch[3/3]    Training Loss: 0.777450680732727\n",
            "Batch: 4718    Epoch[3/3]    Training Loss: 0.7595354318618774\n",
            "Batch: 4719    Epoch[3/3]    Training Loss: 0.44193384051322937\n",
            "Batch: 4720    Epoch[3/3]    Training Loss: 0.25529658794403076\n",
            "Batch: 4721    Epoch[3/3]    Training Loss: 0.19763806462287903\n",
            "Batch: 4722    Epoch[3/3]    Training Loss: 1.1523616313934326\n",
            "Batch: 4723    Epoch[3/3]    Training Loss: 0.8391484022140503\n",
            "Batch: 4724    Epoch[3/3]    Training Loss: 0.29629969596862793\n",
            "Batch: 4725    Epoch[3/3]    Training Loss: 0.48944932222366333\n",
            "Batch: 4726    Epoch[3/3]    Training Loss: 0.5174444317817688\n",
            "Batch: 4727    Epoch[3/3]    Training Loss: 0.519460916519165\n",
            "Batch: 4728    Epoch[3/3]    Training Loss: 0.1588030457496643\n",
            "Batch: 4729    Epoch[3/3]    Training Loss: 1.4440524578094482\n",
            "Batch: 4730    Epoch[3/3]    Training Loss: 0.7448954582214355\n",
            "Batch: 4731    Epoch[3/3]    Training Loss: 0.8846490383148193\n",
            "Batch: 4732    Epoch[3/3]    Training Loss: 0.7007063627243042\n",
            "Batch: 4733    Epoch[3/3]    Training Loss: 0.7632862329483032\n",
            "Batch: 4734    Epoch[3/3]    Training Loss: 0.6522840261459351\n",
            "Batch: 4735    Epoch[3/3]    Training Loss: 0.8095307946205139\n",
            "Batch: 4736    Epoch[3/3]    Training Loss: 0.608905553817749\n",
            "Batch: 4737    Epoch[3/3]    Training Loss: 0.7558647394180298\n",
            "Batch: 4738    Epoch[3/3]    Training Loss: 0.8254882097244263\n",
            "Batch: 4739    Epoch[3/3]    Training Loss: 0.6336176991462708\n",
            "Batch: 4740    Epoch[3/3]    Training Loss: 0.46631625294685364\n",
            "Batch: 4741    Epoch[3/3]    Training Loss: 0.7160708904266357\n",
            "Batch: 4742    Epoch[3/3]    Training Loss: 0.9852826595306396\n",
            "Batch: 4743    Epoch[3/3]    Training Loss: 0.6808978319168091\n",
            "Batch: 4744    Epoch[3/3]    Training Loss: 0.3512645363807678\n",
            "Batch: 4745    Epoch[3/3]    Training Loss: 0.709941565990448\n",
            "Batch: 4746    Epoch[3/3]    Training Loss: 0.7855426073074341\n",
            "Batch: 4747    Epoch[3/3]    Training Loss: 0.6436206102371216\n",
            "Batch: 4748    Epoch[3/3]    Training Loss: 0.651429295539856\n",
            "Batch: 4749    Epoch[3/3]    Training Loss: 0.7745240926742554\n",
            "Batch: 4750    Epoch[3/3]    Training Loss: 0.5529177784919739\n",
            "Batch: 4751    Epoch[3/3]    Training Loss: 0.3823591470718384\n",
            "Batch: 4752    Epoch[3/3]    Training Loss: 0.5815112590789795\n",
            "Batch: 4753    Epoch[3/3]    Training Loss: 0.7119253277778625\n",
            "Batch: 4754    Epoch[3/3]    Training Loss: 0.6799740791320801\n",
            "Batch: 4755    Epoch[3/3]    Training Loss: 1.714289903640747\n",
            "Batch: 4756    Epoch[3/3]    Training Loss: 0.6163567304611206\n",
            "Batch: 4757    Epoch[3/3]    Training Loss: 0.35688287019729614\n",
            "Batch: 4758    Epoch[3/3]    Training Loss: 0.8581506013870239\n",
            "Batch: 4759    Epoch[3/3]    Training Loss: 0.553573727607727\n",
            "Batch: 4760    Epoch[3/3]    Training Loss: 0.42838239669799805\n",
            "Batch: 4761    Epoch[3/3]    Training Loss: 0.616640031337738\n",
            "Batch: 4762    Epoch[3/3]    Training Loss: 0.6667798757553101\n",
            "Batch: 4763    Epoch[3/3]    Training Loss: 0.9931919574737549\n",
            "Batch: 4764    Epoch[3/3]    Training Loss: 0.48241180181503296\n",
            "Batch: 4765    Epoch[3/3]    Training Loss: 0.8230770826339722\n",
            "Batch: 4766    Epoch[3/3]    Training Loss: 0.42076605558395386\n",
            "Batch: 4767    Epoch[3/3]    Training Loss: 0.4540015757083893\n",
            "Batch: 4768    Epoch[3/3]    Training Loss: 0.67848801612854\n",
            "Batch: 4769    Epoch[3/3]    Training Loss: 0.7130105495452881\n",
            "Batch: 4770    Epoch[3/3]    Training Loss: 0.4509959816932678\n",
            "Batch: 4771    Epoch[3/3]    Training Loss: 0.4867589473724365\n",
            "Batch: 4772    Epoch[3/3]    Training Loss: 0.6680071353912354\n",
            "Batch: 4773    Epoch[3/3]    Training Loss: 0.5234816670417786\n",
            "Batch: 4774    Epoch[3/3]    Training Loss: 0.9095862507820129\n",
            "Batch: 4775    Epoch[3/3]    Training Loss: 0.8200139999389648\n",
            "Batch: 4776    Epoch[3/3]    Training Loss: 0.3112022280693054\n",
            "Batch: 4777    Epoch[3/3]    Training Loss: 0.9637330770492554\n",
            "Batch: 4778    Epoch[3/3]    Training Loss: 0.3070378005504608\n",
            "Batch: 4779    Epoch[3/3]    Training Loss: 0.3028640151023865\n",
            "Batch: 4780    Epoch[3/3]    Training Loss: 0.32461512088775635\n",
            "Batch: 4781    Epoch[3/3]    Training Loss: 0.227708637714386\n",
            "Batch: 4782    Epoch[3/3]    Training Loss: 1.047299861907959\n",
            "Batch: 4783    Epoch[3/3]    Training Loss: 1.1708250045776367\n",
            "Batch: 4784    Epoch[3/3]    Training Loss: 0.7524361610412598\n",
            "Batch: 4785    Epoch[3/3]    Training Loss: 0.7857347130775452\n",
            "Batch: 4786    Epoch[3/3]    Training Loss: 0.2870390713214874\n",
            "Batch: 4787    Epoch[3/3]    Training Loss: 0.42104750871658325\n",
            "Batch: 4788    Epoch[3/3]    Training Loss: 0.5130113363265991\n",
            "Batch: 4789    Epoch[3/3]    Training Loss: 0.8203482627868652\n",
            "Batch: 4790    Epoch[3/3]    Training Loss: 0.5712903141975403\n",
            "Batch: 4791    Epoch[3/3]    Training Loss: 0.8134387731552124\n",
            "Batch: 4792    Epoch[3/3]    Training Loss: 0.5032774209976196\n",
            "Batch: 4793    Epoch[3/3]    Training Loss: 0.9369071125984192\n",
            "Batch: 4794    Epoch[3/3]    Training Loss: 0.6553124189376831\n",
            "Batch: 4795    Epoch[3/3]    Training Loss: 0.6356946229934692\n",
            "Batch: 4796    Epoch[3/3]    Training Loss: 0.4627389907836914\n",
            "Batch: 4797    Epoch[3/3]    Training Loss: 0.22991955280303955\n",
            "Batch: 4798    Epoch[3/3]    Training Loss: 0.433856338262558\n",
            "Batch: 4799    Epoch[3/3]    Training Loss: 0.5960443019866943\n",
            "Batch: 4800    Epoch[3/3]    Training Loss: 0.7260358333587646\n",
            "Batch: 4801    Epoch[3/3]    Training Loss: 0.9435359239578247\n",
            "Batch: 4802    Epoch[3/3]    Training Loss: 0.8797601461410522\n",
            "Batch: 4803    Epoch[3/3]    Training Loss: 1.000030517578125\n",
            "Batch: 4804    Epoch[3/3]    Training Loss: 0.9685670137405396\n",
            "Batch: 4805    Epoch[3/3]    Training Loss: 0.3646329343318939\n",
            "Batch: 4806    Epoch[3/3]    Training Loss: 1.0809383392333984\n",
            "Batch: 4807    Epoch[3/3]    Training Loss: 0.7026647329330444\n",
            "Batch: 4808    Epoch[3/3]    Training Loss: 0.4916972517967224\n",
            "Batch: 4809    Epoch[3/3]    Training Loss: 1.2531073093414307\n",
            "Batch: 4810    Epoch[3/3]    Training Loss: 0.5118571519851685\n",
            "Batch: 4811    Epoch[3/3]    Training Loss: 0.9701346158981323\n",
            "Batch: 4812    Epoch[3/3]    Training Loss: 0.46201223134994507\n",
            "Batch: 4813    Epoch[3/3]    Training Loss: 0.8754816055297852\n",
            "Batch: 4814    Epoch[3/3]    Training Loss: 0.56145840883255\n",
            "Batch: 4815    Epoch[3/3]    Training Loss: 0.545372486114502\n",
            "Batch: 4816    Epoch[3/3]    Training Loss: 0.32728952169418335\n",
            "Batch: 4817    Epoch[3/3]    Training Loss: 0.5084299445152283\n",
            "Batch: 4818    Epoch[3/3]    Training Loss: 0.45222944021224976\n",
            "Batch: 4819    Epoch[3/3]    Training Loss: 0.6666265726089478\n",
            "Batch: 4820    Epoch[3/3]    Training Loss: 0.5510208606719971\n",
            "Batch: 4821    Epoch[3/3]    Training Loss: 0.5270248651504517\n",
            "Batch: 4822    Epoch[3/3]    Training Loss: 0.524537205696106\n",
            "Batch: 4823    Epoch[3/3]    Training Loss: 0.8669102191925049\n",
            "Batch: 4824    Epoch[3/3]    Training Loss: 0.2817348539829254\n",
            "Batch: 4825    Epoch[3/3]    Training Loss: 0.25551456212997437\n",
            "Batch: 4826    Epoch[3/3]    Training Loss: 0.5033863186836243\n",
            "Batch: 4827    Epoch[3/3]    Training Loss: 0.535556972026825\n",
            "Batch: 4828    Epoch[3/3]    Training Loss: 0.7499959468841553\n",
            "Batch: 4829    Epoch[3/3]    Training Loss: 0.5747231245040894\n",
            "Batch: 4830    Epoch[3/3]    Training Loss: 0.8040592074394226\n",
            "Batch: 4831    Epoch[3/3]    Training Loss: 0.4108660817146301\n",
            "Batch: 4832    Epoch[3/3]    Training Loss: 0.5865175724029541\n",
            "Batch: 4833    Epoch[3/3]    Training Loss: 0.9319359660148621\n",
            "Batch: 4834    Epoch[3/3]    Training Loss: 0.4038901627063751\n",
            "Batch: 4835    Epoch[3/3]    Training Loss: 0.790651798248291\n",
            "Batch: 4836    Epoch[3/3]    Training Loss: 1.100357174873352\n",
            "Batch: 4837    Epoch[3/3]    Training Loss: 0.5885560512542725\n",
            "Batch: 4838    Epoch[3/3]    Training Loss: 0.6769027709960938\n",
            "Batch: 4839    Epoch[3/3]    Training Loss: 0.42971065640449524\n",
            "Batch: 4840    Epoch[3/3]    Training Loss: 0.616851270198822\n",
            "Batch: 4841    Epoch[3/3]    Training Loss: 0.5025907158851624\n",
            "Batch: 4842    Epoch[3/3]    Training Loss: 0.5580952167510986\n",
            "Batch: 4843    Epoch[3/3]    Training Loss: 0.39693260192871094\n",
            "Batch: 4844    Epoch[3/3]    Training Loss: 0.411583811044693\n",
            "Batch: 4845    Epoch[3/3]    Training Loss: 0.4205288290977478\n",
            "Batch: 4846    Epoch[3/3]    Training Loss: 0.6113768815994263\n",
            "Batch: 4847    Epoch[3/3]    Training Loss: 0.6325501203536987\n",
            "Batch: 4848    Epoch[3/3]    Training Loss: 1.4722797870635986\n",
            "Batch: 4849    Epoch[3/3]    Training Loss: 0.8545519113540649\n",
            "Batch: 4850    Epoch[3/3]    Training Loss: 0.7698755860328674\n",
            "Batch: 4851    Epoch[3/3]    Training Loss: 0.5922117233276367\n",
            "Batch: 4852    Epoch[3/3]    Training Loss: 0.2869718372821808\n",
            "Batch: 4853    Epoch[3/3]    Training Loss: 0.886793851852417\n",
            "Batch: 4854    Epoch[3/3]    Training Loss: 0.7473644018173218\n",
            "Batch: 4855    Epoch[3/3]    Training Loss: 0.4815630316734314\n",
            "Batch: 4856    Epoch[3/3]    Training Loss: 0.45758429169654846\n",
            "Batch: 4857    Epoch[3/3]    Training Loss: 0.6495854258537292\n",
            "Batch: 4858    Epoch[3/3]    Training Loss: 0.41351592540740967\n",
            "Batch: 4859    Epoch[3/3]    Training Loss: 0.8479413986206055\n",
            "Batch: 4860    Epoch[3/3]    Training Loss: 1.2625925540924072\n",
            "Batch: 4861    Epoch[3/3]    Training Loss: 0.43487516045570374\n",
            "Batch: 4862    Epoch[3/3]    Training Loss: 0.397797554731369\n",
            "Batch: 4863    Epoch[3/3]    Training Loss: 0.6331027150154114\n",
            "Batch: 4864    Epoch[3/3]    Training Loss: 0.28149691224098206\n",
            "Batch: 4865    Epoch[3/3]    Training Loss: 1.0312618017196655\n",
            "Batch: 4866    Epoch[3/3]    Training Loss: 0.5459733009338379\n",
            "Batch: 4867    Epoch[3/3]    Training Loss: 0.27534550428390503\n",
            "Batch: 4868    Epoch[3/3]    Training Loss: 0.4569770097732544\n",
            "Batch: 4869    Epoch[3/3]    Training Loss: 0.7622501850128174\n",
            "Batch: 4870    Epoch[3/3]    Training Loss: 0.6968148350715637\n",
            "Batch: 4871    Epoch[3/3]    Training Loss: 0.19868794083595276\n",
            "Batch: 4872    Epoch[3/3]    Training Loss: 0.7311264276504517\n",
            "Batch: 4873    Epoch[3/3]    Training Loss: 0.5240800976753235\n",
            "Batch: 4874    Epoch[3/3]    Training Loss: 0.7181264162063599\n",
            "Batch: 4875    Epoch[3/3]    Training Loss: 0.9490969181060791\n",
            "Batch: 4876    Epoch[3/3]    Training Loss: 0.9312620162963867\n",
            "Batch: 4877    Epoch[3/3]    Training Loss: 0.4265214502811432\n",
            "Batch: 4878    Epoch[3/3]    Training Loss: 0.64512038230896\n",
            "Batch: 4879    Epoch[3/3]    Training Loss: 0.8029414415359497\n",
            "Batch: 4880    Epoch[3/3]    Training Loss: 0.755530834197998\n",
            "Batch: 4881    Epoch[3/3]    Training Loss: 0.32337456941604614\n",
            "Batch: 4882    Epoch[3/3]    Training Loss: 0.48536157608032227\n",
            "Batch: 4883    Epoch[3/3]    Training Loss: 0.4802713394165039\n",
            "Batch: 4884    Epoch[3/3]    Training Loss: 1.0557583570480347\n",
            "Batch: 4885    Epoch[3/3]    Training Loss: 0.5659189820289612\n",
            "Batch: 4886    Epoch[3/3]    Training Loss: 0.25275471806526184\n",
            "Batch: 4887    Epoch[3/3]    Training Loss: 0.2715234160423279\n",
            "Batch: 4888    Epoch[3/3]    Training Loss: 0.9403426051139832\n",
            "Batch: 4889    Epoch[3/3]    Training Loss: 0.5851080417633057\n",
            "Batch: 4890    Epoch[3/3]    Training Loss: 0.6320732831954956\n",
            "Batch: 4891    Epoch[3/3]    Training Loss: 0.4956853687763214\n",
            "Batch: 4892    Epoch[3/3]    Training Loss: 0.7509540915489197\n",
            "Batch: 4893    Epoch[3/3]    Training Loss: 0.4334661364555359\n",
            "Batch: 4894    Epoch[3/3]    Training Loss: 0.4881214499473572\n",
            "Batch: 4895    Epoch[3/3]    Training Loss: 0.5813985466957092\n",
            "Batch: 4896    Epoch[3/3]    Training Loss: 0.5083962082862854\n",
            "Batch: 4897    Epoch[3/3]    Training Loss: 0.4640485942363739\n",
            "Batch: 4898    Epoch[3/3]    Training Loss: 0.5498013496398926\n",
            "Batch: 4899    Epoch[3/3]    Training Loss: 0.5300881266593933\n",
            "Batch: 4900    Epoch[3/3]    Training Loss: 0.45521223545074463\n",
            "Batch: 4901    Epoch[3/3]    Training Loss: 0.5267195105552673\n",
            "Batch: 4902    Epoch[3/3]    Training Loss: 0.7214743494987488\n",
            "Batch: 4903    Epoch[3/3]    Training Loss: 0.32318663597106934\n",
            "Batch: 4904    Epoch[3/3]    Training Loss: 0.13013656437397003\n",
            "Batch: 4905    Epoch[3/3]    Training Loss: 0.7158114910125732\n",
            "Batch: 4906    Epoch[3/3]    Training Loss: 0.7417182326316833\n",
            "Batch: 4907    Epoch[3/3]    Training Loss: 1.2874267101287842\n",
            "Batch: 4908    Epoch[3/3]    Training Loss: 0.6802843809127808\n",
            "Batch: 4909    Epoch[3/3]    Training Loss: 0.598425030708313\n",
            "Batch: 4910    Epoch[3/3]    Training Loss: 0.5457218289375305\n",
            "Batch: 4911    Epoch[3/3]    Training Loss: 1.046292781829834\n",
            "Batch: 4912    Epoch[3/3]    Training Loss: 0.4275485873222351\n",
            "Batch: 4913    Epoch[3/3]    Training Loss: 0.5481287240982056\n",
            "Batch: 4914    Epoch[3/3]    Training Loss: 0.4369211792945862\n",
            "Batch: 4915    Epoch[3/3]    Training Loss: 0.9013186693191528\n",
            "Batch: 4916    Epoch[3/3]    Training Loss: 0.39587467908859253\n",
            "Batch: 4917    Epoch[3/3]    Training Loss: 0.3520658016204834\n",
            "Batch: 4918    Epoch[3/3]    Training Loss: 0.6265136003494263\n",
            "Batch: 4919    Epoch[3/3]    Training Loss: 0.9211045503616333\n",
            "Batch: 4920    Epoch[3/3]    Training Loss: 0.6514636278152466\n",
            "Batch: 4921    Epoch[3/3]    Training Loss: 0.9298344254493713\n",
            "Batch: 4922    Epoch[3/3]    Training Loss: 0.5378658175468445\n",
            "Batch: 4923    Epoch[3/3]    Training Loss: 0.4694155156612396\n",
            "Batch: 4924    Epoch[3/3]    Training Loss: 0.41715770959854126\n",
            "Batch: 4925    Epoch[3/3]    Training Loss: 1.0728039741516113\n",
            "Batch: 4926    Epoch[3/3]    Training Loss: 0.8493990898132324\n",
            "Batch: 4927    Epoch[3/3]    Training Loss: 0.4136805832386017\n",
            "Batch: 4928    Epoch[3/3]    Training Loss: 0.45149070024490356\n",
            "Batch: 4929    Epoch[3/3]    Training Loss: 0.4570727050304413\n",
            "Batch: 4930    Epoch[3/3]    Training Loss: 1.1392649412155151\n",
            "Batch: 4931    Epoch[3/3]    Training Loss: 0.8111177682876587\n",
            "Batch: 4932    Epoch[3/3]    Training Loss: 0.6819629669189453\n",
            "Batch: 4933    Epoch[3/3]    Training Loss: 0.8845323324203491\n",
            "Batch: 4934    Epoch[3/3]    Training Loss: 0.8493779897689819\n",
            "Batch: 4935    Epoch[3/3]    Training Loss: 0.8737327456474304\n",
            "Batch: 4936    Epoch[3/3]    Training Loss: 0.37592124938964844\n",
            "Batch: 4937    Epoch[3/3]    Training Loss: 0.6825599074363708\n",
            "Batch: 4938    Epoch[3/3]    Training Loss: 0.5785479545593262\n",
            "Batch: 4939    Epoch[3/3]    Training Loss: 0.5586742162704468\n",
            "Batch: 4940    Epoch[3/3]    Training Loss: 0.4670441150665283\n",
            "Batch: 4941    Epoch[3/3]    Training Loss: 0.2501716911792755\n",
            "Batch: 4942    Epoch[3/3]    Training Loss: 0.42610466480255127\n",
            "Batch: 4943    Epoch[3/3]    Training Loss: 0.4775155186653137\n",
            "Batch: 4944    Epoch[3/3]    Training Loss: 1.0066921710968018\n",
            "Batch: 4945    Epoch[3/3]    Training Loss: 0.3747155964374542\n",
            "Batch: 4946    Epoch[3/3]    Training Loss: 0.4073444902896881\n",
            "Batch: 4947    Epoch[3/3]    Training Loss: 1.192863941192627\n",
            "Batch: 4948    Epoch[3/3]    Training Loss: 0.4243302047252655\n",
            "Batch: 4949    Epoch[3/3]    Training Loss: 1.5624160766601562\n",
            "Batch: 4950    Epoch[3/3]    Training Loss: 0.38812923431396484\n",
            "Batch: 4951    Epoch[3/3]    Training Loss: 0.34092554450035095\n",
            "Batch: 4952    Epoch[3/3]    Training Loss: 0.6562576293945312\n",
            "Batch: 4953    Epoch[3/3]    Training Loss: 0.4951164126396179\n",
            "Batch: 4954    Epoch[3/3]    Training Loss: 0.6432945132255554\n",
            "Batch: 4955    Epoch[3/3]    Training Loss: 1.4081931114196777\n",
            "Batch: 4956    Epoch[3/3]    Training Loss: 0.9088771343231201\n",
            "Batch: 4957    Epoch[3/3]    Training Loss: 0.4272967576980591\n",
            "Batch: 4958    Epoch[3/3]    Training Loss: 0.9109069108963013\n",
            "Batch: 4959    Epoch[3/3]    Training Loss: 0.7361977696418762\n",
            "Batch: 4960    Epoch[3/3]    Training Loss: 0.5885818600654602\n",
            "Batch: 4961    Epoch[3/3]    Training Loss: 0.6023101806640625\n",
            "Batch: 4962    Epoch[3/3]    Training Loss: 0.4447038471698761\n",
            "Batch: 4963    Epoch[3/3]    Training Loss: 0.7964819073677063\n",
            "Batch: 4964    Epoch[3/3]    Training Loss: 0.5731265544891357\n",
            "Batch: 4965    Epoch[3/3]    Training Loss: 0.35753023624420166\n",
            "Batch: 4966    Epoch[3/3]    Training Loss: 0.5130016207695007\n",
            "Batch: 4967    Epoch[3/3]    Training Loss: 0.28066352009773254\n",
            "Batch: 4968    Epoch[3/3]    Training Loss: 0.531042218208313\n",
            "Batch: 4969    Epoch[3/3]    Training Loss: 0.36827439069747925\n",
            "Batch: 4970    Epoch[3/3]    Training Loss: 0.5140597224235535\n",
            "Batch: 4971    Epoch[3/3]    Training Loss: 0.5053356289863586\n",
            "Batch: 4972    Epoch[3/3]    Training Loss: 0.30528178811073303\n",
            "Batch: 4973    Epoch[3/3]    Training Loss: 0.5834062695503235\n",
            "Batch: 4974    Epoch[3/3]    Training Loss: 0.5819927453994751\n",
            "Batch: 4975    Epoch[3/3]    Training Loss: 0.6641483306884766\n",
            "Batch: 4976    Epoch[3/3]    Training Loss: 0.3630502223968506\n",
            "Batch: 4977    Epoch[3/3]    Training Loss: 0.9153313636779785\n",
            "Batch: 4978    Epoch[3/3]    Training Loss: 0.7722876071929932\n",
            "Batch: 4979    Epoch[3/3]    Training Loss: 0.24903741478919983\n",
            "Batch: 4980    Epoch[3/3]    Training Loss: 0.25611355900764465\n",
            "Batch: 4981    Epoch[3/3]    Training Loss: 0.5364441871643066\n",
            "Batch: 4982    Epoch[3/3]    Training Loss: 0.8104101419448853\n",
            "Batch: 4983    Epoch[3/3]    Training Loss: 0.4827590882778168\n",
            "Batch: 4984    Epoch[3/3]    Training Loss: 0.5327406525611877\n",
            "Batch: 4985    Epoch[3/3]    Training Loss: 1.0574967861175537\n",
            "Batch: 4986    Epoch[3/3]    Training Loss: 0.45796269178390503\n",
            "Batch: 4987    Epoch[3/3]    Training Loss: 0.8199025392532349\n",
            "Batch: 4988    Epoch[3/3]    Training Loss: 0.623021125793457\n",
            "Batch: 4989    Epoch[3/3]    Training Loss: 0.4628668427467346\n",
            "Batch: 4990    Epoch[3/3]    Training Loss: 0.5909308195114136\n",
            "Batch: 4991    Epoch[3/3]    Training Loss: 1.1954357624053955\n",
            "Batch: 4992    Epoch[3/3]    Training Loss: 0.5990852117538452\n",
            "Batch: 4993    Epoch[3/3]    Training Loss: 0.38543131947517395\n",
            "Batch: 4994    Epoch[3/3]    Training Loss: 0.7623013257980347\n",
            "Batch: 4995    Epoch[3/3]    Training Loss: 1.0507476329803467\n",
            "Batch: 4996    Epoch[3/3]    Training Loss: 0.571649968624115\n",
            "Batch: 4997    Epoch[3/3]    Training Loss: 0.23188185691833496\n",
            "Batch: 4998    Epoch[3/3]    Training Loss: 0.5756929516792297\n",
            "Batch: 4999    Epoch[3/3]    Training Loss: 0.3648034334182739\n",
            "Batch: 5000    Epoch[3/3]    Training Loss: 0.9727838039398193\n",
            "Batch: 5001    Epoch[3/3]    Training Loss: 0.8920774459838867\n",
            "Batch: 5002    Epoch[3/3]    Training Loss: 0.5674506425857544\n",
            "Batch: 5003    Epoch[3/3]    Training Loss: 1.0138435363769531\n",
            "Batch: 5004    Epoch[3/3]    Training Loss: 0.4105463922023773\n",
            "Batch: 5005    Epoch[3/3]    Training Loss: 0.6500505805015564\n",
            "Batch: 5006    Epoch[3/3]    Training Loss: 0.7654891014099121\n",
            "Batch: 5007    Epoch[3/3]    Training Loss: 1.0764622688293457\n",
            "Batch: 5008    Epoch[3/3]    Training Loss: 0.9043613076210022\n",
            "Batch: 5009    Epoch[3/3]    Training Loss: 0.7885370254516602\n",
            "Batch: 5010    Epoch[3/3]    Training Loss: 1.0883511304855347\n",
            "Batch: 5011    Epoch[3/3]    Training Loss: 0.9378880262374878\n",
            "Batch: 5012    Epoch[3/3]    Training Loss: 0.576188325881958\n",
            "Batch: 5013    Epoch[3/3]    Training Loss: 0.26003000140190125\n",
            "Batch: 5014    Epoch[3/3]    Training Loss: 0.3477364778518677\n",
            "Batch: 5015    Epoch[3/3]    Training Loss: 0.6047239303588867\n",
            "Batch: 5016    Epoch[3/3]    Training Loss: 0.24135690927505493\n",
            "Batch: 5017    Epoch[3/3]    Training Loss: 0.9975921511650085\n",
            "Batch: 5018    Epoch[3/3]    Training Loss: 0.3938624858856201\n",
            "Batch: 5019    Epoch[3/3]    Training Loss: 0.5231932997703552\n",
            "Batch: 5020    Epoch[3/3]    Training Loss: 0.5306933522224426\n",
            "Batch: 5021    Epoch[3/3]    Training Loss: 0.36037224531173706\n",
            "Batch: 5022    Epoch[3/3]    Training Loss: 0.37968096137046814\n",
            "Batch: 5023    Epoch[3/3]    Training Loss: 0.5582031607627869\n",
            "Batch: 5024    Epoch[3/3]    Training Loss: 0.6470166444778442\n",
            "Batch: 5025    Epoch[3/3]    Training Loss: 0.4066992998123169\n",
            "Batch: 5026    Epoch[3/3]    Training Loss: 0.24311906099319458\n",
            "Batch: 5027    Epoch[3/3]    Training Loss: 0.6277320384979248\n",
            "Batch: 5028    Epoch[3/3]    Training Loss: 1.1436724662780762\n",
            "Batch: 5029    Epoch[3/3]    Training Loss: 0.5054851770401001\n",
            "Batch: 5030    Epoch[3/3]    Training Loss: 0.24294963479042053\n",
            "Batch: 5031    Epoch[3/3]    Training Loss: 1.4927815198898315\n",
            "Batch: 5032    Epoch[3/3]    Training Loss: 0.5504790544509888\n",
            "Batch: 5033    Epoch[3/3]    Training Loss: 0.9823929071426392\n",
            "Batch: 5034    Epoch[3/3]    Training Loss: 0.33044958114624023\n",
            "Batch: 5035    Epoch[3/3]    Training Loss: 0.5493791103363037\n",
            "Batch: 5036    Epoch[3/3]    Training Loss: 0.7323576807975769\n",
            "Batch: 5037    Epoch[3/3]    Training Loss: 1.2645294666290283\n",
            "Batch: 5038    Epoch[3/3]    Training Loss: 0.31966036558151245\n",
            "Batch: 5039    Epoch[3/3]    Training Loss: 0.9513455033302307\n",
            "Batch: 5040    Epoch[3/3]    Training Loss: 0.5307519435882568\n",
            "Batch: 5041    Epoch[3/3]    Training Loss: 0.9345060586929321\n",
            "Batch: 5042    Epoch[3/3]    Training Loss: 0.8500286340713501\n",
            "Batch: 5043    Epoch[3/3]    Training Loss: 0.6461009383201599\n",
            "Batch: 5044    Epoch[3/3]    Training Loss: 0.9676854610443115\n",
            "Batch: 5045    Epoch[3/3]    Training Loss: 0.5696446895599365\n",
            "Batch: 5046    Epoch[3/3]    Training Loss: 0.6517626047134399\n",
            "Batch: 5047    Epoch[3/3]    Training Loss: 0.7880291938781738\n",
            "Batch: 5048    Epoch[3/3]    Training Loss: 0.5642623901367188\n",
            "Batch: 5049    Epoch[3/3]    Training Loss: 0.6445214748382568\n",
            "Batch: 5050    Epoch[3/3]    Training Loss: 0.5196118950843811\n",
            "Batch: 5051    Epoch[3/3]    Training Loss: 0.47594523429870605\n",
            "Batch: 5052    Epoch[3/3]    Training Loss: 0.8056760430335999\n",
            "Batch: 5053    Epoch[3/3]    Training Loss: 0.46161723136901855\n",
            "Batch: 5054    Epoch[3/3]    Training Loss: 1.4017211198806763\n",
            "Batch: 5055    Epoch[3/3]    Training Loss: 1.3736411333084106\n",
            "Batch: 5056    Epoch[3/3]    Training Loss: 1.3248823881149292\n",
            "Batch: 5057    Epoch[3/3]    Training Loss: 0.9530086517333984\n",
            "Batch: 5058    Epoch[3/3]    Training Loss: 0.7662035226821899\n",
            "Batch: 5059    Epoch[3/3]    Training Loss: 0.5902957916259766\n",
            "Batch: 5060    Epoch[3/3]    Training Loss: 0.5385105013847351\n",
            "Batch: 5061    Epoch[3/3]    Training Loss: 0.5635331869125366\n",
            "Batch: 5062    Epoch[3/3]    Training Loss: 0.6445613503456116\n",
            "Batch: 5063    Epoch[3/3]    Training Loss: 0.8256497383117676\n",
            "Batch: 5064    Epoch[3/3]    Training Loss: 0.5365607142448425\n",
            "Batch: 5065    Epoch[3/3]    Training Loss: 1.0166723728179932\n",
            "Batch: 5066    Epoch[3/3]    Training Loss: 0.6148952841758728\n",
            "Batch: 5067    Epoch[3/3]    Training Loss: 0.6064653396606445\n",
            "Batch: 5068    Epoch[3/3]    Training Loss: 0.4926314055919647\n",
            "Batch: 5069    Epoch[3/3]    Training Loss: 0.46686485409736633\n",
            "Batch: 5070    Epoch[3/3]    Training Loss: 0.8675096035003662\n",
            "Batch: 5071    Epoch[3/3]    Training Loss: 0.290593683719635\n",
            "Batch: 5072    Epoch[3/3]    Training Loss: 0.5131414532661438\n",
            "Batch: 5073    Epoch[3/3]    Training Loss: 0.4479987323284149\n",
            "Batch: 5074    Epoch[3/3]    Training Loss: 0.4561693072319031\n",
            "Batch: 5075    Epoch[3/3]    Training Loss: 0.8167312145233154\n",
            "Batch: 5076    Epoch[3/3]    Training Loss: 0.5001082420349121\n",
            "Batch: 5077    Epoch[3/3]    Training Loss: 1.0484614372253418\n",
            "Batch: 5078    Epoch[3/3]    Training Loss: 0.6281133890151978\n",
            "Batch: 5079    Epoch[3/3]    Training Loss: 0.7468041181564331\n",
            "Batch: 5080    Epoch[3/3]    Training Loss: 0.6690717339515686\n",
            "Batch: 5081    Epoch[3/3]    Training Loss: 0.4041350483894348\n",
            "Batch: 5082    Epoch[3/3]    Training Loss: 0.4634021818637848\n",
            "Batch: 5083    Epoch[3/3]    Training Loss: 0.7230908870697021\n",
            "Batch: 5084    Epoch[3/3]    Training Loss: 0.6996992230415344\n",
            "Batch: 5085    Epoch[3/3]    Training Loss: 0.5958063006401062\n",
            "Batch: 5086    Epoch[3/3]    Training Loss: 0.9804697036743164\n",
            "Batch: 5087    Epoch[3/3]    Training Loss: 0.5649620294570923\n",
            "Batch: 5088    Epoch[3/3]    Training Loss: 0.385339617729187\n",
            "Batch: 5089    Epoch[3/3]    Training Loss: 0.41342830657958984\n",
            "Batch: 5090    Epoch[3/3]    Training Loss: 0.5033179521560669\n",
            "Batch: 5091    Epoch[3/3]    Training Loss: 1.3941261768341064\n",
            "Batch: 5092    Epoch[3/3]    Training Loss: 0.1718534678220749\n",
            "Batch: 5093    Epoch[3/3]    Training Loss: 0.5993001461029053\n",
            "Batch: 5094    Epoch[3/3]    Training Loss: 0.8819623589515686\n",
            "Batch: 5095    Epoch[3/3]    Training Loss: 1.6693822145462036\n",
            "Batch: 5096    Epoch[3/3]    Training Loss: 0.5837408304214478\n",
            "Batch: 5097    Epoch[3/3]    Training Loss: 0.580466628074646\n",
            "Batch: 5098    Epoch[3/3]    Training Loss: 0.8386362195014954\n",
            "Batch: 5099    Epoch[3/3]    Training Loss: 0.4991699457168579\n",
            "Batch: 5100    Epoch[3/3]    Training Loss: 0.6005752682685852\n",
            "Batch: 5101    Epoch[3/3]    Training Loss: 0.3955668807029724\n",
            "Batch: 5102    Epoch[3/3]    Training Loss: 0.16342435777187347\n",
            "Batch: 5103    Epoch[3/3]    Training Loss: 0.48195114731788635\n",
            "Batch: 5104    Epoch[3/3]    Training Loss: 0.4735182821750641\n",
            "Batch: 5105    Epoch[3/3]    Training Loss: 0.5584210157394409\n",
            "Batch: 5106    Epoch[3/3]    Training Loss: 0.9203375577926636\n",
            "Batch: 5107    Epoch[3/3]    Training Loss: 0.7424955368041992\n",
            "Batch: 5108    Epoch[3/3]    Training Loss: 0.9997353553771973\n",
            "Batch: 5109    Epoch[3/3]    Training Loss: 0.3523157835006714\n",
            "Batch: 5110    Epoch[3/3]    Training Loss: 0.46007072925567627\n",
            "Batch: 5111    Epoch[3/3]    Training Loss: 0.5673147439956665\n",
            "Batch: 5112    Epoch[3/3]    Training Loss: 0.6997342705726624\n",
            "Batch: 5113    Epoch[3/3]    Training Loss: 0.9720017910003662\n",
            "Batch: 5114    Epoch[3/3]    Training Loss: 0.7643868923187256\n",
            "Batch: 5115    Epoch[3/3]    Training Loss: 0.6509850025177002\n",
            "Batch: 5116    Epoch[3/3]    Training Loss: 1.1412558555603027\n",
            "Batch: 5117    Epoch[3/3]    Training Loss: 0.703555703163147\n",
            "Batch: 5118    Epoch[3/3]    Training Loss: 0.8107513785362244\n",
            "Batch: 5119    Epoch[3/3]    Training Loss: 0.9372790455818176\n",
            "Batch: 5120    Epoch[3/3]    Training Loss: 0.6202200651168823\n",
            "Batch: 5121    Epoch[3/3]    Training Loss: 0.767672061920166\n",
            "Batch: 5122    Epoch[3/3]    Training Loss: 0.6790505647659302\n",
            "Batch: 5123    Epoch[3/3]    Training Loss: 0.3125380277633667\n",
            "Batch: 5124    Epoch[3/3]    Training Loss: 1.0047353506088257\n",
            "Batch: 5125    Epoch[3/3]    Training Loss: 0.736099362373352\n",
            "Batch: 5126    Epoch[3/3]    Training Loss: 0.46861883997917175\n",
            "Batch: 5127    Epoch[3/3]    Training Loss: 0.6104966402053833\n",
            "Batch: 5128    Epoch[3/3]    Training Loss: 0.713975191116333\n",
            "Batch: 5129    Epoch[3/3]    Training Loss: 0.48887068033218384\n",
            "Batch: 5130    Epoch[3/3]    Training Loss: 0.7244798541069031\n",
            "Batch: 5131    Epoch[3/3]    Training Loss: 0.7657456398010254\n",
            "Batch: 5132    Epoch[3/3]    Training Loss: 0.5178670287132263\n",
            "Batch: 5133    Epoch[3/3]    Training Loss: 1.1051735877990723\n",
            "Batch: 5134    Epoch[3/3]    Training Loss: 0.5062735676765442\n",
            "Batch: 5135    Epoch[3/3]    Training Loss: 0.5748775005340576\n",
            "Batch: 5136    Epoch[3/3]    Training Loss: 0.43578895926475525\n",
            "Batch: 5137    Epoch[3/3]    Training Loss: 0.8291667699813843\n",
            "Batch: 5138    Epoch[3/3]    Training Loss: 0.8509691953659058\n",
            "Batch: 5139    Epoch[3/3]    Training Loss: 0.6902037858963013\n",
            "Batch: 5140    Epoch[3/3]    Training Loss: 0.8032046556472778\n",
            "Batch: 5141    Epoch[3/3]    Training Loss: 0.36843353509902954\n",
            "Batch: 5142    Epoch[3/3]    Training Loss: 0.3681343197822571\n",
            "Batch: 5143    Epoch[3/3]    Training Loss: 0.6004798412322998\n",
            "Batch: 5144    Epoch[3/3]    Training Loss: 0.8235033750534058\n",
            "Batch: 5145    Epoch[3/3]    Training Loss: 0.8356326818466187\n",
            "Batch: 5146    Epoch[3/3]    Training Loss: 0.49803534150123596\n",
            "Batch: 5147    Epoch[3/3]    Training Loss: 0.5304635763168335\n",
            "Batch: 5148    Epoch[3/3]    Training Loss: 0.6436876058578491\n",
            "Batch: 5149    Epoch[3/3]    Training Loss: 0.5050667524337769\n",
            "Batch: 5150    Epoch[3/3]    Training Loss: 0.8927607536315918\n",
            "Batch: 5151    Epoch[3/3]    Training Loss: 0.39937424659729004\n",
            "Batch: 5152    Epoch[3/3]    Training Loss: 0.9445998668670654\n",
            "Batch: 5153    Epoch[3/3]    Training Loss: 0.25462427735328674\n",
            "Batch: 5154    Epoch[3/3]    Training Loss: 0.8168573379516602\n",
            "Batch: 5155    Epoch[3/3]    Training Loss: 0.7560794949531555\n",
            "Batch: 5156    Epoch[3/3]    Training Loss: 0.8093101382255554\n",
            "Batch: 5157    Epoch[3/3]    Training Loss: 0.7863448858261108\n",
            "Batch: 5158    Epoch[3/3]    Training Loss: 0.6315204501152039\n",
            "Batch: 5159    Epoch[3/3]    Training Loss: 0.5386002659797668\n",
            "Batch: 5160    Epoch[3/3]    Training Loss: 0.39471670985221863\n",
            "Batch: 5161    Epoch[3/3]    Training Loss: 1.1266779899597168\n",
            "Batch: 5162    Epoch[3/3]    Training Loss: 0.9466288089752197\n",
            "Batch: 5163    Epoch[3/3]    Training Loss: 1.2544145584106445\n",
            "Batch: 5164    Epoch[3/3]    Training Loss: 0.38983970880508423\n",
            "Batch: 5165    Epoch[3/3]    Training Loss: 0.47892850637435913\n",
            "Batch: 5166    Epoch[3/3]    Training Loss: 0.9156376719474792\n",
            "Batch: 5167    Epoch[3/3]    Training Loss: 0.9371774792671204\n",
            "Batch: 5168    Epoch[3/3]    Training Loss: 0.6589730978012085\n",
            "Batch: 5169    Epoch[3/3]    Training Loss: 0.4630463123321533\n",
            "Batch: 5170    Epoch[3/3]    Training Loss: 0.4193716049194336\n",
            "Batch: 5171    Epoch[3/3]    Training Loss: 0.39196866750717163\n",
            "Batch: 5172    Epoch[3/3]    Training Loss: 0.6991567015647888\n",
            "Batch: 5173    Epoch[3/3]    Training Loss: 0.39303916692733765\n",
            "Batch: 5174    Epoch[3/3]    Training Loss: 0.9541198015213013\n",
            "Batch: 5175    Epoch[3/3]    Training Loss: 1.0387037992477417\n",
            "Batch: 5176    Epoch[3/3]    Training Loss: 0.30855292081832886\n",
            "Batch: 5177    Epoch[3/3]    Training Loss: 0.7013809084892273\n",
            "Batch: 5178    Epoch[3/3]    Training Loss: 0.6071425080299377\n",
            "Batch: 5179    Epoch[3/3]    Training Loss: 0.7235761284828186\n",
            "Batch: 5180    Epoch[3/3]    Training Loss: 0.44041216373443604\n",
            "Batch: 5181    Epoch[3/3]    Training Loss: 0.8665657043457031\n",
            "Batch: 5182    Epoch[3/3]    Training Loss: 0.234971821308136\n",
            "Batch: 5183    Epoch[3/3]    Training Loss: 0.40605974197387695\n",
            "Batch: 5184    Epoch[3/3]    Training Loss: 0.8351294994354248\n",
            "Batch: 5185    Epoch[3/3]    Training Loss: 0.5023674368858337\n",
            "Batch: 5186    Epoch[3/3]    Training Loss: 0.5303165912628174\n",
            "Batch: 5187    Epoch[3/3]    Training Loss: 0.708329439163208\n",
            "Batch: 5188    Epoch[3/3]    Training Loss: 0.733229398727417\n",
            "Batch: 5189    Epoch[3/3]    Training Loss: 1.179148554801941\n",
            "Batch: 5190    Epoch[3/3]    Training Loss: 0.24618574976921082\n",
            "Batch: 5191    Epoch[3/3]    Training Loss: 1.1483030319213867\n",
            "Batch: 5192    Epoch[3/3]    Training Loss: 0.7154479026794434\n",
            "Batch: 5193    Epoch[3/3]    Training Loss: 0.7195923328399658\n",
            "Batch: 5194    Epoch[3/3]    Training Loss: 0.8039117455482483\n",
            "Batch: 5195    Epoch[3/3]    Training Loss: 0.5459004044532776\n",
            "Batch: 5196    Epoch[3/3]    Training Loss: 0.9365980625152588\n",
            "Batch: 5197    Epoch[3/3]    Training Loss: 0.7498517036437988\n",
            "Batch: 5198    Epoch[3/3]    Training Loss: 0.5957704782485962\n",
            "Batch: 5199    Epoch[3/3]    Training Loss: 0.5655915141105652\n",
            "Batch: 5200    Epoch[3/3]    Training Loss: 0.6576656699180603\n",
            "Batch: 5201    Epoch[3/3]    Training Loss: 0.4396556317806244\n",
            "Batch: 5202    Epoch[3/3]    Training Loss: 0.9666423797607422\n",
            "Batch: 5203    Epoch[3/3]    Training Loss: 0.45816415548324585\n",
            "Batch: 5204    Epoch[3/3]    Training Loss: 0.471815288066864\n",
            "Batch: 5205    Epoch[3/3]    Training Loss: 0.6822198629379272\n",
            "Batch: 5206    Epoch[3/3]    Training Loss: 0.9220409393310547\n",
            "Batch: 5207    Epoch[3/3]    Training Loss: 0.21459868550300598\n",
            "Batch: 5208    Epoch[3/3]    Training Loss: 0.967923104763031\n",
            "Batch: 5209    Epoch[3/3]    Training Loss: 0.9178571701049805\n",
            "Batch: 5210    Epoch[3/3]    Training Loss: 0.7150863409042358\n",
            "Batch: 5211    Epoch[3/3]    Training Loss: 0.38164108991622925\n",
            "Batch: 5212    Epoch[3/3]    Training Loss: 0.4501156806945801\n",
            "Batch: 5213    Epoch[3/3]    Training Loss: 0.3162788152694702\n",
            "Batch: 5214    Epoch[3/3]    Training Loss: 0.5366645455360413\n",
            "Batch: 5215    Epoch[3/3]    Training Loss: 0.6363846659660339\n",
            "Batch: 5216    Epoch[3/3]    Training Loss: 0.4932412803173065\n",
            "Batch: 5217    Epoch[3/3]    Training Loss: 0.35786405205726624\n",
            "Batch: 5218    Epoch[3/3]    Training Loss: 0.6133087873458862\n",
            "Batch: 5219    Epoch[3/3]    Training Loss: 0.6928157806396484\n",
            "Batch: 5220    Epoch[3/3]    Training Loss: 0.4758610725402832\n",
            "Batch: 5221    Epoch[3/3]    Training Loss: 0.7517037391662598\n",
            "Batch: 5222    Epoch[3/3]    Training Loss: 0.5280706882476807\n",
            "Batch: 5223    Epoch[3/3]    Training Loss: 0.5410494804382324\n",
            "Batch: 5224    Epoch[3/3]    Training Loss: 0.6703895926475525\n",
            "Batch: 5225    Epoch[3/3]    Training Loss: 0.3887231945991516\n",
            "Batch: 5226    Epoch[3/3]    Training Loss: 0.6836698055267334\n",
            "Batch: 5227    Epoch[3/3]    Training Loss: 0.5314602851867676\n",
            "Batch: 5228    Epoch[3/3]    Training Loss: 0.26810190081596375\n",
            "Batch: 5229    Epoch[3/3]    Training Loss: 0.6856566667556763\n",
            "Batch: 5230    Epoch[3/3]    Training Loss: 0.45767879486083984\n",
            "Batch: 5231    Epoch[3/3]    Training Loss: 0.408652126789093\n",
            "Batch: 5232    Epoch[3/3]    Training Loss: 0.4388859272003174\n",
            "Batch: 5233    Epoch[3/3]    Training Loss: 0.2241600900888443\n",
            "Batch: 5234    Epoch[3/3]    Training Loss: 0.49510082602500916\n",
            "Batch: 5235    Epoch[3/3]    Training Loss: 0.5916597843170166\n",
            "Batch: 5236    Epoch[3/3]    Training Loss: 0.19014106690883636\n",
            "Batch: 5237    Epoch[3/3]    Training Loss: 0.5611602067947388\n",
            "Batch: 5238    Epoch[3/3]    Training Loss: 0.7043861150741577\n",
            "Batch: 5239    Epoch[3/3]    Training Loss: 0.8385477066040039\n",
            "Batch: 5240    Epoch[3/3]    Training Loss: 0.3389933109283447\n",
            "Batch: 5241    Epoch[3/3]    Training Loss: 0.688119649887085\n",
            "Batch: 5242    Epoch[3/3]    Training Loss: 0.3267357647418976\n",
            "Batch: 5243    Epoch[3/3]    Training Loss: 0.22509175539016724\n",
            "Batch: 5244    Epoch[3/3]    Training Loss: 0.9483951330184937\n",
            "Batch: 5245    Epoch[3/3]    Training Loss: 0.43787115812301636\n",
            "Batch: 5246    Epoch[3/3]    Training Loss: 0.5615040063858032\n",
            "Batch: 5247    Epoch[3/3]    Training Loss: 0.6203670501708984\n",
            "Batch: 5248    Epoch[3/3]    Training Loss: 0.3964393138885498\n",
            "Batch: 5249    Epoch[3/3]    Training Loss: 0.6996951103210449\n",
            "Batch: 5250    Epoch[3/3]    Training Loss: 0.5669136643409729\n",
            "Batch: 5251    Epoch[3/3]    Training Loss: 0.43224436044692993\n",
            "Batch: 5252    Epoch[3/3]    Training Loss: 0.6827093362808228\n",
            "Batch: 5253    Epoch[3/3]    Training Loss: 0.60829758644104\n",
            "Batch: 5254    Epoch[3/3]    Training Loss: 0.6844322085380554\n",
            "Batch: 5255    Epoch[3/3]    Training Loss: 1.0611951351165771\n",
            "Batch: 5256    Epoch[3/3]    Training Loss: 1.2165993452072144\n",
            "Batch: 5257    Epoch[3/3]    Training Loss: 0.8586515188217163\n",
            "Batch: 5258    Epoch[3/3]    Training Loss: 0.7687045931816101\n",
            "Batch: 5259    Epoch[3/3]    Training Loss: 0.4254054129123688\n",
            "Batch: 5260    Epoch[3/3]    Training Loss: 0.36699044704437256\n",
            "Batch: 5261    Epoch[3/3]    Training Loss: 1.0110869407653809\n",
            "Batch: 5262    Epoch[3/3]    Training Loss: 0.7853637337684631\n",
            "Batch: 5263    Epoch[3/3]    Training Loss: 0.5007678866386414\n",
            "Batch: 5264    Epoch[3/3]    Training Loss: 0.6391189098358154\n",
            "Batch: 5265    Epoch[3/3]    Training Loss: 0.7966107130050659\n",
            "Batch: 5266    Epoch[3/3]    Training Loss: 0.43453267216682434\n",
            "Batch: 5267    Epoch[3/3]    Training Loss: 0.7210169434547424\n",
            "Batch: 5268    Epoch[3/3]    Training Loss: 1.0528652667999268\n",
            "Batch: 5269    Epoch[3/3]    Training Loss: 0.5588760375976562\n",
            "Batch: 5270    Epoch[3/3]    Training Loss: 0.5053225159645081\n",
            "Batch: 5271    Epoch[3/3]    Training Loss: 0.7679455280303955\n",
            "Batch: 5272    Epoch[3/3]    Training Loss: 0.5203481912612915\n",
            "Batch: 5273    Epoch[3/3]    Training Loss: 1.1394543647766113\n",
            "Batch: 5274    Epoch[3/3]    Training Loss: 0.2983173727989197\n",
            "Batch: 5275    Epoch[3/3]    Training Loss: 0.47665438055992126\n",
            "Batch: 5276    Epoch[3/3]    Training Loss: 0.40687617659568787\n",
            "Batch: 5277    Epoch[3/3]    Training Loss: 0.47412824630737305\n",
            "Batch: 5278    Epoch[3/3]    Training Loss: 0.3933018445968628\n",
            "Batch: 5279    Epoch[3/3]    Training Loss: 0.4868830144405365\n",
            "Batch: 5280    Epoch[3/3]    Training Loss: 0.5279615521430969\n",
            "Batch: 5281    Epoch[3/3]    Training Loss: 1.3431832790374756\n",
            "Batch: 5282    Epoch[3/3]    Training Loss: 0.3733414113521576\n",
            "Batch: 5283    Epoch[3/3]    Training Loss: 0.5336989164352417\n",
            "Batch: 5284    Epoch[3/3]    Training Loss: 0.7645878791809082\n",
            "Batch: 5285    Epoch[3/3]    Training Loss: 1.3486391305923462\n",
            "Batch: 5286    Epoch[3/3]    Training Loss: 0.2700822949409485\n",
            "Batch: 5287    Epoch[3/3]    Training Loss: 1.039311170578003\n",
            "Batch: 5288    Epoch[3/3]    Training Loss: 0.4184954762458801\n",
            "Batch: 5289    Epoch[3/3]    Training Loss: 0.32479479908943176\n",
            "Batch: 5290    Epoch[3/3]    Training Loss: 0.5524163246154785\n",
            "Batch: 5291    Epoch[3/3]    Training Loss: 0.48519259691238403\n",
            "Batch: 5292    Epoch[3/3]    Training Loss: 0.9836773872375488\n",
            "Batch: 5293    Epoch[3/3]    Training Loss: 1.0638741254806519\n",
            "Batch: 5294    Epoch[3/3]    Training Loss: 0.19746214151382446\n",
            "Batch: 5295    Epoch[3/3]    Training Loss: 1.0729732513427734\n",
            "Batch: 5296    Epoch[3/3]    Training Loss: 0.9318445920944214\n",
            "Batch: 5297    Epoch[3/3]    Training Loss: 0.7088115215301514\n",
            "Batch: 5298    Epoch[3/3]    Training Loss: 0.3464354872703552\n",
            "Batch: 5299    Epoch[3/3]    Training Loss: 0.5780825614929199\n",
            "Batch: 5300    Epoch[3/3]    Training Loss: 1.1887693405151367\n",
            "Batch: 5301    Epoch[3/3]    Training Loss: 0.9525773525238037\n",
            "Batch: 5302    Epoch[3/3]    Training Loss: 0.3483017086982727\n",
            "Batch: 5303    Epoch[3/3]    Training Loss: 0.5964747667312622\n",
            "Batch: 5304    Epoch[3/3]    Training Loss: 0.31840482354164124\n",
            "Batch: 5305    Epoch[3/3]    Training Loss: 0.7078453302383423\n",
            "Batch: 5306    Epoch[3/3]    Training Loss: 0.7752116918563843\n",
            "Batch: 5307    Epoch[3/3]    Training Loss: 0.514986515045166\n",
            "Batch: 5308    Epoch[3/3]    Training Loss: 0.7576508522033691\n",
            "Batch: 5309    Epoch[3/3]    Training Loss: 0.44317638874053955\n",
            "Batch: 5310    Epoch[3/3]    Training Loss: 0.6180634498596191\n",
            "Batch: 5311    Epoch[3/3]    Training Loss: 0.6137708425521851\n",
            "Batch: 5312    Epoch[3/3]    Training Loss: 1.0630196332931519\n",
            "Batch: 5313    Epoch[3/3]    Training Loss: 0.8613332509994507\n",
            "Batch: 5314    Epoch[3/3]    Training Loss: 0.9097110033035278\n",
            "Batch: 5315    Epoch[3/3]    Training Loss: 0.3696114420890808\n",
            "Batch: 5316    Epoch[3/3]    Training Loss: 1.2917311191558838\n",
            "Batch: 5317    Epoch[3/3]    Training Loss: 0.2105293869972229\n",
            "Batch: 5318    Epoch[3/3]    Training Loss: 0.7280133962631226\n",
            "Batch: 5319    Epoch[3/3]    Training Loss: 0.6329299211502075\n",
            "Batch: 5320    Epoch[3/3]    Training Loss: 0.5569064617156982\n",
            "Batch: 5321    Epoch[3/3]    Training Loss: 0.7153878211975098\n",
            "Batch: 5322    Epoch[3/3]    Training Loss: 0.6071295738220215\n",
            "Batch: 5323    Epoch[3/3]    Training Loss: 0.40889108180999756\n",
            "Batch: 5324    Epoch[3/3]    Training Loss: 0.43615707755088806\n",
            "Batch: 5325    Epoch[3/3]    Training Loss: 0.4861353933811188\n",
            "Batch: 5326    Epoch[3/3]    Training Loss: 0.5944802761077881\n",
            "Batch: 5327    Epoch[3/3]    Training Loss: 0.639181911945343\n",
            "Batch: 5328    Epoch[3/3]    Training Loss: 0.5061643719673157\n",
            "Batch: 5329    Epoch[3/3]    Training Loss: 0.6424375176429749\n",
            "Batch: 5330    Epoch[3/3]    Training Loss: 0.6383212804794312\n",
            "Batch: 5331    Epoch[3/3]    Training Loss: 1.4255485534667969\n",
            "Batch: 5332    Epoch[3/3]    Training Loss: 0.5451050996780396\n",
            "Batch: 5333    Epoch[3/3]    Training Loss: 0.6702380180358887\n",
            "Batch: 5334    Epoch[3/3]    Training Loss: 0.29220902919769287\n",
            "Batch: 5335    Epoch[3/3]    Training Loss: 0.81641685962677\n",
            "Batch: 5336    Epoch[3/3]    Training Loss: 0.6295216083526611\n",
            "Batch: 5337    Epoch[3/3]    Training Loss: 0.29387301206588745\n",
            "Batch: 5338    Epoch[3/3]    Training Loss: 0.5300224423408508\n",
            "Batch: 5339    Epoch[3/3]    Training Loss: 0.5094689130783081\n",
            "Batch: 5340    Epoch[3/3]    Training Loss: 0.778258204460144\n",
            "Batch: 5341    Epoch[3/3]    Training Loss: 0.7336005568504333\n",
            "Batch: 5342    Epoch[3/3]    Training Loss: 0.3323124647140503\n",
            "Batch: 5343    Epoch[3/3]    Training Loss: 0.2901771664619446\n",
            "Batch: 5344    Epoch[3/3]    Training Loss: 0.5421732068061829\n",
            "Batch: 5345    Epoch[3/3]    Training Loss: 0.7582457661628723\n",
            "Batch: 5346    Epoch[3/3]    Training Loss: 0.8067829012870789\n",
            "Batch: 5347    Epoch[3/3]    Training Loss: 0.5589361190795898\n",
            "Batch: 5348    Epoch[3/3]    Training Loss: 0.4502890706062317\n",
            "Batch: 5349    Epoch[3/3]    Training Loss: 0.4297410547733307\n",
            "Batch: 5350    Epoch[3/3]    Training Loss: 0.3887034058570862\n",
            "Batch: 5351    Epoch[3/3]    Training Loss: 0.5974292159080505\n",
            "Batch: 5352    Epoch[3/3]    Training Loss: 0.1717568039894104\n",
            "Batch: 5353    Epoch[3/3]    Training Loss: 0.9996641874313354\n",
            "Batch: 5354    Epoch[3/3]    Training Loss: 0.44182053208351135\n",
            "Batch: 5355    Epoch[3/3]    Training Loss: 0.534334659576416\n",
            "Batch: 5356    Epoch[3/3]    Training Loss: 0.6084263324737549\n",
            "Batch: 5357    Epoch[3/3]    Training Loss: 0.620693564414978\n",
            "Batch: 5358    Epoch[3/3]    Training Loss: 0.7736866474151611\n",
            "Batch: 5359    Epoch[3/3]    Training Loss: 0.6349782943725586\n",
            "Batch: 5360    Epoch[3/3]    Training Loss: 0.9614981412887573\n",
            "Batch: 5361    Epoch[3/3]    Training Loss: 0.7325038909912109\n",
            "Batch: 5362    Epoch[3/3]    Training Loss: 0.4195042848587036\n",
            "Batch: 5363    Epoch[3/3]    Training Loss: 0.3239595293998718\n",
            "Batch: 5364    Epoch[3/3]    Training Loss: 0.3398381471633911\n",
            "Batch: 5365    Epoch[3/3]    Training Loss: 0.43691742420196533\n",
            "Batch: 5366    Epoch[3/3]    Training Loss: 1.1085894107818604\n",
            "Batch: 5367    Epoch[3/3]    Training Loss: 0.6364521980285645\n",
            "Batch: 5368    Epoch[3/3]    Training Loss: 1.2947418689727783\n",
            "Batch: 5369    Epoch[3/3]    Training Loss: 0.37045764923095703\n",
            "Batch: 5370    Epoch[3/3]    Training Loss: 0.4309333264827728\n",
            "Batch: 5371    Epoch[3/3]    Training Loss: 1.0126314163208008\n",
            "Batch: 5372    Epoch[3/3]    Training Loss: 0.4808601140975952\n",
            "Batch: 5373    Epoch[3/3]    Training Loss: 0.6605465412139893\n",
            "Batch: 5374    Epoch[3/3]    Training Loss: 0.2365904003381729\n",
            "Batch: 5375    Epoch[3/3]    Training Loss: 0.31742414832115173\n",
            "Batch: 5376    Epoch[3/3]    Training Loss: 0.6910948157310486\n",
            "Batch: 5377    Epoch[3/3]    Training Loss: 0.6042364239692688\n",
            "Batch: 5378    Epoch[3/3]    Training Loss: 0.7456488013267517\n",
            "Batch: 5379    Epoch[3/3]    Training Loss: 0.5818715691566467\n",
            "Batch: 5380    Epoch[3/3]    Training Loss: 0.5795401334762573\n",
            "Batch: 5381    Epoch[3/3]    Training Loss: 0.6835196018218994\n",
            "Batch: 5382    Epoch[3/3]    Training Loss: 0.4729892909526825\n",
            "Batch: 5383    Epoch[3/3]    Training Loss: 0.7052143812179565\n",
            "Batch: 5384    Epoch[3/3]    Training Loss: 0.3878205418586731\n",
            "Batch: 5385    Epoch[3/3]    Training Loss: 1.0028387308120728\n",
            "Batch: 5386    Epoch[3/3]    Training Loss: 0.3603041172027588\n",
            "Batch: 5387    Epoch[3/3]    Training Loss: 1.2221567630767822\n",
            "Batch: 5388    Epoch[3/3]    Training Loss: 0.6564701795578003\n",
            "Batch: 5389    Epoch[3/3]    Training Loss: 0.959359884262085\n",
            "Batch: 5390    Epoch[3/3]    Training Loss: 0.5561590194702148\n",
            "Batch: 5391    Epoch[3/3]    Training Loss: 0.6082320213317871\n",
            "Batch: 5392    Epoch[3/3]    Training Loss: 0.44345101714134216\n",
            "Batch: 5393    Epoch[3/3]    Training Loss: 0.9536432027816772\n",
            "Batch: 5394    Epoch[3/3]    Training Loss: 0.7653272151947021\n",
            "Batch: 5395    Epoch[3/3]    Training Loss: 0.5705138444900513\n",
            "Batch: 5396    Epoch[3/3]    Training Loss: 0.5769432187080383\n",
            "Batch: 5397    Epoch[3/3]    Training Loss: 0.4518973231315613\n",
            "Batch: 5398    Epoch[3/3]    Training Loss: 0.4277861416339874\n",
            "Batch: 5399    Epoch[3/3]    Training Loss: 0.797774612903595\n",
            "Batch: 5400    Epoch[3/3]    Training Loss: 0.7430474758148193\n",
            "Batch: 5401    Epoch[3/3]    Training Loss: 0.5609182119369507\n",
            "Batch: 5402    Epoch[3/3]    Training Loss: 0.46162474155426025\n",
            "Batch: 5403    Epoch[3/3]    Training Loss: 0.34544432163238525\n",
            "Batch: 5404    Epoch[3/3]    Training Loss: 0.42141032218933105\n",
            "Batch: 5405    Epoch[3/3]    Training Loss: 1.460103988647461\n",
            "Batch: 5406    Epoch[3/3]    Training Loss: 0.4991189241409302\n",
            "Batch: 5407    Epoch[3/3]    Training Loss: 0.4216040372848511\n",
            "Batch: 5408    Epoch[3/3]    Training Loss: 0.6599706411361694\n",
            "Batch: 5409    Epoch[3/3]    Training Loss: 0.33309492468833923\n",
            "Batch: 5410    Epoch[3/3]    Training Loss: 0.6069717407226562\n",
            "Batch: 5411    Epoch[3/3]    Training Loss: 0.6931857466697693\n",
            "Batch: 5412    Epoch[3/3]    Training Loss: 1.0155214071273804\n",
            "Batch: 5413    Epoch[3/3]    Training Loss: 0.36376821994781494\n",
            "Batch: 5414    Epoch[3/3]    Training Loss: 0.9317635297775269\n",
            "Batch: 5415    Epoch[3/3]    Training Loss: 0.5171933174133301\n",
            "Batch: 5416    Epoch[3/3]    Training Loss: 0.7443820834159851\n",
            "Batch: 5417    Epoch[3/3]    Training Loss: 0.3261720538139343\n",
            "Batch: 5418    Epoch[3/3]    Training Loss: 0.443845272064209\n",
            "Batch: 5419    Epoch[3/3]    Training Loss: 0.4461546242237091\n",
            "Batch: 5420    Epoch[3/3]    Training Loss: 0.9288531541824341\n",
            "Batch: 5421    Epoch[3/3]    Training Loss: 0.620871901512146\n",
            "Batch: 5422    Epoch[3/3]    Training Loss: 0.7836896181106567\n",
            "Batch: 5423    Epoch[3/3]    Training Loss: 0.34727293252944946\n",
            "Batch: 5424    Epoch[3/3]    Training Loss: 0.6177617311477661\n",
            "Batch: 5425    Epoch[3/3]    Training Loss: 0.9277418851852417\n",
            "Batch: 5426    Epoch[3/3]    Training Loss: 0.36120301485061646\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertForQuestionAnswering(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (1): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (2): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (3): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (4): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (5): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20ZlRK1d_u0X",
        "outputId": "3a7c63c9-4ac8-4316-ecdb-c829512dfc1f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-miBzNyjAlRc"
      },
      "source": [
        "import os\n",
        "PATH = \"/content/drive/MyDrive/Projects/Clubs/Analytics/Coord Projects/NLP/BERT\""
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3Y9r3Ru_yoC"
      },
      "source": [
        "torch.save({\n",
        "            'epoch': 3,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optim.state_dict(),\n",
        "            'loss': epoch_loss[-1]\n",
        "            }, os.path.join(PATH, \"bert.pth\"))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-ftCrQtDzAT"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "4UVyxYecD8o5",
        "outputId": "cf662a46-abeb-430a-f551-f56eea2c50dd"
      },
      "source": [
        "loss_plot = epoch_loss[::200]\n",
        "plt.plot(np.arange(len(loss_plot)), loss_plot)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f99b102e0d0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bitqx8eEWsk"
      },
      "source": [
        "pred = []\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "for idx, batch in enumerate(val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_positions = batch['start_positions'].to(device)\n",
        "        end_positions = batch['end_positions'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "\n",
        "        pred_start_positions = torch.argmax(outputs['start_logits'])\n",
        "        pred_end_positions = torch.argmax(outputs['end_logits'])+1\n",
        "\n",
        "        pred.append({'Inputs': {'inputs': input_ids[0], 'start': start_positions[0], 'end': end_positions[0]+1},\n",
        "                     'Outputs': {'start': pred_start_positions, 'end': pred_end_positions}})"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDDgqg5BNK92"
      },
      "source": [
        "outputs = []\n",
        "\n",
        "for prediction in pred:\n",
        "    input = prediction['Inputs']['inputs']\n",
        "    \n",
        "    start = prediction['Inputs']['start']\n",
        "    end = prediction['Inputs']['end']\n",
        "    pred_start = prediction['Outputs']['start']\n",
        "    pred_end = prediction['Outputs']['end']\n",
        "\n",
        "    sentence = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input[start:end]))\n",
        "    pred_sentence = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input[pred_start:pred_end]))\n",
        "\n",
        "    outputs.append({'Actual': sentence, 'Predicted': pred_sentence})\n",
        "\n",
        "outputs = pd.DataFrame(outputs)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Wdgb_-pkQQXG",
        "outputId": "a743b595-5efb-4cb1-8c3b-776343cf6b44"
      },
      "source": [
        "outputs.head(50)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Actual</th>\n",
              "      <th>Predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>france</td>\n",
              "      <td>france</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>france</td>\n",
              "      <td>france</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>france</td>\n",
              "      <td>france</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>france</td>\n",
              "      <td>france</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10th and 11th centuries</td>\n",
              "      <td>10th and 11th centuries</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>in the 10th and 11th centuries</td>\n",
              "      <td>10th and 11th centuries</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>10th and 11th centuries</td>\n",
              "      <td>10th and 11th centuries</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>10th and 11th centuries</td>\n",
              "      <td>10th and 11th centuries</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>denmark, iceland and norway</td>\n",
              "      <td>denmark, iceland and norway</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>denmark, iceland and norway</td>\n",
              "      <td>denmark, iceland and norway</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>denmark, iceland and norway</td>\n",
              "      <td>denmark, iceland and norway</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>denmark, iceland and norway</td>\n",
              "      <td>denmark, iceland and norway</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>rollo</td>\n",
              "      <td>rollo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>rollo</td>\n",
              "      <td>rollo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>rollo</td>\n",
              "      <td>rollo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>rollo</td>\n",
              "      <td>rollo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>10th century</td>\n",
              "      <td>10th</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>the first half of the 10th century</td>\n",
              "      <td>10th</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>10th</td>\n",
              "      <td>10th</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>10th</td>\n",
              "      <td>10th</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>william the conqueror</td>\n",
              "      <td>william the conqueror</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>william the conqueror</td>\n",
              "      <td>william the conqueror</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>william the conqueror</td>\n",
              "      <td>william the conqueror</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>richard i</td>\n",
              "      <td>richard i of normandy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>richard i</td>\n",
              "      <td>richard i of normandy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>richard i</td>\n",
              "      <td>richard i of normandy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>catholic</td>\n",
              "      <td>catholic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>catholic orthodoxy</td>\n",
              "      <td>catholic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>catholic</td>\n",
              "      <td>catholic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>viking</td>\n",
              "      <td>norseman, viking</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>norseman, viking</td>\n",
              "      <td>norseman, viking</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>norseman, viking</td>\n",
              "      <td>norseman, viking</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>9th century</td>\n",
              "      <td>medieval latin, 9th century</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>9th century</td>\n",
              "      <td>medieval latin, 9th century</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>9th century</td>\n",
              "      <td>medieval latin, 9th century</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>911</td>\n",
              "      <td>911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>911</td>\n",
              "      <td>911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>911</td>\n",
              "      <td>911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>king charles iii</td>\n",
              "      <td>king charles iii of west francia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>king charles iii</td>\n",
              "      <td>king charles iii of west francia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>king charles iii</td>\n",
              "      <td>king charles iii of west francia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>seine</td>\n",
              "      <td>river seine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>epte</td>\n",
              "      <td>river seine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>seine</td>\n",
              "      <td>river seine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>rollo</td>\n",
              "      <td>the 880s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>rollo</td>\n",
              "      <td>the 880s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>rollo</td>\n",
              "      <td>the 880s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>catholicism</td>\n",
              "      <td>norse religion and old norse language with cat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>catholicism</td>\n",
              "      <td>norse religion and old norse language with cat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>catholicism</td>\n",
              "      <td>norse religion and old norse language with cat...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                Actual                                          Predicted\n",
              "0                               france                                             france\n",
              "1                               france                                             france\n",
              "2                               france                                             france\n",
              "3                               france                                             france\n",
              "4              10th and 11th centuries                            10th and 11th centuries\n",
              "5       in the 10th and 11th centuries                            10th and 11th centuries\n",
              "6              10th and 11th centuries                            10th and 11th centuries\n",
              "7              10th and 11th centuries                            10th and 11th centuries\n",
              "8          denmark, iceland and norway                        denmark, iceland and norway\n",
              "9          denmark, iceland and norway                        denmark, iceland and norway\n",
              "10         denmark, iceland and norway                        denmark, iceland and norway\n",
              "11         denmark, iceland and norway                        denmark, iceland and norway\n",
              "12                               rollo                                              rollo\n",
              "13                               rollo                                              rollo\n",
              "14                               rollo                                              rollo\n",
              "15                               rollo                                              rollo\n",
              "16                        10th century                                               10th\n",
              "17  the first half of the 10th century                                               10th\n",
              "18                                10th                                               10th\n",
              "19                                10th                                               10th\n",
              "20               william the conqueror                              william the conqueror\n",
              "21               william the conqueror                              william the conqueror\n",
              "22               william the conqueror                              william the conqueror\n",
              "23                           richard i                              richard i of normandy\n",
              "24                           richard i                              richard i of normandy\n",
              "25                           richard i                              richard i of normandy\n",
              "26                            catholic                                           catholic\n",
              "27                  catholic orthodoxy                                           catholic\n",
              "28                            catholic                                           catholic\n",
              "29                              viking                                   norseman, viking\n",
              "30                    norseman, viking                                   norseman, viking\n",
              "31                    norseman, viking                                   norseman, viking\n",
              "32                         9th century                        medieval latin, 9th century\n",
              "33                         9th century                        medieval latin, 9th century\n",
              "34                         9th century                        medieval latin, 9th century\n",
              "35                                 911                                                911\n",
              "36                                 911                                                911\n",
              "37                                 911                                                911\n",
              "38                    king charles iii                   king charles iii of west francia\n",
              "39                    king charles iii                   king charles iii of west francia\n",
              "40                    king charles iii                   king charles iii of west francia\n",
              "41                               seine                                        river seine\n",
              "42                                epte                                        river seine\n",
              "43                               seine                                        river seine\n",
              "44                               rollo                                           the 880s\n",
              "45                               rollo                                           the 880s\n",
              "46                               rollo                                           the 880s\n",
              "47                         catholicism  norse religion and old norse language with cat...\n",
              "48                         catholicism  norse religion and old norse language with cat...\n",
              "49                         catholicism  norse religion and old norse language with cat..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51yMRxbbRvGA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}